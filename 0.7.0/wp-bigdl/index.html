<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>BigDL - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "BigDL: A Distributed Deep Learning Framework for Big Data", url: "#bigdl-a-distributed-deep-learning-framework-for-big-data", children: [
          ]},
          {title: "Abstract", url: "#abstract", children: [
          ]},
          {title: "1. Introduction", url: "#1-introduction", children: [
          ]},
          {title: "2. Programming Model", url: "#2-programming-model", children: [
              {title: "2.1. Spark", url: "#21-spark" },
              {title: "2.2. Data transformation", url: "#22-data-transformation" },
              {title: "2.3. Model Construction", url: "#23-model-construction" },
              {title: "2.4. Model training", url: "#24-model-training" },
              {title: "2.5. Model Inference", url: "#25-model-inference" },
              {title: "2.6. Spark DataFrame and ML Pipeline", url: "#26-spark-dataframe-and-ml-pipeline" },
          ]},
          {title: "3. Execution Model", url: "#3-execution-model", children: [
              {title: "3.1. Data-parallel training", url: "#31-data-parallel-training" },
              {title: "3.2. Parameter synchronization", url: "#32-parameter-synchronization" },
              {title: "3.3. Task scheduling", url: "#33-task-scheduling" },
              {title: "3.4. Model quantization", url: "#34-model-quantization" },
              {title: "3.5. Local execution", url: "#35-local-execution" },
          ]},
          {title: "4. Applications", url: "#4-applications", children: [
              {title: "4.1. Model Inference: image feature extraction", url: "#41-model-inference-image-feature-extraction" },
              {title: "4.2. Distributed training: precipitation nowcasting", url: "#42-distributed-training-precipitation-nowcasting" },
              {title: "4.3. Transfer learning: image-similarity based house recommendations", url: "#43-transfer-learning-image-similarity-based-house-recommendations" },
          ]},
          {title: "5.Related Work", url: "#5related-work", children: [
          ]},
          {title: "6. Summary", url: "#6-summary", children: [
          ]},
          {title: "7. Acknowledgement", url: "#7-acknowledgement", children: [
          ]},
          {title: "8. Reference", url: "#8-reference", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>BigDL</strong></h1>
    <hr>
    <h2 id="bigdl-a-distributed-deep-learning-framework-for-big-data"><strong>BigDL: A Distributed Deep Learning Framework for Big Data</strong></h2>
<p>Jason (Jinquan) Dai<sup>1</sup>, Yiheng Wang<sup>1</sup>, Xin Qiu<sup>1</sup>, Ding Ding<sup>1</sup>, Yao Zhang<sup>2 ǂ</sup>, Yanzhang Wang<sup>1</sup>, Xianyan Jia<sup>2 ǂ</sup>, Cherry (Li) Zhang<sup>1</sup>, Yan Wan<sup>3 ǂ</sup>, Zhichao Li<sup>1</sup>, Jiao Wang<sup>1</sup>, Shengsheng Huang<sup>1</sup>, Zhongyuan Wu<sup>1</sup>, Yang Wang<sup>1</sup>, Yuhao Yang<sup>1</sup>, Bowen She<sup>1</sup>, Dongjie Shi<sup>1</sup>, Qi Lu<sup>1</sup>, Kai Huang<sup>1</sup>, Guoqiong Song<sup>1</sup></p>
<p><sup>1</sup>Intel Corporation,    <sup>2</sup>Tencent Inc.,    <sup>3</sup>Alibaba Group</p>
<p><sup>ǂ</sup>Work was done when the author worked at Intel</p>
<hr />
<h2 id="abstract"><strong>Abstract</strong></h2>
<p><em>In this paper, we present BigDL, a distributed deep learning framework for Big Data platforms and workflows. It is implemented on top of Apache Spark, and allows users to write their deep learning applications as standard Spark programs (running directly on large-scale big data clusters in a distributed fashion). It provides an expressive, “data-analytics integrated” deep learning programming model, so that users can easily build the end-to-end analytics + AI pipelines under a unified programing paradigm; by implementing an </em>AllReduce<em> like operation using existing primitives in Spark (e.g., shuffle, broadcast, and in-memory data persistence), it also provides a highly efficient “parameter server” style architecture, so as to achieve highly scalable, data-parallel distributed training. Since its initial open source release, BigDL users have built many analytics and deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark.</em></p>
<h2 id="1-introduction"><strong>1. Introduction</strong></h2>
<p>Recent breakthroughs in artificial intelligence have brought deep learning to the forefront of new generations of data analytics; as the requirements and usage models expand, new systems and architecture beyond existing deep learning frameworks (e.g., Caffe [1], Torch [2], TensorFlow [3], MXNet [4], Chainer [5], etc.) have inevitably emerged. In particular, there is increasing demand from organizations to apply deep learning technologies (such as computer vision, natural language processing, generative adversary networks, etc.) to their big data platforms and pipelines. This emerging convergence of deep learning and big data analytics is driven by several important technology and industry trends:</p>
<ul>
<li>
<p><strong>Data scale drives deep learning process.</strong> Today users are building even deeper, more complex neural networks to take advantage of the massive amount of data that they have access to. In practice, big data (e.g., Apache Hadoop [6] or Apache Spark [7]) clusters are ubiquitously deployed as the global data platform, where all the production data are stored and made available to all the users. Therefore, it is usually much more efficient to run the algorithm directly on the big data cluster where the data are stored and shared (than copying data to a separate infrastructure).</p>
</li>
<li>
<p><strong>Real-world deep learning applications are complex big data pipelines,</strong> which require a lot of data processing (such as cleaning, transformation, augmentation, feature extraction, etc.) beyond model training/inference. Therefore, it is much simpler and more efficient (for development and workflow management) to seamlessly integrate deep learning functionalities into existing big data workflow running on the same infrastructure, especially given the recent improvements that reduce deep learning training time from weeks to hours [9] or even minutes [10].</p>
</li>
<li>
<p><strong>Deep learning is increasingly adopted by the big data and data science community.</strong> Unfortunately, mainstream data engineers and data scientists are usually not deep learning experts; as the usages of deep learning expand and scale to larger deployment, it will be much more easier if these users can continue the use of familiar software tools and programming models (e.g., Spark [8] or even SQL) and existing big data cluster infrastructures to build their deep learning applications.</p>
</li>
</ul>
<p>We have developed <em>BigDL</em> [11], a distributed deep learning framework for big data platforms and workflows. It is implemented as a library on top of Apache Spark, and allows users to write their large-scale deep learning applications (including model training, fine-tuning and inference) as standard Spark programs, which can run directly on existing big data (Hadoop or Spark) clusters. BigDL provides comprehensive support of deep learning technologies (neural network operations, layers, losses and optimizers); in particular, users can directly run existing models defined in other frameworks (such as TensorFlow, Keras [12], Caffe and Torch) on Spark in a distributed fashion.</p>
<p>BigDL also provides seamless integrations of deep learning technologies into the big data ecosystem. Not only can a BigDL program directly interact with different components in the Spark framework (e.g., DataFrames [13], Spark Streaming [14], ML Pipelines [15], etc.), it can also directly run in a variety of big data frameworks (such as Apache Storm [17], Apache Flink [18], Apache Kafka [19], etc.). Since its initial open source on Dec 30, 2016, BigDL has enabled many community users to build their deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark and big data platforms.</p>
<h2 id="2-programming-model"><strong>2. Programming Model</strong></h2>
<p>BigDL is implemented on Apache Spark, a widely used cluster computing engine for big data analysis. Spark provides a comprehensive set of libraries for relational processing, streaming, graph processing [16] and machine learning (in Python, Scala or Java); as a result, one can easily build the end-to-end, “data-analytics integrated” deep learning and AI pipelines (under a unified programing paradigm) using Spark and BigDL, as illustrated in Figure 1.</p>
<pre><code>1    spark = SparkContext(appName=&quot;text_classifier&quot;, …)
2    //load input data: (text, label) pairs
3    texts_rdd = spark.textFile(&quot;hdfs://...&quot;)
4    //convert text to list of words
5    words_rdd = texts_rdd.map(lambda text, label: 
6                               ([w for w in to_words(text)], label))
7    //load GloVe embedding
8    w2v = news20.get_glove_w2v(dim=…)
9    //convert word list to list of vertors using GloVe embeddings
10   vector_rdd = words_rdd.map(lambda word_list, label:
11                              ([to_vec(w, w2v) for w in word_list], label))
12   //convert (list of vertors, label) pair to Sample
13   sample_rdd = vector_rdd.map(lambda vector_list, label: 
14                                 to_sample(vector_list, label))
15   //construct neural network model  
16   model = Sequential().add(Recurrent().add(LSTM(…)))
17                       .add(Linear(…))
18                       .add(LogSoftMax())
19   //train the model 
20   loss = ClassNLLCriterion()
21   optim_method = Adagrad()
22   optimizer = Optimizer(model=model, training_rdd=sample_rdd, 
23                         criterion=loss, optim_method= optim_method, …)
24   optimizer.set_train_summary(summary = TrainSummary(…))
25   trained_model =optimizer.optimize()
26   //model prediction
27   test_rdd = …
28   prediction_rdd = trained_model.predict(test_rdd)
</code></pre>

<p><em>Figure 1. The end-to-end text classification pipeline (including data loading, tokenization, word vectorization, training, prediction, etc.) on Spark and BigDL.</em></p>
<h3 id="21-spark"><strong>2.1. Spark</strong></h3>
<p>Spark provides the <em>Resilient Distributed Dataset</em> (RDD) [8] in-memory storage abstraction, which is an immutable collection of Python or Scala/Java objects partitioned across a cluster, and can be transformed to derive new RDDs through data-parallel functional operators like <em>map, filter and reduce.</em> Consequently, users can efficiently load very large dataset and process the loaded data in a distributed fashion using Spark, and then feed the processed data into the analytics and AI pipeline. For example, lines 1 ~ 6 in Figure 1 illustrates how to load the input data (article texts and their associated labels) from the Hadoop Distributed File System (HDFS) [20], and transforms each text string into a list of words.</p>
<h3 id="22-data-transformation"><strong>2.2. Data transformation</strong></h3>
<p>Spark supports general dataflow DAGs [8] by composing multiple data-parallel operators on RDD, where each vertex represents an RDD and each edge represents the transformation by the RDD operator. By constructing the dataflow DAG in Spark, users can easily transform the input data (for, e.g., image augmentations, word vectorizations, etc.), which can then be used by the neural network models. For example, lines 7 ~ 11 in Figure 1 illustrates how to apply GloVe word embedding [21] to transform each word to a vector. </p>
<ul>
<li>
<p><strong>N-dimensional array:</strong> In BigDL, we model the basic data elements used in neural network computations as N-dimensional numeric (int8, float32, etc.) arrays. These arrays are represented by <em>numpy.ndarry</em> [22] and <em>BigDL.Tensor</em> (similar to <em>Torch.Tensor</em> [23]) for BigDL Python and Scala/Java APIs respectively.</p>
</li>
<li>
<p><strong>Sample:</strong> Each record used in BigDL model training and prediction is modelled as a Sample, which contains an input feature and an optional label. Each input feature is one or more N-dimensional arrays, while each label is either a scalar (float32) value, or one or more <em>N-dimensional arrays.</em> For instance, lines 12 ~ 14 in Figure 1 shows how to turn the transformed data into an RDD of <em>Samples,</em> which will later be used by BigDL model training.</p>
</li>
</ul>
<h3 id="23-model-construction"><strong>2.3. Model Construction</strong></h3>
<p>Similar to Torch and Keras, BigDL uses a dataflow representation for the neural network model, where each vertex in the dataflow graph represents a neural network layer (such as <em>ReLu, Spatial Convolution and LSTM</em>). BigDL then uses the semantics of the layers for model evaluation (<em>forward</em>) and gradient computation (<em>backward</em>). For example, lines 15 ~ 18 in Figure 1 illustrates the model definition used in the text classification example.</p>
<h3 id="24-model-training"><strong>2.4. Model training</strong></h3>
<p>The transformed input data (RDD of Samples) and the constructed model can then be passed over to the <em>Optimizer</em> in BigDL, which automatically performs distributed model training across the cluster, as illustrated by lines 19 ~ 25 in <em>Figure 1.</em></p>
<ul>
<li>
<p><strong>Optimizer:</strong> In BigDL, the distributed training process is modelled by the Optimizer abstraction, which runs multiple, iterative Spark jobs to minimize the loss (as defined by the user specified Criterion) using specific optimization method (such as <em>SGD, AdaGrad [24], Adam [25], etc.</em>).</p>
</li>
<li>
<p><strong>Visualization:</strong> To make it easy for users to understand the behaviors of model training, the <em>optimizer</em> in BigDL can be configured to produce a <em>TrainSummary</em> that contains various summary data (e.g., loss, weight, etc.), as illustrated by line 24 in Figure 1; the summary data can then be visualized in, for instance, TensorBoard [26] or Jupytor Notebooks [27].</p>
</li>
</ul>
<h3 id="25-model-inference"><strong>2.5. Model Inference</strong></h3>
<p>BigDL also allows users to directly use existing models (pre-trained by Caffe, Keras, TensorFlow, Torch or BigDL) in Spark, so as to directly perform model prediction in a distributed fashion (using RDD transformations), as illustrated by lines 26 ~ 28 in Figure 1. </p>
<ul>
<li><strong>ModelBroadcast:</strong> BigDL provides the <em>ModelBroadcast</em> abstraction to manage the deployment of the pre-trained model across the cluster in a Spark job; the model prediction operation (<em>predict</em>) in BigDL uses <em>ModelBroadcast</em> to cache a single copy of the model on each machine (by leveraging the <em>broadcast</em> [28] mechanism in Spark), and manage the model cloning and weight sharing among different tasks in the same machine.</li>
</ul>
<h3 id="26-spark-dataframe-and-ml-pipeline"><strong>2.6. Spark DataFrame and ML Pipeline</strong></h3>
<p>Besides RDD, Spark provides a high level <em>DataFrame</em> abstraction [13], which is a distributed collection of rows with a specific schema (similar to a table in a relational database), and implements data-parallel relational operators like <em>filter</em> and <em>join</em> for efficient structured data analysis. On top of DataFrame, Spark introduces a high level ML <em>(machine learning) pipeline</em> [15] similar to SciKit-Learn [29], which allows users to construct the machine learning workflow as a graph of transformations on data (e.g., feature extraction, normalization, model training, etc.). BigDL also provides native integration with the high level Spark DataFrame and ML Pipeline APIs (using its <em>DLModel</em> and <em>DLEstimator</em> abstractions). </p>
<h2 id="3-execution-model"><strong>3. Execution Model</strong></h2>
<p>Similar to other Big Data systems (such as MapReduce [30]), a Spark cluster consists of a single driver node and multiple worker nodes, as shown in Figure 2.  The driver node is responsible for coordinating the tasks in a Spark job (e.g., scheduling and dispatching), while the worker nodes are responsible for the actual computation and physical data storage. To automatically parallelize the large-scale data processing across the cluster in a fault-tolerant fashion, Spark provides a functional compute model where immutable RDDs are transformed through coarse-grained operators (i.e., applying the same operation to all data items). </p>
<p><img alt="fig2" src="../Image/WP/fig2.jpg" /> </p>
<p><em>Figure 2. A Spark job contains many Spark tasks; the driver node is responsible for scheduling and dispatching the tasks to worker nodes, which runs the actual Spark tasks.</em></p>
<p>On the other hand, efficient and distributed training of deep neural networks would necessitate very different operations (such as fine-grained data access and in-place data mutation [3]). In this section, we describe in details how BigDL supports highly efficient and scalable distributed training, directly on top of the data parallel and functional compute model of Spark (in addition to various optimizations for model inference).</p>
<h3 id="31-data-parallel-training"><strong>3.1. Data-parallel training</strong></h3>
<p>To train a deep neural network model across the cluster, BigDL provides data-parallel training on Spark using synchronous mini-batch SGD, which is shown to achieve better scalability and efficiency (in terms of time-to-quality) compared to asynchronous training [31][32]. The distributed training in BigDL is implemented as an iterative process, as illustrated in Figure 3; each iteration runs a couple of Spark jobs to first compute the gradients using the current mini-batch, and then make a single update to the parameters of the neural network model.</p>
<pre><code>for (i &lt;- 1 to N) {
  //&quot;model forward-backward&quot; job
  for each task in the Spark job:
     read the latest weights
     get a random batch of data from local Sample partition
     compute errors (forward on local model replica)
     compute gradients (backward on local model replica)
  //&quot;parameter synchronization&quot; job
  aggregate (sum) all the gradients
  update the weights per specified optimization method
}
</code></pre>

<p><em>Figure 3. BigDL provides efficient, data-parallel, synchronous mini-batch SGD, where each iteration runs two Spark jobs for “model forward-backward” and “parameter synchronization”.</em></p>
<p>As described in Section 2, BigDL models the training data as an RDD of Samples, which are automatically partitioned and potentially cached in memory across the Spark cluster. In addition, to implement the data-parallel training, BigDL also constructs an RDD of models, each of which is a replica of the original neural network model. The model and Sample RDDs are co-partitioned and co-located [14] across the cluster, as shown in Figure 4; consequently, in each iteration of the model training, a single “model forward-backward” Spark job can apply the functional <em>zip</em> operator to the partitions of model and Sample RDDs, and compute the gradients in parallel for each model replica (using a small batch of data in the co-located Sample partition), as illustrated in Figure 4.</p>
<p><img alt="fig4" src="../Image/WP/fig4.jpg" /> </p>
<p><em>Figure 4. The “model forward-backward” spark job, which computes the local gradients for each model replica in parallel.</em></p>
<h3 id="32-parameter-synchronization"><strong>3.2. Parameter synchronization</strong></h3>
<p>Parameter synchronization is a performance critical operation for data-parallel training (in terms of speed and scalability). To support efficient parameter synchronization, existing deep learning frameworks usually implement the <em>parameter server</em> [33][34][35] architecture or <em>AllReduce</em> [36] operation, which unfortunately cannot be directly supported by the functional compute model provided by the Big Data systems.</p>
<p>In BigDL, we have adapted the primitives available in Spark (e.g., <em>shuffle, broadcast, in-memory cache</em>, etc.) to implement an efficient AllReduce-like operation, so as to mimic the functionality of a parameter server architecture (as illustrated in Figure 5).</p>
<p><img alt="fig5" src="../Image/WP/fig5.jpg" /> </p>
<p><em>Figure 5. Parameter synchronization in BigDL. Each local gradient (computed by a task in the “model forward-backward” job) is evenly divided into N partitions; then each task n in the “parameter synchronization” job aggregates these local gradients and update the weights for the nth partition.</em></p>
<ul>
<li>
<p>A Spark job has <em>N</em> tasks, each of which is assigned a unique Id ranging from <em>1</em> to <em>N</em> in BigDL. After each task in the “<em>model forward-backward</em>” job computes the local gradients (as described in section 3.1), it evenly divides the local gradients into <em>N</em> partitions, as shown in Figure 5.</p>
</li>
<li>
<p>Next, another “<em>parameter synchronization</em>” job is launched; each task <em>n</em> in the “<em>parameter synchronization</em>” job is responsible for managing the n<sup>th</sup> partition of the parameters, just like a parameter server (as shown in Figure 6). Specifically, the n<sup>th</sup> partition of the gradients (from all the tasks of the previous “<em>model forward-backward</em>” job) are first <strong>shuffled</strong> to task <em>n</em>, which then aggregates (sums) these gradients, and applies the updates to the n<sup>th</sup> partition of the weights (using the specific <em>optimization method</em>), as illustrated in Figure 5.</p>
</li>
</ul>
<p><img alt="fig6" src="../Image/WP/fig6.jpg" /> </p>
<p><em>Figure 6. The “parameter synchronization” Spark job, manages the n<sup>th</sup> partition of the parameters (similar to a parameter server).</em></p>
<ul>
<li>
<p>After that, each task <em>n</em> in the “<em>parameter synchronization</em>” job <strong>broadcasts</strong> the n<sup>th</sup> partition of the updated weights; consequently, tasks in the “<em>model forward-backward</em>” job of the next iteration can read the latest value of all the weights before the next training step begins.</p>
</li>
<li>
<p>The <em>shuffle</em> and <em>task-side broadcast</em> operations described above are implemented on top of the distributed <strong>in-memory</strong> storage in Spark: both the shuffled <em>gradients</em> and broadcasted <em>weights</em> are materialized in memory, which can be read remotely by the Spark tasks with extremely low latency.</p>
</li>
</ul>
<p>By implementing the AllReduce operation using primitives in Spark, BigDL provides a highly efficient “parameter server” style architecture directly on top of Big Data frameworks. As a result, it is demonstrated to support highly scalable distributed training on up to 256-node, as reported by Cray [37] and shown in Figure 7. </p>
<p><img alt="fig7" src="../Image/WP/fig7.jpg" /> </p>
<p><em>Figure 7. Throughput of ImageNet Inception v1 training reported by Cary [37] (using BigDL 0.3.0 and dual-socket Intel Broadwell 2.1 GHz); the training throughput scales almost linear up to 128 nodes (and continue to scale reasonably up to 256 nodes).</em></p>
<h3 id="33-task-scheduling"><strong>3.3. Task scheduling</strong></h3>
<p>While BigDL provides a highly efficient “parameter server” style architecture, it has a fundamentally different implementation than existing deep learning frameworks. In particular, existing deep learning frameworks are typically deployed as multiple long-running, potentially stateful tasks [3], which interact with each other (in a blocking fashion to support synchronous mini-batch SGD) for model computation and parameter synchronization.</p>
<p>In contrast, BigDL runs a series of short-lived Spark jobs (e.g., two jobs per mini-batch as described in earlier sections), and each task in the job is stateless and non-blocking. As a result, BigDL programs can automatically adapt to the dynamic resource changes (e.g., preemption, failures, incremental scaling, resource sharing, etc.) in a timely fashion. On the other hand, task scheduling in Spark can become a potential bottleneck of the distributed training on a large cluster. For instance, Figure 8 shows that, for ImageNet Inception v1 training, the overhead of launching tasks (as a fraction of average compute time) in BigDL, while low for 100~200 tasks, can grows to over 10% when there are close to 500 tasks [39]. To address this issue, BigDL will launch a single, multi-threaded task on each worker, so as to achieve high scalability on large clusters (e.g., up to 256 servers as shown in Figure 7 above). </p>
<p>To scale to an even larger number (e.g., 500) of workers, one can potentially leverages the iterative nature of the model training (in which the same operations are executed repeatedly). For instance, group scheduling introduced by <em>Drizzle</em> [38] (a low latency execution engine for Spark) can help schedule multiple iterations (or a group) of computations at once, so as to greatly reduce scheduling overheads even if there are a large number of tasks, as benchmarked by RISELab [39] and shown in Figure 8.</p>
<p><img alt="fig8" src="../Image/WP/fig8.jpg" /> </p>
<p><em>Figure 8. Overheads of task scheduling and dispatch (as a fraction of average compute time) for ImageNet Inception v1 training in BigDL [39].</em></p>
<h3 id="34-model-quantization"><strong>3.4. Model quantization</strong></h3>
<p>Quantization refers to using technologies that store numbers and perform calculations on them in more compact and lower precision form (than their original format such as 32-bit floating point). BigDL takes advantage of this type of low precision computing to quantize existing models (which can be pre-trained by various frameworks such as Caffe, Keras, TensorFlow, Torch or BigDL) for optimized inference.</p>
<p>BigDL first loads existing models and then quantizes the parameters of some selected layers (e.g., Spatial Convolution) into 8-bit integer (using the equation shown in Figure 9) to produce a quantized model. During model inference, each quantized layer quantizes the input (float32) data into 8-bit integer on the fly, applies the 8-bit calculations (such as GEMM) using the quantized parameters and data, and dequantizes the results to 32-bit floating point. Many of these operations can be fused in the implementation, and consequently the quantization and dequantization overheads are very low at inference time.</p>
<pre><code>Math.round(1.0 * value 
           / Math.max(Math.abs(max), Math.abs(min)) 
           * Byte.MaxValue).toByte
</code></pre>

<p><em>Figure 9. Equation for quantizing 32-bit floating point to 8-bit integer.</em></p>
<p>Unlike many existing quantization implementations, BigDL adopts a new local quantization scheme. That is, it performs the quantization and dequantization operations (as described above) in each small local quantization window, a small sub-block (such as a patch or kernel in convolution) of the parameters or input data. As a result, BigDL can use very low bit integers, such as 8-bit, in model quantization with extremely low model accuracy drop (less than 0.1%), 4x model size reduction, and up to 2x inference speedup, as benchmarked on AWS EC2 [40] and shown in Figure 10.</p>
<p><img alt="fig10" src="../Image/WP/fig10.jpg" /></p>
<p><em>Figure 10. Model quantization results (accuracy, inference speed and model size) for SSD, VGG16 and VGG19 (using BigDL 0.3.0 and AWS EC2 C5.18xlarge instances) [40].</em></p>
<h3 id="35-local-execution"><strong>3.5. Local execution</strong></h3>
<p>In addition to being a standard Spark program, BigDL also provide support to run the model training and inference on a local JVM (without Spark). This helps improve the efficiency when running BigDL on a single node, as there are no overheads such as parameter synchronizations or task scheduling. More importantly, it makes it easy to directly integrate BigDL models (for either inference or fine-tuning) with various big data frameworks, such as Apache Storm, Apache Flink or Apache Kafka, which are usually JVM based.</p>
<h2 id="4-applications"><strong>4. Applications</strong></h2>
<p>Since its initial open source release (on Dec 30, 2016), BigDL users have built many deep learning applications on Spark and Big Data platforms. In this section, we describes three typical use cases (namely, model inference, distributed training and transfer learning) using Spark and BigDL.</p>
<h3 id="41-model-inference-image-feature-extraction"><strong>4.1. Model Inference: image feature extraction</strong></h3>
<p>JD.com [41] is one of the largest online retailers in the world. It has built an end-to-end <em>object detection and image feature extraction</em> pipeline on top of Spark and BigDL[42], as illustrated in Figure 11.</p>
<p><img alt="fig11" src="../Image/WP/fig11.jpg" /> </p>
<p><em>Figure 11. End-to-end object detection and image feature extraction pipeline (using SSD and DeepBit models) on top of Spark and BigDL [42].</em></p>
<ul>
<li>
<p>The pipeline first reads hundreds of millions of pictures from a distributed database into Spark (as an RDD of pictures), and then pre-processes the RDD of pictures (including <em>resizing</em>, <em>normalization</em>, and <em>batching</em>) in a distributed fashion using Spark.</p>
</li>
<li>
<p>After that, it uses BigDL to load a <em>SSD</em> [43] model (pre-trained in Caffe) for large scale, distributed object detection on Spark, which generates the coordinates and scores for the detected objects in each of the pictures.</p>
</li>
<li>
<p>It then generates the target images (by keeping the object with highest score as the target, and cropping the original picture based on the coordinates of the target), and further pre-processes the RDD of target images (including <em>resizing</em> and <em>batching</em>).</p>
</li>
<li>
<p>Finally it uses BigDL to load a <em>DeepBit</em> [44] model (again pre-trained in Caffe) for distributed feature extraction of the target images to generate the corresponding features, and stores the results (RDD of extracted object features) in the Hadoop Distributed File System (HDFS).</p>
</li>
</ul>
<p>The entire data analytics and deep learning pipeline, including data loading, partitioning, preprocessing, model inference, and storing the results, can be easily implemented under a unified programming paradigm (using Spark and BigDL). In addition, the end-to-end pipeline also delivers ~3.83x speedup compared to running the same solution on a GPU cluster, as reported by JD [42] and shown in Figure 12.</p>
<p><img alt="fig12" src="../Image/WP/fig12.jpg" /> </p>
<p><em>Figure 12. Throughput of GPU clusters and Xeon clusters for the image feature extraction pipeline benchmarked by JD [42]; the GPU throughput is tested on 20 NVIDIA Tesla K40 cards, and the Xeon throughput is tested on 1200 logical cores (where each dual-socket Intel Xeon E5-2650 v4 server runs 50 logical cores).</em></p>
<h3 id="42-distributed-training-precipitation-nowcasting"><strong>4.2. Distributed training: precipitation nowcasting</strong></h3>
<p>Cray has integrated BigDL to their Urika-XC analytics software suite, and built an end-to-end precipitation nowcasting (<em>predicting short-term precipitation</em>) workflow on spark and BigDL[37], including data preparation, model training and inference (as illustrated in Figure 13). </p>
<p><img alt="fig13" src="../Image/WP/fig13.jpg" /> </p>
<p><em>Figure 13. End-to-end precipitation nowcasting workflow (using sequence-to-sequence model) [37] on Spark and BigDL.</em></p>
<ul>
<li>
<p>The application first reads over a terabyte of raw radar scan data into Spark (as an RDD of radar images), and then converts it into an RDD of <em>NumPy ndarrays</em>.</p>
</li>
<li>
<p>It then trains a <em>sequence-to-sequence</em> model [45][46] (as illustrated in Figure 13), using a sequence of images leading up to the current time as the input, and a sequence of predicted images in the future as the output.</p>
</li>
<li>
<p>After the model is trained, it can be used to predict, say, precipitation patterns for the next hour, as illustrated in Figure 14.</p>
</li>
</ul>
<p><img alt="fig14" src="../Image/WP/fig14.jpg" /> </p>
<p><em>Figure 14. Predicting precipitation patterns for the next hour (i.e., a sequence of images for the future time steps of the next hour) on Spark and BigDL [37]</em></p>
<h3 id="43-transfer-learning-image-similarity-based-house-recommendations"><strong>4.3. Transfer learning: image-similarity based house recommendations</strong></h3>
<p>MLSListings Inc. is a large <em>Multiple Listing Service</em> (MLS) for real estate listings, who has been building an image-similarity based house recommendation system on Spark and BigDL [47]. The end-to-end workflow is implemented by leveraging transfer learning (including feature extractions and fine-tuning) technologies, so as to compute both the semantic and visual similarity of the house photos, as illustrated in Figure 15.</p>
<p><img alt="fig15" src="../Image/WP/fig15.jpg" /> </p>
<p><em>Figure 15. End-to-end workflow for image-similarity based house recommendations on Spark and BigDL [47]</em></p>
<p>To compute the <em>semantic similarity</em> for the photos, the system fine-tunes the Inception v1 [48] model pre-trained on the Places dataset [49], so as to train three new classifiers (namely, whether the photo shows the house front exterior, the house style and the house stories). In particular, it first loads three pre-trained Inception v1 models, and then appends two new layers (a fully-connected layer followed by a Softmax layer) to each model, so as to train the new classifiers (using photos for which MLSListings have been assigned copyrights). After the training, it can use these classifiers to produce the tags (or labels) for each house listing photo.</p>
<p>To compute the visual similarity, the system use the VGG-16 [50] model pre-trained on the Places dataset to extract the image feature for each house listing photo, which is then combined with the tags generated by the classifiers and stored into a distributed table storage.</p>
<p>At <em>model serving</em> time, the user can select a house listing photo, and have the system to recommend house listings of similar visual characteristics (by computing the cosine similarity score using the image features, while taking into considerations other properties of the houses such as photo tags, house prices, locations, etc.), as illustrated in the <em>“Similar Houses”</em> section of the webpage in Figure 16.</p>
<p><img alt="fig16" src="../Image/WP/fig16.jpg" /> </p>
<p><em>Figure 16. Automatically recommending “Similar Houses” with similar visual characteristics [47]</em></p>
<h2 id="5related-work"><strong>5.Related Work</strong></h2>
<p>Existing big data systems, such as MapReduce [30], Dryad [51] and Spark [8], provide a data-parallel, functional compute model (with potentially dataflow DAG support), so as to efficiently support data partitioning, parallel and distributed computing, fault tolerance, incremental scale-out, etc., in an automatic and transparent fashion. BigDL is built on top of this data-parallel, functional compute model, and adds new support of deep learning technologies to Apache Spark, so as to provide the “data-analytics integrated” deep learning programming model.</p>
<p>Existing deep learning frameworks, such as Caffe [1], Torch [2], TensorFlow [3], Keras [12], MXNet [4] and DL4J [52], usually use a dataflow graph (of either primitive operators or more complex layers) to represent neural network models. For distributed training, they typically implement the parameter server architecture or AllReduce operation (with fine-grained data access and in-place data mutation [3]), which are however not supported by existing big data systems. In contrast, BigDL adopts the similar dataflow representation of neural network models, but provides efficient distributed training directly on top of Apache Spark.</p>
<p>Recently there are also a lot of efforts to bring existing deep learning frameworks to Apache Spark. For instance, TensorFrames [53] and Deep Learning Pipelines [54] allow users to directly run TensorFlow or Keras models on each individual partition of Spark Dataframes, for both model inference and single-node model tuning; however, they do not support distributed model training or fine-tuning across multiple machines in a cluster. CaffeOnSpark [55] and TensorFlowOnSpark [56] frameworks use Spark as the orchestration layer to allocate resources from the cluster, and then launch the distributed Caffe or TensorFlow job on the allocated machines; however, the Caffe or TensorFlow job still runs outside of the big data framework, and has very limited interactions with the analytics pipelines. SparkNet [57] uses asynchronous SGD for distributed training on Spark; the master first broadcasts weights to the workers, and each work then trains its own Caffe model for a certain period of time, after which the weights on each worker are sent to the master and averaged to form the new weights; however, the broadcast and weight averaging is very inefficient in SparkNet (e.g., ~20 seconds with just 5 workers [57]). In contrast, BigDL provides highly efficient and scalable distributed training, directly on top of big data framework (using the primitives available in Spark). </p>
<h2 id="6-summary"><strong>6. Summary</strong></h2>
<p>We have described BigDL, including its programming model, execution model and typical use cases. It combines the benefits of big data and HPC (high performance computing) architecture, so as to provide both an expressive, “data-analytics integrated” deep learning programing model for users to build their analytics + AI pipelines, and a highly efficient “parameter server” style architecture directly on top of Big Data platforms for scalable data-parallel training. </p>
<p>BigDL is a work in progress, but our initial experience is encouraging. Since its initial open source release (on Dec 30, 2016), it has received over 2400 stars on Github; and it have enabled many users to build new analytics and deep learning applications, which can directly run on top of existing Hadoop and/or Spark clusters.</p>
<h2 id="7-acknowledgement"><strong>7. Acknowledgement</strong></h2>
<p>We gratefully acknowledge contributions from our (current and former) colleagues at Intel (including Jun Wang, Liangying Lv, Andy Chen, Yan Dai, Sergey Ermolin, Zewei Chen, Ning Wang, Yulia Tell, Pengfei Yue, Wesley Du, Erjin Ren, Xiao Dong Wang, Radhika Rangarajan, Jack Chen, Milind Damle and Dave Nielsen), and numerous users and collaborators from the open source community (including Shivaram Venkataraman, Xiao Xu, Zhenhua Wang, Alex Heye, Omid Khanmohamadi, Mike Ringenburg, Joseph Spisak, Gopi Kumar, Suqiang Song, Karthik Palaniappan, Rong Gu, etc.) to the BigDL project.</p>
<h2 id="8-reference"><strong>8. Reference</strong></h2>
<p>[1] Caffe. <a href="http://caffe.berkeleyvision.org">http://caffe.berkeleyvision.org</a></p>
<p>[2] Torch. <a href="http://torch.ch">http://torch.ch</a></p>
<p>[3] Martín Abadi, et al. “TensorFlow: A System for Large-Scale Machine Learning”, OSDI 2016.</p>
<p>[4] Tianqi Chen, et al. “MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems”, LearningSys 2015.</p>
<p>[5] Seiya Tokui, et al. “Chainer: a Next-Generation Open Source Framework for Deep Learning”. LearningSys 2015.</p>
<p>[6] Apache Hadoop. <a href="http://hadoop.apache.org">http://hadoop.apache.org</a></p>
<p>[7] Apache Spark. <a href="https://spark.apache.org">https://spark.apache.org</a></p>
<p>[8] Matei Zaharia , et al. “Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing”, NSDI 2012.</p>
<p>[9] Priya Goyal, et al. “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour”, arXiv: 1706.02677 [cs.CV]</p>
<p>[10] Yang You, et al. “ImageNet Training in Minutes”, arXiv:1709.05011 [cs.CV]</p>
<p>[11] BigDL. <a href="https://github.com/intel-analytics/BigDL/">https://github.com/intel-analytics/BigDL/</a></p>
<p>[12] Keras. <a href="https://keras.io">https://keras.io</a></p>
<p>[13] Michael Armbrust, et al. “Spark SQL: Relational Data Processing in Spark”, SIGMOD 2015.</p>
<p>[14] Matei Zaharia, et al. “Discretized Streams: Fault-Tolerant Streaming Computation at Scale”, SOSP 2013.</p>
<p>[15] Xiangrui Meng, et al. “MLlib: Machine Learning in Apache Spark”, Journal of Machine Learning Research (JMLR) 2016.</p>
<p>[16] Reynold S. Xin, et al. “GraphX: Unifying Data-Parallel and Graph-Parallel Analytics”, OSDI 2014.</p>
<p>[17] Apache Storm. <a href="http://storm.apache.org">http://storm.apache.org</a></p>
<p>[18] Apache Flink. <a href="https://flink.apache.org">https://flink.apache.org</a></p>
<p>[19] Apache Kafka. <a href="https://kafka.apache.org">https://kafka.apache.org</a></p>
<p>[20] Konstantin Shvachko, et al. “The Hadoop Distributed File System”, MSST 2010.</p>
<p>[21] Jeffrey Pennington, et al. “GloVe: Global Vectors for Word Representation”, EMNLP 2014.</p>
<p>[22] Numpy. <a href="http://www.numpy.org">http://www.numpy.org</a></p>
<p>[23] Torch7. <a href="https://github.com/torch/torch7">https://github.com/torch/torch7</a></p>
<p>[24] J. Duchi, et al. “Adaptive subgradient methods for online learning and stochastic optimization.” Journal of Machine Learning Research (JMLR) 2011.</p>
<p>[25] Diederik P. Kingma, et al. “Adam: A Method for Stochastic Optimization”, ICLR 2015.</p>
<p>[26] M. Abadi, et al.<a href="https://arxiv.org/abs/1603.04467">“Tensorflow: Large-scale machine learning on heterogeneous distributed systems.</a>, 2016.</p>
<p>[27] Project Jupyter. <a href="http://jupyter.org">http://jupyter.org</a></p>
<p>[28] Reynold Xin, et al. “Shark: SQL and Rich Analytics at Scale”, SIGMOD 2013.</p>
<p>[29] SciKit-Learn. <a href="http://scikit-learn.org/stable/">http://scikit-learn.org/stable/</a></p>
<p>[30] Jeffrey Dean, et al. “MapReduce: simplified data processing on large clusters”, OSDI 2014.</p>
<p>[31] J. Chen, et al. “Revisiting distributed synchronous SGD”, ICLR Workshop 2016.</p>
<p>[32] H. Cui, et al. “GeePS: Scalable deep learning on distributed GPUs with a GPU specialized parameter server”, EuroSys 2016.</p>
<p>[33] J. Dean, et al. “Large scale distributed deep networks”, NIPS 2012.</p>
<p>[34] T. Chilimbi, et al. “Project Adam: Building an efficient and scalable deep learning training system”, OSDI 2014.</p>
<p>[35] M. Li, et al. “Scaling distributed machine learning with the Parameter Server”, OSDI 2014.</p>
<p>[36] Andrew Gibiansky. <a href="http://research.baidu.com/bringing-hpc-techniques-deep-learning/">Bringing HPC Techniques to Deep Learning</a></p>
<p>[37] Alex Heye, et al. <a href="https://www.cray.com/blog/scalable-deep-learning-bigdl-urika-xc-software-suite/">Scalable Deep Learning with BigDL on the Urika-XC Software Suite</a></p>
<p>[38] Shivaram Venkataraman, et al. “Drizzle: Fast and Adaptable Stream Processing at Scale”, SOSP 2017.</p>
<p>[39] Shivaram Venkataraman, et al. <a href="https://rise.cs.berkeley.edu/blog/accelerating-deep-learning-training-with-bigdl-and-drizzle-on-apache-spark/">Accelerating Deep Learning Training with BigDL and Drizzle on Apache Spark</a></p>
<p>[40] Jason (Jinquan) Dai, et al. <a href="https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/">Leveraging Low Precision and Quantization for Deep Learning Using the Amazon EC2 C5 Instance and BigDL</a></p>
<p>[41] JD. <a href="https://en.wikipedia.org/wiki/JD.com">https://en.wikipedia.org/wiki/JD.com</a></p>
<p>[42] Jason (Jinquan) Dai, et al. <a href="https://software.intel.com/en-us/articles/building-large-scale-image-feature-extraction-with-bigdl-at-jdcom">Building Large-Scale Image Feature Extraction with BigDL at JD.com</a></p>
<p>[43] Wei Liu, et al. “SSD: Single Shot MultiBox Detector”, ECCV 2016.</p>
<p>[44] Kevin Lin, et al. “Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks”, CVPR 2016.</p>
<p>[45] I. Sutskever, et al. “Sequence to sequence learning with neural networks”, NIPS 2014. </p>
<p>[46] Xingjian Shi, et al. “Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”, NIPS 2015.</p>
<p>[47] Jason (Jinquan) Dai, et al. <a href="https://software.intel.com/en-us/articles/using-bigdl-to-build-image-similarity-based-house-recommendations">“Using BigDL to Build Image Similarity-Based House Recommendations”</a></p>
<p>[48] Christian Szegedy, et al. “Going deeper with convolutions”, CVPR 2015.</p>
<p>[49] B. Zhou, et al. “Places: A 10 million Image Database for Scene Recognition”, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.</p>
<p>[50] Karen Simonyan, at al. <a href="https://arxiv.org/abs/1409.1556">“Very Deep Convolutional Networks for Large-Scale Image Recognition”</a>, 2014.</p>
<p>[51]    Michael Isard, et al. “Dryad: distributed data-parallel programs from sequential building blocks”, EuroSys 2017.</p>
<p>[52]    DJ4J. https://deeplearning4j.org/</p>
<p>[53]    TensorFrames. https://github.com/databricks/tensorframes</p>
<p>[54]    Deep Learning Pipelines. https://github.com/databricks/spark-deep-learning</p>
<p>[55]    CaffeOnSpark. https://github.com/yahoo/CaffeOnSpark</p>
<p>[56]    TensorFlowOnSpark. https://github.com/yahoo/TensorFlowOnSpark</p>
<p>[57]    Philipp Moritz, et al.  “SparkNet: Training Deep Networks in Spark”, ICLR 2016.</p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>