<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Run - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Run after pip install", url: "#run-after-pip-install", children: [
          ]},
          {title: "Run on Yarn after pip install", url: "#run-on-yarn-after-pip-install", children: [
          ]},
          {title: "Run without pip install", url: "#run-without-pip-install", children: [
              {title: "Run with pyspark", url: "#run-with-pyspark" },
              {title: "Run with spark-submit", url: "#run-with-spark-submit" },
              {title: "Run with Jupyter Notebook", url: "#run-with-jupyter-notebook" },
              {title: "Run with conda environment on Yarn", url: "#run-with-conda-environment-on-yarn" },
          ]},
          {title: "Example code", url: "#example-code", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Run</strong></h1>
    <hr>
    <p>You need to first <a href="../install/">install</a> analytics-zoo, either <a href="../install/#install-from-pip-for-local-usage">from pip</a> or <a href="../install/#install-without-pip">without pip</a>.</p>
<p><strong>NOTE</strong>: We have tested on <strong>Python 3.6</strong> and <strong>Python 3.7</strong>. Support for Python 2.7 has been removed due to its end of life.</p>
<hr />
<h2 id="run-after-pip-install"><strong>Run after pip install</strong></h2>
<p><strong>Important:</strong></p>
<ol>
<li>
<p>Installing analytics-zoo from pip will automatically install <code>pyspark</code>. To avoid possible conflicts, you are highly recommended to <strong>unset <code>SPARK_HOME</code></strong> if it exists in your environment.</p>
</li>
<li>
<p>Please always first call <code>init_nncontext()</code> at the very beginning of your code after pip install. This will create a SparkContext with optimized performance configuration and initialize the BigDL engine.</p>
</li>
</ol>
<pre><code class="python">from zoo.common.nncontext import *
sc = init_nncontext()
</code></pre>

<p><strong><em>Use an Interactive Shell</em></strong></p>
<ul>
<li>Type <code>python</code> in the command line to start a REPL.</li>
<li>Try to run the <a href="#example-code">example code</a> to verify the installation.</li>
</ul>
<p><strong><em>Use Jupyter Notebook</em></strong></p>
<ul>
<li>Start jupyter notebook as you normally do, e.g.</li>
</ul>
<pre><code class="bash">jupyter notebook --notebook-dir=./ --ip=* --no-browser
</code></pre>

<ul>
<li>Try to run the <a href="#example-code">example code</a> to verify the installation.</li>
</ul>
<p><strong><em>Configurations</em></strong></p>
<ul>
<li>Increase memory</li>
</ul>
<pre><code class="bash">export SPARK_DRIVER_MEMORY=20g
</code></pre>

<ul>
<li>Add extra jars or python packages</li>
</ul>
<p>&emsp; Set the environment variables <code>BIGDL_JARS</code> and <code>BIGDL_PACKAGES</code> <strong>BEFORE</strong> creating <code>SparkContext</code>:</p>
<pre><code class="bash">export BIGDL_JARS=...
export BIGDL_PACKAGES=...
</code></pre>

<hr />
<h2 id="run-on-yarn-after-pip-install"><strong>Run on Yarn after pip install</strong></h2>
<p>You should use <code>init_spark_on_yarn</code> rather than <code>init_nncontext()</code> here to create a SparkContext on Yarn.</p>
<p>Start python and then execute the following code:</p>
<pre><code class="python">from zoo import init_spark_on_yarn

sc = init_spark_on_yarn(
    hadoop_conf=&quot;path to the yarn configuration folder&quot;,
    conda_name=&quot;zoo&quot;, # The name of the created conda-env
    num_executor=2,
    executor_cores=4,
    executor_memory=&quot;8g&quot;,
    driver_memory=&quot;2g&quot;,
    driver_cores=4,
    extra_executor_memory_for_ray=&quot;10g&quot;)
</code></pre>

<hr />
<h2 id="run-without-pip-install"><strong>Run without pip install</strong></h2>
<ul>
<li>Note that <strong>Python 3.6</strong> is only compatible with Spark 1.6.4, 2.0.3, 2.1.1 and &gt;=2.2.0. See <a href="https://issues.apache.org/jira/browse/SPARK-19019">this issue</a> for more discussion.</li>
</ul>
<p><strong><em>Set SPARK_HOME and ANALYTICS_ZOO_HOME</em></strong></p>
<ul>
<li>If you download Analytics Zoo from the <a href="../../release-download/">Release Page</a>:</li>
</ul>
<pre><code class="bash">export SPARK_HOME=the root directory of Spark
export ANALYTICS_ZOO_HOME=the path where you extract the analytics-zoo package
</code></pre>

<ul>
<li>If you build Analytics Zoo by yourself:</li>
</ul>
<pre><code class="bash">export SPARK_HOME=the root directory of Spark
export ANALYTICS_ZOO_HOME=the dist directory of Analytics Zoo
</code></pre>

<p><strong><em>Update spark-analytics-zoo.conf (Optional)</em></strong></p>
<p>If you have some customized properties in some files, which will be used with the <code>--properties-file</code> option
in <code>spark-submit/pyspark</code>, you can add these customized properties into ${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf.</p>
<hr />
<h4 id="run-with-pyspark"><strong><em>Run with pyspark</em></strong></h4>
<pre><code class="bash">${ANALYTICS_ZOO_HOME}/bin/pyspark-shell-with-zoo.sh --master local[*]
</code></pre>

<ul>
<li><code>--master</code> set the master URL to connect to</li>
<li><code>--jars</code> if there are extra jars needed.</li>
<li><code>--py-files</code> if there are extra python packages needed.</li>
</ul>
<p>You can also specify other options available for pyspark in the above command if needed.</p>
<p>Try to run the <a href="#example-code">example code</a> for verification.</p>
<hr />
<h4 id="run-with-spark-submit"><strong><em>Run with spark-submit</em></strong></h4>
<p>An Analytics Zoo Python program runs as a standard pyspark program, which requires all Python dependencies
(e.g., numpy) used by the program to be installed on each node in the Spark cluster. You can try
running the Analytics Zoo <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/objectdetection">Object Detection Python example</a>
as follows:</p>
<pre><code class="bash">${ANALTICS_ZOO_HOME}/bin/spark-submit-python-with-zoo.sh --master local[*] predict.py model_path image_path output_path
</code></pre>

<hr />
<h4 id="run-with-jupyter-notebook"><strong><em>Run with Jupyter Notebook</em></strong></h4>
<p>With the full Python API support in Analytics Zoo, users can use our package together with powerful notebooks
(such as Jupyter Notebook) in a distributed fashion across the cluster, combining Python libraries,
Spark SQL/DataFrames and MLlib, deep learning models in Analytics Zoo, as well as interactive
visualization tools.</p>
<p><strong>Prerequisites</strong>: Install all the necessary libraries on the local node where you will run Jupyter, e.g., </p>
<pre><code class="bash">sudo apt install python
sudo apt install python-pip
sudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud
</code></pre>

<p>Launch the Jupyter Notebook as follows:</p>
<pre><code class="bash">${ANALYTICS_ZOO_HOME}/bin/jupyter-with-zoo.sh --master local[*]
</code></pre>

<ul>
<li><code>--master</code> set the master URL to connect to</li>
<li><code>--jars</code> if there are extra jars needed.</li>
<li><code>--py-files</code> if there are extra python packages needed.</li>
</ul>
<p>You can also specify other options available for pyspark in the above command if needed.</p>
<p>After successfully launching Jupyter, you will be able to navigate to the notebook dashboard using
your browser. You can find the exact URL in the console output when you started Jupyter; by default,
the dashboard URL is http://your_node:8888/</p>
<p>Try to run the <a href="#example-code">example code</a> for verification.</p>
<hr />
<h4 id="run-with-conda-environment-on-yarn"><strong><em>Run with conda environment on Yarn</em></strong></h4>
<p>If you have already created Analytics Zoo dependency conda environment package according to Yarn cluster guide <a href="../install/#for-yarn-cluster">here</a>,
you can run Python programs using Analytics Zoo using the following command.</p>
<p>Here we use Analytics Zoo <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/objectdetection">Object Detection Python example</a> for illustration.</p>
<ul>
<li>Yarn cluster mode (with conda package name "environment.tar.gz" for example)</li>
</ul>
<pre><code class="bash">export SPARK_HOME=the root directory of Spark
export ANALYTICS_ZOO_HOME=the folder where you extract the downloaded Analytics Zoo zip package
export ENV_HOME=the parent directory of your conda environment package

PYSPARK_PYTHON=./environment/bin/python ${ANALYTICS_ZOO_HOME}/bin/spark-submit-python-with-zoo.sh \
    --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/bin/python \
    --master yarn-cluster \
    --executor-memory 10g \
    --driver-memory 10g \
    --executor-cores 8 \
    --num-executors 2 \
    --archives ${ENV_HOME}/environment.tar.gz#environment \
    predict.py model_path image_path output_path
</code></pre>

<ul>
<li>Yarn client mode (with conda package name "environment.tar.gz" for example)</li>
</ul>
<pre><code class="bash">export SPARK_HOME=the root directory of Spark
export ANALYTICS_ZOO_HOME=the folder where you extract the downloaded Analytics Zoo zip package
export ENV_HOME=the parent directory of your conda environment package

mkdir ${ENV_HOME}/environment
tar -xzf ${ENV_HOME}/environment.tar.gz -C ${ENV_HOME}/environment

PYSPARK_DRIVER_PYTHON=${ENV_HOME}/environment/bin/python PYSPARK_PYTHON=./environment/bin/python ${ANALYTICS_ZOO_HOME}/bin/spark-submit-python-with-zoo.sh \
    --master yarn \
    --deploy-mode client \
    --executor-memory 10g \
    --driver-memory 10g \
    --executor-cores 16 \
    --num-executors 2 \
    --archives ${ENV_HOME}/environment.tar.gz#environment \
    predict.py model_path image_path output_path
</code></pre>

<hr />
<h2 id="example-code"><strong>Example code</strong></h2>
<p>To verify if Analytics Zoo can run successfully, run the following simple code:</p>
<pre><code class="python">import zoo
from zoo.common.nncontext import *
from zoo.pipeline.api.keras.models import *
from zoo.pipeline.api.keras.layers import *

# Get the current Analytics Zoo version
zoo.__version__
# Create a SparkContext and initialize the BigDL engine.
sc = init_nncontext()
# Create a Sequential model containing a Dense layer.
model = Sequential()
model.add(Dense(8, input_shape=(10, )))
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>