{
    "docs": [
        {
            "location": "/",
            "text": "What is Analytics-Zoo?\n\n\n\n\nWhy Analytics-Zoo?",
            "title": "What is Zoo and Why"
        },
        {
            "location": "/#what-is-analytics-zoo",
            "text": "",
            "title": "What is Analytics-Zoo?"
        },
        {
            "location": "/#why-analytics-zoo",
            "text": "",
            "title": "Why Analytics-Zoo?"
        },
        {
            "location": "/gettinghelp/",
            "text": "TBD",
            "title": "Getting Help"
        },
        {
            "location": "/gettinghelp/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/release-download/",
            "text": "Release 0.1.0\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload",
            "title": "Download"
        },
        {
            "location": "/release-download/#release-010",
            "text": "Linux x64  Mac      Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download",
            "title": "Release 0.1.0"
        },
        {
            "location": "/release-docs/",
            "text": "Release 0.1.1\n\n\nAnalytics-Zoo 0.1 Docs",
            "title": "Documentation"
        },
        {
            "location": "/release-docs/#release-011",
            "text": "Analytics-Zoo 0.1 Docs",
            "title": "Release 0.1.1"
        },
        {
            "location": "/getting-started/",
            "text": "Before using Analytics-Zoo\n\n\nBefore using Analytics-Zoo, you need to ......",
            "title": "Getting Started"
        },
        {
            "location": "/getting-started/#before-using-analytics-zoo",
            "text": "Before using Analytics-Zoo, you need to ......",
            "title": "Before using Analytics-Zoo"
        },
        {
            "location": "/ScalaUserGuide/examples/",
            "text": "TBD",
            "title": "Examples"
        },
        {
            "location": "/ScalaUserGuide/examples/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/ScalaUserGuide/modelapi/",
            "text": "TBD",
            "title": "Model API"
        },
        {
            "location": "/ScalaUserGuide/modelapi/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/ScalaUserGuide/PipelineAPI/keras-style/",
            "text": "TBD",
            "title": "Keras-Style"
        },
        {
            "location": "/ScalaUserGuide/PipelineAPI/keras-style/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/ScalaUserGuide/PipelineAPI/nnframes/",
            "text": "TBD",
            "title": "NNFrames"
        },
        {
            "location": "/ScalaUserGuide/PipelineAPI/nnframes/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/PythonUserGuide/python-examples/",
            "text": "TBD",
            "title": "Examples"
        },
        {
            "location": "/PythonUserGuide/python-examples/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/PythonUserGuide/model-api/",
            "text": "TBD",
            "title": "Model API"
        },
        {
            "location": "/PythonUserGuide/model-api/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/PythonUserGuide/PipelineAPI/keras/",
            "text": "TBD",
            "title": "Keras"
        },
        {
            "location": "/PythonUserGuide/PipelineAPI/keras/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/PythonUserGuide/PipelineAPI/nnframes/",
            "text": "TBD",
            "title": "NNFrames"
        },
        {
            "location": "/PythonUserGuide/PipelineAPI/nnframes/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/programming-guide/",
            "text": "TBD",
            "title": "Programming Guide"
        },
        {
            "location": "/programming-guide/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/",
            "text": "Introduction\n\n\nWe hereby introduce a new set of \nKeras-Style API\n based on \nKeras 1.2.2\n in BigDL for the sake of user-friendliness. Users, especially those familiar with Keras, are recommended to use the new API to create a BigDL model and train, evaluate or tune it in a distributed fashion.\n\n\nTo define a model in Python using the Keras-Style API, now one just need to import the following packages:\n\n\nfrom bigdl.nn.keras.topology import *\nfrom bigdl.nn.keras.layer import *\n\n\n\n\nOne of the highlighted features with regard to the new API is \nshape inference\n. Users only need to specify the input shape (a shape tuple \nexcluding\n batch dimension, for example, \ninput_shape=(3, 4)\n for 3D input) for the first layer of a model and for the remaining layers, the input dimension will be automatically inferred.\n\n\n\n\nDefine a model\n\n\nYou can define a model either using \nSequential API\n or \nFunctional API\n. Remember to specify the input shape for the first layer.\n\n\nAfter creating a model, you can call the following \nmethods\n:\n\n\nget_input_shape()\n\n\n\n\nget_output_shape()\n\n\n\n\n\n\nReturn the input or output shape of a model, which is a shape tuple. The first entry is \nNone\n representing the batch dimension. For a model with multiple inputs or outputs, a list of shape tuples will be returned.\n\n\n\n\nset_name(name)\n\n\n\n\n\n\nSet the name of the model. Can alternatively specify the argument \nname\n in the constructor when creating a model.\n\n\n\n\nSee \nhere\n on how to train, predict or evaluate a defined model.\n\n\n\n\nSequential API\n\n\nThe model is described as a linear stack of layers in the Sequential API. Layers can be added into the \nSequential\n container one by one and the order of the layers in the model will be the same as the insertion order.\n\n\nTo create a sequential container:\n\n\nSequential()\n\n\n\n\nExample code to create a sequential model:\n\n\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Dense, Activation\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(128, )))\nmodel.add(Activation(\"relu\"))\n\n\n\n\n\n\nFunctional API\n\n\nThe model is described as a graph in the Functional API. It is more convenient than the Sequential API when defining some complex model (for example, a model with multiple outputs).\n\n\nTo create an input node:\n\n\nInput(shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nshape\n: A shape tuple indicating the shape of the input node, not including batch.\n\n\nname\n: String to set the name of the input node. If not specified, its name will by default to be a generated string.\n\n\n\n\nTo create a graph container:\n\n\nModel(input, output)\n\n\n\n\nParameters:\n\n\n\n\ninput\n: An input node or a list of input nodes.\n\n\noutput\n: An output node or a list of output nodes.\n\n\n\n\nTo merge a list of input \nnodes\n (\nNOT\n layers), following some merge mode in the Functional API:\n\n\nmerge(inputs, mode=\"sum\", concat_axis=-1) # This will return an output NODE.\n\n\n\n\nParameters:\n\n\n\n\ninputs\n: A list of node instances. Must be more than one node.\n\n\nmode\n: Merge mode. String, must be one of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'. Default is 'sum'.\n\n\nconcat_axis\n: Int, axis to use when concatenating nodes. Only specify this when merge mode is 'concat'. Default is -1, meaning the last axis of the input.\n\n\n\n\nExample code to create a graph model:\n\n\nfrom bigdl.nn.keras.topology import Model\nfrom bigdl.nn.keras.layer import Input, Dense, merge\n\n# instantiate input nodes\ninput1 = Input(shape=(8, )) \ninput2 = Input(shape=(6, ))\n# pass an input node into a layer and get an output node\ndense1 = Dense(10)(input1)\ndense2 = Dense(10)(input2)\n# merge two nodes following some merge mode\noutput = merge([dense1, dense2], mode=\"sum\")\n# create a graph container\nmodel = Model([input1, input2], output)\n\n\n\n\n\n\nLayers\n\n\nSee \nhere\n for all the available layers for the new set of Keras-Style API.\n\n\nTo set the name of a layer, you can either call \nset_name(name)\n or alternatively specify the argument \nname\n in the constructor when creating a layer.\n\n\n\n\nLeNet Example\n\n\nHere we adopt our Keras-Style API to define a LeNet CNN model to be trained on the MNIST dataset:\n\n\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import *\n\nmodel = Sequential()\nmodel.add(Reshape((1, 28, 28), input_shape=(28, 28, 1)))\nmodel.add(Convolution2D(6, 5, 5, activation=\"tanh\", name=\"conv1_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Convolution2D(12, 5, 5, activation=\"tanh\", name=\"conv2_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(100, activation=\"tanh\", name=\"fc1\"))\nmodel.add(Dense(10, activation=\"softmax\", name=\"fc2\"))\n\nmodel.get_input_shape() # (None, 28, 28, 1)\nmodel.get_output_shape() # (None, 10)\n\n\n\n\nSee \nhere\n for detailed introduction of LeNet, the full example code and running instructions.\n\n\n\n\nKeras Code Support\n\n\nIf you have an existing piece of Keras code for a model definition, without installing Keras, you can directly migrate the code to construct a BigDL model by just replacing Keras import lines with:\n\n\nfrom bigdl.nn.keras.topology import *\nfrom bigdl.nn.keras.layer import *\n\n\n\n\nand making modifications subject to the following limitations:\n\n\n\n\n\n\nThe Keras version we support and test is \nKeras 1.2.2\n with TensorFlow backend.\n\n\n\n\n\n\nThere exist some arguments supported in Keras layers but not supported in BigDL for now. See \nhere\n for the full list of unsupported layer arguments. Also, currently we haven't supported self-defined Keras layers or \nLambda\n layers.\n\n\n\n\n\n\nThe default dim_ordering in BigDL is \nth\n (Channel First, channel_axis=1).\n\n\n\n\n\n\nKeras \nbackend\n related code needs to be deleted or refactored appropriately.\n\n\n\n\n\n\nCode involving Keras utility functions or loading weights from HDF5 files should be removed.\n\n\n\n\n\n\nRemark:\n We have tested for migrating Keras code definition of \nVGG16\n, \nVGG19\n, \nResNet50\n and \nInceptionV3\n into BigDL.",
            "title": "Python Guide"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#introduction",
            "text": "We hereby introduce a new set of  Keras-Style API  based on  Keras 1.2.2  in BigDL for the sake of user-friendliness. Users, especially those familiar with Keras, are recommended to use the new API to create a BigDL model and train, evaluate or tune it in a distributed fashion.  To define a model in Python using the Keras-Style API, now one just need to import the following packages:  from bigdl.nn.keras.topology import *\nfrom bigdl.nn.keras.layer import *  One of the highlighted features with regard to the new API is  shape inference . Users only need to specify the input shape (a shape tuple  excluding  batch dimension, for example,  input_shape=(3, 4)  for 3D input) for the first layer of a model and for the remaining layers, the input dimension will be automatically inferred.",
            "title": "Introduction"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#define-a-model",
            "text": "You can define a model either using  Sequential API  or  Functional API . Remember to specify the input shape for the first layer.  After creating a model, you can call the following  methods :  get_input_shape()  get_output_shape()   Return the input or output shape of a model, which is a shape tuple. The first entry is  None  representing the batch dimension. For a model with multiple inputs or outputs, a list of shape tuples will be returned.   set_name(name)   Set the name of the model. Can alternatively specify the argument  name  in the constructor when creating a model.   See  here  on how to train, predict or evaluate a defined model.",
            "title": "Define a model"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#sequential-api",
            "text": "The model is described as a linear stack of layers in the Sequential API. Layers can be added into the  Sequential  container one by one and the order of the layers in the model will be the same as the insertion order.  To create a sequential container:  Sequential()  Example code to create a sequential model:  from bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Dense, Activation\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(128, )))\nmodel.add(Activation(\"relu\"))",
            "title": "Sequential API"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#functional-api",
            "text": "The model is described as a graph in the Functional API. It is more convenient than the Sequential API when defining some complex model (for example, a model with multiple outputs).  To create an input node:  Input(shape=None, name=None)  Parameters:   shape : A shape tuple indicating the shape of the input node, not including batch.  name : String to set the name of the input node. If not specified, its name will by default to be a generated string.   To create a graph container:  Model(input, output)  Parameters:   input : An input node or a list of input nodes.  output : An output node or a list of output nodes.   To merge a list of input  nodes  ( NOT  layers), following some merge mode in the Functional API:  merge(inputs, mode=\"sum\", concat_axis=-1) # This will return an output NODE.  Parameters:   inputs : A list of node instances. Must be more than one node.  mode : Merge mode. String, must be one of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'. Default is 'sum'.  concat_axis : Int, axis to use when concatenating nodes. Only specify this when merge mode is 'concat'. Default is -1, meaning the last axis of the input.   Example code to create a graph model:  from bigdl.nn.keras.topology import Model\nfrom bigdl.nn.keras.layer import Input, Dense, merge\n\n# instantiate input nodes\ninput1 = Input(shape=(8, )) \ninput2 = Input(shape=(6, ))\n# pass an input node into a layer and get an output node\ndense1 = Dense(10)(input1)\ndense2 = Dense(10)(input2)\n# merge two nodes following some merge mode\noutput = merge([dense1, dense2], mode=\"sum\")\n# create a graph container\nmodel = Model([input1, input2], output)",
            "title": "Functional API"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#layers",
            "text": "See  here  for all the available layers for the new set of Keras-Style API.  To set the name of a layer, you can either call  set_name(name)  or alternatively specify the argument  name  in the constructor when creating a layer.",
            "title": "Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#lenet-example",
            "text": "Here we adopt our Keras-Style API to define a LeNet CNN model to be trained on the MNIST dataset:  from bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import *\n\nmodel = Sequential()\nmodel.add(Reshape((1, 28, 28), input_shape=(28, 28, 1)))\nmodel.add(Convolution2D(6, 5, 5, activation=\"tanh\", name=\"conv1_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Convolution2D(12, 5, 5, activation=\"tanh\", name=\"conv2_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(100, activation=\"tanh\", name=\"fc1\"))\nmodel.add(Dense(10, activation=\"softmax\", name=\"fc2\"))\n\nmodel.get_input_shape() # (None, 28, 28, 1)\nmodel.get_output_shape() # (None, 10)  See  here  for detailed introduction of LeNet, the full example code and running instructions.",
            "title": "LeNet Example"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-python/#keras-code-support",
            "text": "If you have an existing piece of Keras code for a model definition, without installing Keras, you can directly migrate the code to construct a BigDL model by just replacing Keras import lines with:  from bigdl.nn.keras.topology import *\nfrom bigdl.nn.keras.layer import *  and making modifications subject to the following limitations:    The Keras version we support and test is  Keras 1.2.2  with TensorFlow backend.    There exist some arguments supported in Keras layers but not supported in BigDL for now. See  here  for the full list of unsupported layer arguments. Also, currently we haven't supported self-defined Keras layers or  Lambda  layers.    The default dim_ordering in BigDL is  th  (Channel First, channel_axis=1).    Keras  backend  related code needs to be deleted or refactored appropriately.    Code involving Keras utility functions or loading weights from HDF5 files should be removed.    Remark:  We have tested for migrating Keras code definition of  VGG16 ,  VGG19 ,  ResNet50  and  InceptionV3  into BigDL.",
            "title": "Keras Code Support"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/",
            "text": "Introduction\n\n\nWe hereby introduce a new set of \nKeras-Style API\n based on \nKeras 1.2.2\n in BigDL for the sake of user-friendliness. Users, especially those familiar with Keras, are recommended to use the new API to create a BigDL model and train, evaluate or tune it in a distributed fashion.\n\n\nTo define a model in Scala using the Keras-Style API, now one just need to import the following packages:\n\n\nimport com.intel.analytics.bigdl.nn.keras._\nimport com.intel.analytics.bigdl.utils.Shape\n\n\n\n\nOne of the highlighted features with regard to the new API is \nshape inference\n. Users only need to specify the input shape (a \nShape\n object \nexcluding\n batch dimension, for example, \ninputShape=Shape(3, 4)\n for 3D input) for the first layer of a model and for the remaining layers, the input dimension will be automatically inferred.\n\n\n\n\nShape\n\n\nInput and output shapes of a model in the Keras-Style API are described by the \nShape\n object in Scala, which can be classified into \nSingleShape\n and \nMultiShape\n.\n\n\nSingleShape\n is just a list of Int indicating shape dimensions while \nMultiShape\n is essentially a list of \nShape\n.\n\n\nExample code to create a shape:\n\n\n// create a SingleShape\nval shape1 = Shape(3, 4)\n// create a MultiShape consisting of two SingleShape\nval shape2 = Shape(List(Shape(1, 2, 3), Shape(4, 5, 6)))\n\n\n\n\nYou can use method \ntoSingle()\n to cast a \nShape\n to a \nSingleShape\n. Similarly, use \ntoMulti()\n to cast a \nShape\n to a \nMultiShape\n.\n\n\n\n\nDefine a model\n\n\nYou can define a model either using \nSequential API\n or \nFunctional API\n. Remember to specify the input shape for the first layer.\n\n\nAfter creating a model, you can call the following \nmethods\n:\n\n\ngetInputShape()\n\n\n\n\ngetOutputShape()\n\n\n\n\n\n\nReturn the input or output shape of a model, which is a \nShape\n object. For \nSingleShape\n, the first entry is \n-1\n representing the batch dimension. For a model with multiple inputs or outputs, it will return a \nMultiShape\n.\n\n\n\n\nsetName(name)\n\n\n\n\n\n\nSet the name of the model.\n\n\n\n\nSee \nhere\n on how to train, predict or evaluate a defined model.\n\n\n\n\nSequential API\n\n\nThe model is described as a linear stack of layers in the Sequential API. Layers can be added into the \nSequential\n container one by one and the order of the layers in the model will be the same as the insertion order.\n\n\nTo create a sequential container:\n\n\nSequential()\n\n\n\n\nExample code to create a sequential model:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Dense, Activation}\nimport com.intel.analytics.bigdl.utils.Shape\n\nval model = Sequential[Float]()\nmodel.add(Dense(32, inputShape = Shape(128)))\nmodel.add(Activation(\"relu\"))\n\n\n\n\n\n\nFunctional API\n\n\nThe model is described as a graph in the Functional API. It is more convenient than the Sequential API when defining some complex model (for example, a model with multiple outputs).\n\n\nTo create an input node:\n\n\nInput(inputShape = null, name = null)\n\n\n\n\nParameters:\n\n\n\n\ninputShape\n: A \nShape\n object indicating the shape of the input node, not including batch.\n\n\nname\n: String to set the name of the input node. If not specified, its name will by default to be a generated string.\n\n\n\n\nTo create a graph container:\n\n\nModel(input, output)\n\n\n\n\nParameters:\n\n\n\n\ninput\n: An input node or an array of input nodes.\n\n\noutput\n: An output node or an array of output nodes.\n\n\n\n\nTo merge a list of input \nnodes\n (\nNOT\n layers), following some merge mode in the Functional API:\n\n\nimport com.intel.analytics.bigdl.nn.keras.Merge.merge\n\nmerge(inputs, mode = \"sum\", concatAxis = -1) // This will return an output NODE.\n\n\n\n\nParameters:\n\n\n\n\ninputs\n: A list of node instances. Must be more than one node.\n\n\nmode\n: Merge mode. String, must be one of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'. Default is 'sum'.\n\n\nconcatAxis\n: Int, axis to use when concatenating nodes. Only specify this when merge mode is 'concat'. Default is -1, meaning the last axis of the input.\n\n\n\n\nExample code to create a graph model:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Input, Dense, Model}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.nn.keras.Merge.merge\n\n// instantiate input nodes\nval input1 = Input[Float](inputShape = Shape(8))\nval input2 = Input[Float](inputShape = Shape(6))\n// call inputs() with an input node and get an output node\nval dense1 = Dense[Float](10).inputs(input1)\nval dense2 = Dense[Float](10).inputs(input2)\n// merge two nodes following some merge mode\nval output = merge(inputs = List(dense1, dense2), mode = \"sum\")\n// create a graph container\nval model = Model[Float](Array(input1, input2), output)\n\n\n\n\n\n\nLayers\n\n\nSee \nhere\n for all the available layers for the new set of Keras-Style API.\n\n\nTo set the name of a layer, call the method \nsetName(name)\n of the layer.\n\n\n\n\nLeNet Example\n\n\nHere we adopt our Keras-Style API to define a LeNet CNN model to be trained on the MNIST dataset:\n\n\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.keras._\nimport com.intel.analytics.bigdl.utils.Shape\n\nval model = Sequential()\nmodel.add(Reshape(Array(1, 28, 28), inputShape = Shape(28, 28, 1)))\nmodel.add(Convolution2D(6, 5, 5, activation = \"tanh\").setName(\"conv1_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Convolution2D(12, 5, 5, activation = \"tanh\").setName(\"conv2_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(100, activation = \"tanh\").setName(\"fc1\"))\nmodel.add(Dense(10, activation = \"softmax\").setName(\"fc2\"))\n\nmodel.getInputShape().toSingle().toArray // Array(-1, 28, 28, 1)\nmodel.getOutputShape().toSingle().toArray // Array(-1, 10)\n\n\n\n\nSee \nhere\n for detailed introduction of LeNet, the full example code and running instructions.",
            "title": "Scala Guide"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#introduction",
            "text": "We hereby introduce a new set of  Keras-Style API  based on  Keras 1.2.2  in BigDL for the sake of user-friendliness. Users, especially those familiar with Keras, are recommended to use the new API to create a BigDL model and train, evaluate or tune it in a distributed fashion.  To define a model in Scala using the Keras-Style API, now one just need to import the following packages:  import com.intel.analytics.bigdl.nn.keras._\nimport com.intel.analytics.bigdl.utils.Shape  One of the highlighted features with regard to the new API is  shape inference . Users only need to specify the input shape (a  Shape  object  excluding  batch dimension, for example,  inputShape=Shape(3, 4)  for 3D input) for the first layer of a model and for the remaining layers, the input dimension will be automatically inferred.",
            "title": "Introduction"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#shape",
            "text": "Input and output shapes of a model in the Keras-Style API are described by the  Shape  object in Scala, which can be classified into  SingleShape  and  MultiShape .  SingleShape  is just a list of Int indicating shape dimensions while  MultiShape  is essentially a list of  Shape .  Example code to create a shape:  // create a SingleShape\nval shape1 = Shape(3, 4)\n// create a MultiShape consisting of two SingleShape\nval shape2 = Shape(List(Shape(1, 2, 3), Shape(4, 5, 6)))  You can use method  toSingle()  to cast a  Shape  to a  SingleShape . Similarly, use  toMulti()  to cast a  Shape  to a  MultiShape .",
            "title": "Shape"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#define-a-model",
            "text": "You can define a model either using  Sequential API  or  Functional API . Remember to specify the input shape for the first layer.  After creating a model, you can call the following  methods :  getInputShape()  getOutputShape()   Return the input or output shape of a model, which is a  Shape  object. For  SingleShape , the first entry is  -1  representing the batch dimension. For a model with multiple inputs or outputs, it will return a  MultiShape .   setName(name)   Set the name of the model.   See  here  on how to train, predict or evaluate a defined model.",
            "title": "Define a model"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#sequential-api",
            "text": "The model is described as a linear stack of layers in the Sequential API. Layers can be added into the  Sequential  container one by one and the order of the layers in the model will be the same as the insertion order.  To create a sequential container:  Sequential()  Example code to create a sequential model:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Dense, Activation}\nimport com.intel.analytics.bigdl.utils.Shape\n\nval model = Sequential[Float]()\nmodel.add(Dense(32, inputShape = Shape(128)))\nmodel.add(Activation(\"relu\"))",
            "title": "Sequential API"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#functional-api",
            "text": "The model is described as a graph in the Functional API. It is more convenient than the Sequential API when defining some complex model (for example, a model with multiple outputs).  To create an input node:  Input(inputShape = null, name = null)  Parameters:   inputShape : A  Shape  object indicating the shape of the input node, not including batch.  name : String to set the name of the input node. If not specified, its name will by default to be a generated string.   To create a graph container:  Model(input, output)  Parameters:   input : An input node or an array of input nodes.  output : An output node or an array of output nodes.   To merge a list of input  nodes  ( NOT  layers), following some merge mode in the Functional API:  import com.intel.analytics.bigdl.nn.keras.Merge.merge\n\nmerge(inputs, mode = \"sum\", concatAxis = -1) // This will return an output NODE.  Parameters:   inputs : A list of node instances. Must be more than one node.  mode : Merge mode. String, must be one of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'. Default is 'sum'.  concatAxis : Int, axis to use when concatenating nodes. Only specify this when merge mode is 'concat'. Default is -1, meaning the last axis of the input.   Example code to create a graph model:  import com.intel.analytics.bigdl.nn.keras.{Input, Dense, Model}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.nn.keras.Merge.merge\n\n// instantiate input nodes\nval input1 = Input[Float](inputShape = Shape(8))\nval input2 = Input[Float](inputShape = Shape(6))\n// call inputs() with an input node and get an output node\nval dense1 = Dense[Float](10).inputs(input1)\nval dense2 = Dense[Float](10).inputs(input2)\n// merge two nodes following some merge mode\nval output = merge(inputs = List(dense1, dense2), mode = \"sum\")\n// create a graph container\nval model = Model[Float](Array(input1, input2), output)",
            "title": "Functional API"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#layers",
            "text": "See  here  for all the available layers for the new set of Keras-Style API.  To set the name of a layer, call the method  setName(name)  of the layer.",
            "title": "Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/keras-api-scala/#lenet-example",
            "text": "Here we adopt our Keras-Style API to define a LeNet CNN model to be trained on the MNIST dataset:  import com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn.keras._\nimport com.intel.analytics.bigdl.utils.Shape\n\nval model = Sequential()\nmodel.add(Reshape(Array(1, 28, 28), inputShape = Shape(28, 28, 1)))\nmodel.add(Convolution2D(6, 5, 5, activation = \"tanh\").setName(\"conv1_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Convolution2D(12, 5, 5, activation = \"tanh\").setName(\"conv2_5x5\"))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(100, activation = \"tanh\").setName(\"fc1\"))\nmodel.add(Dense(10, activation = \"softmax\").setName(\"fc2\"))\n\nmodel.getInputShape().toSingle().toArray // Array(-1, 28, 28, 1)\nmodel.getOutputShape().toSingle().toArray // Array(-1, 10)  See  here  for detailed introduction of LeNet, the full example code and running instructions.",
            "title": "LeNet Example"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/activation/",
            "text": "Activation\n\n\nSimple activation function to be applied to the output.\n\n\nScala:\n\n\nActivation(activation, inputShape = null)\n\n\n\n\nPython:\n\n\nActivation(activation, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nactivation\n: Name of the activation function as string. See \nhere\n for available activation strings.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Activation}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Activation(\"tanh\", inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.1659365   0.28006053  -0.20148286\n0.9146865    3.4301455    1.0930616\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.9740552    0.2729611    -0.1988\n 0.723374   0.99790496  0.7979928\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Activation\n\nmodel = Sequential()\nmodel.add(Activation(\"tanh\", input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[ 0.26202468  0.15868397  0.27812652]\n [ 0.45931689  0.32100054  0.51839282]]\n\n\n\n\nOutput is\n\n\n[[ 0.2561883   0.15736534  0.27117023]\n [ 0.42952728  0.31041133  0.47645861]]\n\n\n\n\nNote that the following two pieces of code will be equivalent:\n\n\nmodel.add(Dense(32))\nmodel.add(Activation('relu'))\n\n\n\n\nmodel.add(Dense(32, activation=\"relu\"))\n\n\n\n\n\n\nAvailable Activations\n\n\n\n\nrelu\n\n\ntanh\n\n\nsigmoid\n\n\nhard_sigmoid\n\n\nsoftmax\n\n\nsoftplus\n\n\nsoftsign",
            "title": "Activation"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/activation/#activation",
            "text": "Simple activation function to be applied to the output.  Scala:  Activation(activation, inputShape = null)  Python:  Activation(activation, input_shape=None, name=None)  Parameters:   activation : Name of the activation function as string. See  here  for available activation strings.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Activation}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Activation(\"tanh\", inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.1659365   0.28006053  -0.20148286\n0.9146865    3.4301455    1.0930616\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.9740552    0.2729611    -0.1988\n 0.723374   0.99790496  0.7979928\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Activation\n\nmodel = Sequential()\nmodel.add(Activation(\"tanh\", input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[ 0.26202468  0.15868397  0.27812652]\n [ 0.45931689  0.32100054  0.51839282]]  Output is  [[ 0.2561883   0.15736534  0.27117023]\n [ 0.42952728  0.31041133  0.47645861]]  Note that the following two pieces of code will be equivalent:  model.add(Dense(32))\nmodel.add(Activation('relu'))  model.add(Dense(32, activation=\"relu\"))",
            "title": "Activation"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/activation/#available-activations",
            "text": "relu  tanh  sigmoid  hard_sigmoid  softmax  softplus  softsign",
            "title": "Available Activations"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/initialization/",
            "text": "Initialization methods initialize weights for layers. String representations of initialization methods can be passed into the layer argument \ninit\n to take effect.\n\n\n\n\nAvailable Initialization Methods\n\n\n\n\nzero\n\n\none\n\n\nuniform\n\n\nnormal\n\n\nglorot_uniform (a.k.a \nXavier\n)",
            "title": "Initialization"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/initialization/#available-initialization-methods",
            "text": "zero  one  uniform  normal  glorot_uniform (a.k.a  Xavier )",
            "title": "Available Initialization Methods"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/",
            "text": "InputLayer\n\n\nCan be used as an entry point into a model.\n\n\nScala:\n\n\nInputLayer(inputShape = null, name = null)\n\n\n\n\nPython:\n\n\nInputLayer(input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ninputShape\n: For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\nname\n: String to set the name of the input node. If not specified, its name will by default to be a generated string.\n\n\n\n\n\n\nDense\n\n\nA densely-connected NN layer.\n\n\nThe most common input is 2D.\n\n\nScala:\n\n\nDense(outputDim, init = \"glorot_uniform\", activation = null, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nDense(output_dim, init=\"glorot_uniform\", activation=None, W_regularizer=None, b_regularizer=None, bias=True, input_dim=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\noutputDim\n: The size of the output dimension.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Dense}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Dense(5, activation = \"relu\", inputShape = Shape(4)))\nval input = Tensor[Float](2, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n 1.8646977  -0.059090078  0.091468036   0.6387431\n-0.4485392  1.5150243     -0.60176533   -0.6811443\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n1.0648216  0.0         0.0  0.0  0.0\n0.0        0.20690927  0.0  0.0  0.34191078\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Dense\n\nmodel = Sequential()\nmodel.add(Dense(5, activation=\"relu\", input_shape=(4, )))\ninput = np.random.random([2, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[ 0.26202468  0.15868397  0.27812652  0.45931689]\n [ 0.32100054  0.51839282  0.26194293  0.97608528]]\n\n\n\n\nOutput is\n\n\n[[ 0.0   0.0     0.0     0.02094215  0.38839486]\n [ 0.0   0.0     0.0     0.24498197  0.38024583]]\n\n\n\n\n\n\nFlatten\n\n\nFlattens the input without affecting the batch size.\n\n\nScala:\n\n\nFlatten(inputShape = null)\n\n\n\n\nPython:\n\n\nFlatten(input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Flatten}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Flatten(inputShape = Shape(2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.2196734   -0.37271047 -0.31215316\n-0.68951845 -0.20356052 -0.85899264\n\n(1,2,.,.) =\n-1.7452804  -0.1138052  -0.9124519\n-0.94204897 0.28943604  -0.71905166\n\n(2,1,.,.) =\n0.7228912   -0.51781553 -0.5869045\n-0.82529205 0.26846665  -0.6199292\n\n(2,2,.,.) =\n-0.4529333  -0.57688874 0.9097755\n0.7112487   -0.6711465  1.3074298\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n1.2196734   -0.37271047 -0.31215316 -0.68951845 -0.20356052 -0.85899264 -1.7452804  -0.1138052  -0.9124519  -0.94204897 0.28943604  -0.71905166\n0.7228912   -0.51781553 -0.5869045  -0.82529205 0.26846665  -0.6199292  -0.4529333  -0.57688874 0.9097755   0.7112487   -0.6711465  1.3074298\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x12]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Flatten\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(2, 2, 3)))\ninput = np.random.random([2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.86901694 0.18961039 0.40317114]\n   [0.03546013 0.44338256 0.14267447]]\n  [[0.08971508 0.04943281 0.47568212]\n   [0.21874466 0.54040762 0.19513549]]]\n\n [[[0.89994454 0.10154699 0.19762439]\n   [0.90341835 0.44006613 0.08758557]]\n  [[0.51165122 0.15523108 0.47434121]\n   [0.24526962 0.79663289 0.52078471]]]]\n\n\n\n\nOutput is\n\n\n[[0.86901695 0.18961039 0.40317115 0.03546013 0.44338256 0.14267448\n  0.08971508 0.04943281 0.4756821  0.21874467 0.5404076  0.19513549]\n [0.89994454 0.10154699 0.1976244  0.90341836 0.44006613 0.08758558\n  0.5116512  0.15523107 0.4743412  0.24526963 0.7966329  0.52078474]]\n\n\n\n\n\n\nReshape\n\n\nReshapes an output to a certain shape.\n\n\nSupports shape inference by allowing one -1 in the target shape. For example, if input shape is (2, 3, 4), target shape is (3, -1), then output shape will be (3, 8).\n\n\nScala:\n\n\nReshape(targetShape, inputShape = null)\n\n\n\n\nPython:\n\n\nReshape(target_shape, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ntargetShape\n: The target shape that you desire to have. Batch dimension should be excluded.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Reshape}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Reshape(Array(3, 8), inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.7092276  -1.3941092  -0.6348466  0.71309644\n0.3605411   0.025597548 0.4287048   -0.548675\n0.4623341   -2.3912702  0.22030865  -0.058272455\n\n(1,2,.,.) =\n-1.5049093  -1.8828062  0.8230564   -0.020209199\n-0.3415721  1.1219939   1.1089007   -0.74697906\n-1.503861   -1.616539   0.048006497 1.1613717\n\n(2,1,.,.) =\n0.21216023  1.0107462   0.8586909   -0.05644316\n-0.31436008 1.6892323   -0.9961186  -0.08169463\n0.3559391   0.010261055 -0.70408463 -1.2480727\n\n(2,2,.,.) =\n1.7663039   0.07122444  0.073556066 -0.7847014\n0.17604464  -0.99110585 -1.0302067  -0.39024687\n-0.0260166  -0.43142694 0.28443158  0.72679126\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-1.7092276  -1.3941092  -0.6348466  0.71309644      0.3605411   0.025597548 0.4287048   -0.548675\n0.4623341   -2.3912702  0.22030865  -0.058272455    -1.5049093  -1.8828062  0.8230564   -0.020209199\n-0.3415721  1.1219939   1.1089007   -0.74697906     -1.503861   -1.616539   0.048006497 1.1613717\n\n(2,.,.) =\n0.21216023  1.0107462   0.8586909   -0.05644316     -0.31436008 1.6892323   -0.9961186  -0.08169463\n0.3559391   0.010261055 -0.70408463 -1.2480727      1.7663039   0.07122444  0.073556066 -0.7847014\n0.17604464  -0.99110585 -1.0302067  -0.39024687     -0.0260166  -0.43142694 0.28443158  0.72679126\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Reshape\n\nmodel = Sequential()\nmodel.add(Reshape(target_shape=(3, 8), input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.39260304 0.10383185 0.87490319 0.89167328]\n   [0.61649117 0.43285247 0.86851582 0.97743004]\n   [0.90018969 0.04303951 0.74263493 0.14208656]]\n  [[0.66193405 0.93432157 0.76160537 0.70437459]\n   [0.99953431 0.23016734 0.42293405 0.66078049]\n   [0.03357645 0.9695145  0.30111138 0.67109948]]]\n\n [[[0.39640201 0.92930203 0.86027666 0.13958544]\n   [0.34584767 0.14743425 0.93804016 0.38053062]\n   [0.55068792 0.77375329 0.84161166 0.48131356]]\n  [[0.90116368 0.53253689 0.03332962 0.58278686]\n   [0.34935685 0.32599554 0.97641892 0.57696434]\n   [0.53974677 0.90682861 0.20027319 0.05962118]]]]\n\n\n\n\nOutput is\n\n\n[[[0.39260304 0.10383185 0.8749032  0.89167327 0.6164912  0.43285248 0.86851585 0.97743005]\n  [0.9001897  0.04303951 0.74263495 0.14208655 0.661934   0.9343216  0.7616054  0.7043746 ]\n  [0.9995343  0.23016734 0.42293406 0.6607805  0.03357645 0.9695145  0.30111137 0.6710995 ]]\n\n [[0.396402   0.92930204 0.86027664 0.13958544 0.34584767 0.14743425 0.93804014 0.38053063]\n  [0.5506879  0.7737533  0.8416117  0.48131356 0.9011637  0.53253686 0.03332962 0.58278686]\n  [0.34935686 0.32599553 0.9764189  0.5769643  0.53974676 0.9068286  0.20027319 0.05962119]]]\n\n\n\n\n\n\nPermute\n\n\nPermutes the dimensions of the input according to a given pattern.\n\n\nUseful for connecting RNNs and convnets together.\n\n\nScala:\n\n\nPermute(dims, inputShape = null)\n\n\n\n\nPython:\n\n\nPermute(dims, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ndims\n: Permutation pattern, does not include the batch dimension. Indexing starts at 1.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Permute}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Permute(Array(2, 3, 1), inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.1030567  -1.4624393  0.6139582   0.21287616\n-2.2278674  -2.5211496  1.9219213   0.85134244\n0.32953477  -2.1209111  -0.82459116 -0.82447577\n\n(1,2,.,.) =\n1.0540756   2.2638302   0.19139263  -0.9037997\n-0.20562297 -0.07835103 0.3883783   0.20750551\n-0.56583923 0.9617757   -0.5792387  0.9008493\n\n(2,1,.,.) =\n-0.54270995 -1.9089237     0.9289245    0.27833897\n-1.4734148  -0.9408616     -0.40362656  -1.1730295\n0.9813707   -0.0040280274  -1.5321463   -1.4322052\n\n(2,2,.,.) =\n-0.056844145   2.2309854    2.1172705     0.10043324\n1.121064       0.16069101   -0.51750094   -1.9682871\n0.9011646      0.47903928   -0.54172426   -0.6604068\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-1.1030567  1.0540756\n-1.4624393  2.2638302\n0.6139582   0.19139263\n0.21287616  -0.9037997\n\n(1,2,.,.) =\n-2.2278674  -0.20562297\n-2.5211496  -0.07835103\n1.9219213   0.3883783\n0.85134244  0.20750551\n\n(1,3,.,.) =\n0.32953477  -0.56583923\n-2.1209111  0.9617757\n-0.82459116 -0.5792387\n-0.82447577 0.9008493\n\n(2,1,.,.) =\n-0.54270995 -0.056844145\n-1.9089237  2.2309854\n0.9289245   2.1172705\n0.27833897  0.10043324\n\n(2,2,.,.) =\n-1.4734148  1.121064\n-0.9408616  0.16069101\n-0.40362656 -0.51750094\n-1.1730295  -1.9682871\n\n(2,3,.,.) =\n0.9813707      0.9011646\n-0.0040280274  0.47903928\n-1.5321463     -0.54172426\n-1.4322052     -0.6604068\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Permute\n\nmodel = Sequential()\nmodel.add(Permute(dims=(2, 3, 1), input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.47372355 0.18103412 0.07076151 0.51208742]\n   [0.3830121  0.2036672  0.24978515 0.3458438 ]\n   [0.34180976 0.54635229 0.90048856 0.89178666]]\n  [[0.15893009 0.62223068 0.1060953  0.26898095]\n   [0.97659789 0.72022333 0.12613522 0.66538681]\n   [0.79589927 0.32906473 0.27806256 0.99698214]]]\n\n [[[0.14608597 0.96667223 0.17876087 0.37672275]\n   [0.89726934 0.09588159 0.19987136 0.99728596]\n   [0.592439   0.40126537 0.18349086 0.88102044]]\n  [[0.29313258 0.94066727 0.57244849 0.79352687]\n   [0.31302252 0.65390325 0.54829736 0.63749209]\n   [0.76679177 0.43937809 0.06966902 0.27204878]]]]\n\n\n\n\nOutput is\n\n\n[[[[0.47372353 0.1589301 ]\n   [0.18103412 0.6222307 ]\n   [0.07076152 0.1060953 ]\n   [0.5120874  0.26898095]]\n  [[0.38301212 0.9765979 ]\n   [0.2036672  0.7202233 ]\n   [0.24978516 0.12613523]\n   [0.3458438  0.6653868 ]]\n  [[0.34180975 0.7958993 ]\n   [0.54635227 0.32906473]\n   [0.90048856 0.27806255]\n   [0.89178663 0.99698216]]]\n\n [[[0.14608598 0.29313257]\n   [0.96667224 0.9406673 ]\n   [0.17876087 0.5724485 ]\n   [0.37672275 0.7935269 ]]\n\n  [[0.8972693  0.31302252]\n   [0.09588159 0.65390325]\n   [0.19987136 0.54829735]\n   [0.99728596 0.63749206]]\n\n  [[0.592439   0.76679176]\n   [0.40126538 0.43937808]\n   [0.18349086 0.06966902]\n   [0.8810204  0.27204877]]]]\n\n\n\n\n\n\nRepeatVector\n\n\nRepeats the input n times.\n\n\nThe input of this layer should be 2D.\n\n\nScala:\n\n\nRepeatVector(n, inputShape = null)\n\n\n\n\nPython:\n\n\nRepeatVector(n, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nn\n: Repetition factor. Integer.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, RepeatVector}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(RepeatVector(4, inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.4182444   2.858577    1.3975657\n-0.19606766 0.8585809   0.3027246\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n1.4182444   2.858577    1.3975657\n1.4182444   2.858577    1.3975657\n1.4182444   2.858577    1.3975657\n1.4182444   2.858577    1.3975657\n\n(2,.,.) =\n-0.19606766 0.8585809   0.3027246\n-0.19606766 0.8585809   0.3027246\n-0.19606766 0.8585809   0.3027246\n-0.19606766 0.8585809   0.3027246\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import RepeatVector\n\nmodel = Sequential()\nmodel.add(RepeatVector(4, input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.51416513 0.87768557 0.48015041]\n [0.66598164 0.58916225 0.03983186]]\n\n\n\n\nOutput is\n\n\n[[[0.5141651  0.87768555 0.4801504 ]\n  [0.5141651  0.87768555 0.4801504 ]\n  [0.5141651  0.87768555 0.4801504 ]\n  [0.5141651  0.87768555 0.4801504 ]]\n\n [[0.66598165 0.58916223 0.03983186]\n  [0.66598165 0.58916223 0.03983186]\n  [0.66598165 0.58916223 0.03983186]\n  [0.66598165 0.58916223 0.03983186]]]\n\n\n\n\n\n\nMerge\n\n\nUsed to merge a list of inputs into a single output, following some merge mode.\n\n\nMerge must have at least two input layers.\n\n\nScala:\n\n\nMerge(layers = null, mode = \"sum\", concatAxis = -1, inputShape = null)\n\n\n\n\nPython:\n\n\nMerge(layers=None, mode=\"sum\", concat_axis=-1, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nlayers\n: A list of layer instances. Must be more than one layer.\n\n\nmode\n: Merge mode. String, must be one of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'. Default is 'sum'.\n\n\nconcatAxis\n: Integer, axis to use when concatenating layers. Only specify this when merge mode is 'concat'. Default is -1, meaning the last axis of the input.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Merge, InputLayer}\nimport com.intel.analytics.bigdl.utils.{Shape, T}\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nval l1 = InputLayer[Float](inputShape = Shape(2, 3))\nval l2 = InputLayer[Float](inputShape = Shape(2, 3))\nval layer = Merge[Float](layers = List(l1, l2), mode = \"sum\")\nmodel.add(layer)\nval input1 = Tensor[Float](2, 2, 3).rand(0, 1)\nval input2 = Tensor[Float](2, 2, 3).rand(0, 1)\nval input = T(1 -> input1, 2 -> input2)\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.utils.Table =\n {\n    2: (1,.,.) =\n       0.87815475   0.15025006  0.34412447\n       0.07909282   0.008027249 0.111715704\n\n       (2,.,.) =\n       0.52245367   0.2547527   0.35857987\n       0.7718501    0.26783863  0.8642062\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n    1: (1,.,.) =\n       0.5377018    0.28364193  0.3424284\n       0.0075349305 0.9018168   0.9435114\n\n       (2,.,.) =\n       0.09112563   0.88585275  0.3100201\n       0.7910178    0.57497376  0.39764535\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n }\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n1.4158566   0.433892    0.6865529\n0.08662775  0.90984404  1.0552272\n\n(2,.,.) =\n0.6135793   1.1406054   0.66859996\n1.5628679   0.8428124   1.2618515\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Merge, InputLayer\n\nmodel = Sequential()\nl1 = InputLayer(input_shape=(3, 4))\nl2 = InputLayer(input_shape=(3, 4))\nmodel.add(Merge(layers=[l1, l2], mode='sum'))\ninput = [np.random.random([2, 3, 4]), np.random.random([2, 3, 4])]\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[array([[[0.28764351, 0.0236015 , 0.78927442, 0.52646492],\n        [0.63922826, 0.45101604, 0.4555552 , 0.70105653],\n        [0.75790798, 0.78551523, 0.00686686, 0.61290369]],\n\n       [[0.00430865, 0.3303661 , 0.59915782, 0.90362298],\n        [0.26230717, 0.99383052, 0.50630521, 0.99119486],\n        [0.56138318, 0.68165639, 0.10644523, 0.51860127]]]),\n\n array([[[0.84365767, 0.8854741 , 0.84183673, 0.96322321],\n        [0.49354248, 0.97936826, 0.2266097 , 0.88083622],\n        [0.11011776, 0.65762034, 0.17446099, 0.76658969]],\n\n       [[0.58266689, 0.86322199, 0.87122999, 0.19031255],\n        [0.42275118, 0.76379413, 0.21355413, 0.81132937],\n        [0.97294728, 0.68601731, 0.39871792, 0.63172344]]])]\n\n\n\n\nOutput is\n\n\n[[[1.1313012  0.90907556 1.6311111  1.4896882 ]\n  [1.1327708  1.4303843  0.6821649  1.5818927 ]\n  [0.8680257  1.4431355  0.18132785 1.3794935 ]]\n\n [[0.5869755  1.1935881  1.4703878  1.0939355 ]\n  [0.68505836 1.7576246  0.71985936 1.8025242 ]\n  [1.5343305  1.3676738  0.50516313 1.1503248 ]]]\n\n\n\n\n\n\nMasking\n\n\nUse a mask value to skip timesteps for a sequence.\n\n\nScala:\n\n\nMasking(maskValue = 0.0, inputShape = null)\n\n\n\n\nPython:\n\n\nMasking(mask_value=0.0, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nmaskValue\n: Mask value. For each timestep in the input (the second dimension), if all the values in the input at that timestep are equal to 'maskValue', then the timestep will masked (skipped) in all downstream layers.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Masking}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Masking(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.00938185  -1.1461893  -1.0204586\n0.24702129  -2.2756217  0.010394359\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.00938185  -1.1461893  -1.0204586\n0.24702129  -2.2756217  0.010394359\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Masking\n\nmodel = Sequential()\nmodel.add(Masking(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.59540156 0.24933489 0.04434161]\n [0.89243422 0.68499562 0.36788333]]\n\n\n\n\nOutput is\n\n\n[[0.5954016  0.24933489 0.04434161]\n [0.89243424 0.68499565 0.36788332]]\n\n\n\n\n\n\nMaxoutDense\n\n\nA dense maxout layer that takes the element-wise maximum of linear layers.\n\n\nThis allows the layer to learn a convex, piecewise linear activation function over the inputs.\n\n\nThe input of this layer should be 2D.\n\n\nScala:\n\n\nMaxoutDense(outputDim, nbFeature = 4, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nMaxoutDense(output_dim, nb_feature=4, W_regularizer=None, b_regularizer=None, bias=True, input_dim=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\noutputDim\n: The size of output dimension.\n\n\nnbFeature\n: Number of Dense layers to use internally. Integer. Default is 4.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, MaxoutDense}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxoutDense(2, inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-1.3550005  -1.1668127  -1.2882779\n0.83600295  -1.94683    1.323666\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.71675766  1.2987505\n0.9871184   0.6634239\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxoutDense\n\nmodel = Sequential()\nmodel.add(MaxoutDense(2, input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.15996114 0.8391686  0.81922903]\n [0.52929427 0.35061754 0.88167693]]\n\n\n\n\nOutput is\n\n\n[[0.4479192  0.4842512]\n [0.16833156 0.521764 ]]",
            "title": "Core Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#inputlayer",
            "text": "Can be used as an entry point into a model.  Scala:  InputLayer(inputShape = null, name = null)  Python:  InputLayer(input_shape=None, name=None)  Parameters:   inputShape : For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.  name : String to set the name of the input node. If not specified, its name will by default to be a generated string.",
            "title": "InputLayer"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#dense",
            "text": "A densely-connected NN layer.  The most common input is 2D.  Scala:  Dense(outputDim, init = \"glorot_uniform\", activation = null, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  Dense(output_dim, init=\"glorot_uniform\", activation=None, W_regularizer=None, b_regularizer=None, bias=True, input_dim=None, input_shape=None, name=None)  Parameters:   outputDim : The size of the output dimension.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  wRegularizer : An instance of  Regularizer , applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Dense}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Dense(5, activation = \"relu\", inputShape = Shape(4)))\nval input = Tensor[Float](2, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n 1.8646977  -0.059090078  0.091468036   0.6387431\n-0.4485392  1.5150243     -0.60176533   -0.6811443\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n1.0648216  0.0         0.0  0.0  0.0\n0.0        0.20690927  0.0  0.0  0.34191078\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Dense\n\nmodel = Sequential()\nmodel.add(Dense(5, activation=\"relu\", input_shape=(4, )))\ninput = np.random.random([2, 4])\noutput = model.forward(input)  Input is:  [[ 0.26202468  0.15868397  0.27812652  0.45931689]\n [ 0.32100054  0.51839282  0.26194293  0.97608528]]  Output is  [[ 0.0   0.0     0.0     0.02094215  0.38839486]\n [ 0.0   0.0     0.0     0.24498197  0.38024583]]",
            "title": "Dense"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#flatten",
            "text": "Flattens the input without affecting the batch size.  Scala:  Flatten(inputShape = null)  Python:  Flatten(input_shape=None, name=None)  Parameters:   inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Flatten}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Flatten(inputShape = Shape(2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.2196734   -0.37271047 -0.31215316\n-0.68951845 -0.20356052 -0.85899264\n\n(1,2,.,.) =\n-1.7452804  -0.1138052  -0.9124519\n-0.94204897 0.28943604  -0.71905166\n\n(2,1,.,.) =\n0.7228912   -0.51781553 -0.5869045\n-0.82529205 0.26846665  -0.6199292\n\n(2,2,.,.) =\n-0.4529333  -0.57688874 0.9097755\n0.7112487   -0.6711465  1.3074298\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n1.2196734   -0.37271047 -0.31215316 -0.68951845 -0.20356052 -0.85899264 -1.7452804  -0.1138052  -0.9124519  -0.94204897 0.28943604  -0.71905166\n0.7228912   -0.51781553 -0.5869045  -0.82529205 0.26846665  -0.6199292  -0.4529333  -0.57688874 0.9097755   0.7112487   -0.6711465  1.3074298\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x12]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Flatten\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(2, 2, 3)))\ninput = np.random.random([2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[0.86901694 0.18961039 0.40317114]\n   [0.03546013 0.44338256 0.14267447]]\n  [[0.08971508 0.04943281 0.47568212]\n   [0.21874466 0.54040762 0.19513549]]]\n\n [[[0.89994454 0.10154699 0.19762439]\n   [0.90341835 0.44006613 0.08758557]]\n  [[0.51165122 0.15523108 0.47434121]\n   [0.24526962 0.79663289 0.52078471]]]]  Output is  [[0.86901695 0.18961039 0.40317115 0.03546013 0.44338256 0.14267448\n  0.08971508 0.04943281 0.4756821  0.21874467 0.5404076  0.19513549]\n [0.89994454 0.10154699 0.1976244  0.90341836 0.44006613 0.08758558\n  0.5116512  0.15523107 0.4743412  0.24526963 0.7966329  0.52078474]]",
            "title": "Flatten"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#reshape",
            "text": "Reshapes an output to a certain shape.  Supports shape inference by allowing one -1 in the target shape. For example, if input shape is (2, 3, 4), target shape is (3, -1), then output shape will be (3, 8).  Scala:  Reshape(targetShape, inputShape = null)  Python:  Reshape(target_shape, input_shape=None, name=None)  Parameters:   targetShape : The target shape that you desire to have. Batch dimension should be excluded.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Reshape}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Reshape(Array(3, 8), inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.7092276  -1.3941092  -0.6348466  0.71309644\n0.3605411   0.025597548 0.4287048   -0.548675\n0.4623341   -2.3912702  0.22030865  -0.058272455\n\n(1,2,.,.) =\n-1.5049093  -1.8828062  0.8230564   -0.020209199\n-0.3415721  1.1219939   1.1089007   -0.74697906\n-1.503861   -1.616539   0.048006497 1.1613717\n\n(2,1,.,.) =\n0.21216023  1.0107462   0.8586909   -0.05644316\n-0.31436008 1.6892323   -0.9961186  -0.08169463\n0.3559391   0.010261055 -0.70408463 -1.2480727\n\n(2,2,.,.) =\n1.7663039   0.07122444  0.073556066 -0.7847014\n0.17604464  -0.99110585 -1.0302067  -0.39024687\n-0.0260166  -0.43142694 0.28443158  0.72679126\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-1.7092276  -1.3941092  -0.6348466  0.71309644      0.3605411   0.025597548 0.4287048   -0.548675\n0.4623341   -2.3912702  0.22030865  -0.058272455    -1.5049093  -1.8828062  0.8230564   -0.020209199\n-0.3415721  1.1219939   1.1089007   -0.74697906     -1.503861   -1.616539   0.048006497 1.1613717\n\n(2,.,.) =\n0.21216023  1.0107462   0.8586909   -0.05644316     -0.31436008 1.6892323   -0.9961186  -0.08169463\n0.3559391   0.010261055 -0.70408463 -1.2480727      1.7663039   0.07122444  0.073556066 -0.7847014\n0.17604464  -0.99110585 -1.0302067  -0.39024687     -0.0260166  -0.43142694 0.28443158  0.72679126\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Reshape\n\nmodel = Sequential()\nmodel.add(Reshape(target_shape=(3, 8), input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.39260304 0.10383185 0.87490319 0.89167328]\n   [0.61649117 0.43285247 0.86851582 0.97743004]\n   [0.90018969 0.04303951 0.74263493 0.14208656]]\n  [[0.66193405 0.93432157 0.76160537 0.70437459]\n   [0.99953431 0.23016734 0.42293405 0.66078049]\n   [0.03357645 0.9695145  0.30111138 0.67109948]]]\n\n [[[0.39640201 0.92930203 0.86027666 0.13958544]\n   [0.34584767 0.14743425 0.93804016 0.38053062]\n   [0.55068792 0.77375329 0.84161166 0.48131356]]\n  [[0.90116368 0.53253689 0.03332962 0.58278686]\n   [0.34935685 0.32599554 0.97641892 0.57696434]\n   [0.53974677 0.90682861 0.20027319 0.05962118]]]]  Output is  [[[0.39260304 0.10383185 0.8749032  0.89167327 0.6164912  0.43285248 0.86851585 0.97743005]\n  [0.9001897  0.04303951 0.74263495 0.14208655 0.661934   0.9343216  0.7616054  0.7043746 ]\n  [0.9995343  0.23016734 0.42293406 0.6607805  0.03357645 0.9695145  0.30111137 0.6710995 ]]\n\n [[0.396402   0.92930204 0.86027664 0.13958544 0.34584767 0.14743425 0.93804014 0.38053063]\n  [0.5506879  0.7737533  0.8416117  0.48131356 0.9011637  0.53253686 0.03332962 0.58278686]\n  [0.34935686 0.32599553 0.9764189  0.5769643  0.53974676 0.9068286  0.20027319 0.05962119]]]",
            "title": "Reshape"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#permute",
            "text": "Permutes the dimensions of the input according to a given pattern.  Useful for connecting RNNs and convnets together.  Scala:  Permute(dims, inputShape = null)  Python:  Permute(dims, input_shape=None, name=None)  Parameters:   dims : Permutation pattern, does not include the batch dimension. Indexing starts at 1.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Permute}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Permute(Array(2, 3, 1), inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.1030567  -1.4624393  0.6139582   0.21287616\n-2.2278674  -2.5211496  1.9219213   0.85134244\n0.32953477  -2.1209111  -0.82459116 -0.82447577\n\n(1,2,.,.) =\n1.0540756   2.2638302   0.19139263  -0.9037997\n-0.20562297 -0.07835103 0.3883783   0.20750551\n-0.56583923 0.9617757   -0.5792387  0.9008493\n\n(2,1,.,.) =\n-0.54270995 -1.9089237     0.9289245    0.27833897\n-1.4734148  -0.9408616     -0.40362656  -1.1730295\n0.9813707   -0.0040280274  -1.5321463   -1.4322052\n\n(2,2,.,.) =\n-0.056844145   2.2309854    2.1172705     0.10043324\n1.121064       0.16069101   -0.51750094   -1.9682871\n0.9011646      0.47903928   -0.54172426   -0.6604068\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-1.1030567  1.0540756\n-1.4624393  2.2638302\n0.6139582   0.19139263\n0.21287616  -0.9037997\n\n(1,2,.,.) =\n-2.2278674  -0.20562297\n-2.5211496  -0.07835103\n1.9219213   0.3883783\n0.85134244  0.20750551\n\n(1,3,.,.) =\n0.32953477  -0.56583923\n-2.1209111  0.9617757\n-0.82459116 -0.5792387\n-0.82447577 0.9008493\n\n(2,1,.,.) =\n-0.54270995 -0.056844145\n-1.9089237  2.2309854\n0.9289245   2.1172705\n0.27833897  0.10043324\n\n(2,2,.,.) =\n-1.4734148  1.121064\n-0.9408616  0.16069101\n-0.40362656 -0.51750094\n-1.1730295  -1.9682871\n\n(2,3,.,.) =\n0.9813707      0.9011646\n-0.0040280274  0.47903928\n-1.5321463     -0.54172426\n-1.4322052     -0.6604068\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Permute\n\nmodel = Sequential()\nmodel.add(Permute(dims=(2, 3, 1), input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.47372355 0.18103412 0.07076151 0.51208742]\n   [0.3830121  0.2036672  0.24978515 0.3458438 ]\n   [0.34180976 0.54635229 0.90048856 0.89178666]]\n  [[0.15893009 0.62223068 0.1060953  0.26898095]\n   [0.97659789 0.72022333 0.12613522 0.66538681]\n   [0.79589927 0.32906473 0.27806256 0.99698214]]]\n\n [[[0.14608597 0.96667223 0.17876087 0.37672275]\n   [0.89726934 0.09588159 0.19987136 0.99728596]\n   [0.592439   0.40126537 0.18349086 0.88102044]]\n  [[0.29313258 0.94066727 0.57244849 0.79352687]\n   [0.31302252 0.65390325 0.54829736 0.63749209]\n   [0.76679177 0.43937809 0.06966902 0.27204878]]]]  Output is  [[[[0.47372353 0.1589301 ]\n   [0.18103412 0.6222307 ]\n   [0.07076152 0.1060953 ]\n   [0.5120874  0.26898095]]\n  [[0.38301212 0.9765979 ]\n   [0.2036672  0.7202233 ]\n   [0.24978516 0.12613523]\n   [0.3458438  0.6653868 ]]\n  [[0.34180975 0.7958993 ]\n   [0.54635227 0.32906473]\n   [0.90048856 0.27806255]\n   [0.89178663 0.99698216]]]\n\n [[[0.14608598 0.29313257]\n   [0.96667224 0.9406673 ]\n   [0.17876087 0.5724485 ]\n   [0.37672275 0.7935269 ]]\n\n  [[0.8972693  0.31302252]\n   [0.09588159 0.65390325]\n   [0.19987136 0.54829735]\n   [0.99728596 0.63749206]]\n\n  [[0.592439   0.76679176]\n   [0.40126538 0.43937808]\n   [0.18349086 0.06966902]\n   [0.8810204  0.27204877]]]]",
            "title": "Permute"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#repeatvector",
            "text": "Repeats the input n times.  The input of this layer should be 2D.  Scala:  RepeatVector(n, inputShape = null)  Python:  RepeatVector(n, input_shape=None, name=None)  Parameters:   n : Repetition factor. Integer.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, RepeatVector}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(RepeatVector(4, inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.4182444   2.858577    1.3975657\n-0.19606766 0.8585809   0.3027246\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n1.4182444   2.858577    1.3975657\n1.4182444   2.858577    1.3975657\n1.4182444   2.858577    1.3975657\n1.4182444   2.858577    1.3975657\n\n(2,.,.) =\n-0.19606766 0.8585809   0.3027246\n-0.19606766 0.8585809   0.3027246\n-0.19606766 0.8585809   0.3027246\n-0.19606766 0.8585809   0.3027246\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import RepeatVector\n\nmodel = Sequential()\nmodel.add(RepeatVector(4, input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.51416513 0.87768557 0.48015041]\n [0.66598164 0.58916225 0.03983186]]  Output is  [[[0.5141651  0.87768555 0.4801504 ]\n  [0.5141651  0.87768555 0.4801504 ]\n  [0.5141651  0.87768555 0.4801504 ]\n  [0.5141651  0.87768555 0.4801504 ]]\n\n [[0.66598165 0.58916223 0.03983186]\n  [0.66598165 0.58916223 0.03983186]\n  [0.66598165 0.58916223 0.03983186]\n  [0.66598165 0.58916223 0.03983186]]]",
            "title": "RepeatVector"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#merge",
            "text": "Used to merge a list of inputs into a single output, following some merge mode.  Merge must have at least two input layers.  Scala:  Merge(layers = null, mode = \"sum\", concatAxis = -1, inputShape = null)  Python:  Merge(layers=None, mode=\"sum\", concat_axis=-1, input_shape=None, name=None)  Parameters:   layers : A list of layer instances. Must be more than one layer.  mode : Merge mode. String, must be one of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'. Default is 'sum'.  concatAxis : Integer, axis to use when concatenating layers. Only specify this when merge mode is 'concat'. Default is -1, meaning the last axis of the input.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Merge, InputLayer}\nimport com.intel.analytics.bigdl.utils.{Shape, T}\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nval l1 = InputLayer[Float](inputShape = Shape(2, 3))\nval l2 = InputLayer[Float](inputShape = Shape(2, 3))\nval layer = Merge[Float](layers = List(l1, l2), mode = \"sum\")\nmodel.add(layer)\nval input1 = Tensor[Float](2, 2, 3).rand(0, 1)\nval input2 = Tensor[Float](2, 2, 3).rand(0, 1)\nval input = T(1 -> input1, 2 -> input2)\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.utils.Table =\n {\n    2: (1,.,.) =\n       0.87815475   0.15025006  0.34412447\n       0.07909282   0.008027249 0.111715704\n\n       (2,.,.) =\n       0.52245367   0.2547527   0.35857987\n       0.7718501    0.26783863  0.8642062\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n    1: (1,.,.) =\n       0.5377018    0.28364193  0.3424284\n       0.0075349305 0.9018168   0.9435114\n\n       (2,.,.) =\n       0.09112563   0.88585275  0.3100201\n       0.7910178    0.57497376  0.39764535\n\n       [com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n }  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n1.4158566   0.433892    0.6865529\n0.08662775  0.90984404  1.0552272\n\n(2,.,.) =\n0.6135793   1.1406054   0.66859996\n1.5628679   0.8428124   1.2618515\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Merge, InputLayer\n\nmodel = Sequential()\nl1 = InputLayer(input_shape=(3, 4))\nl2 = InputLayer(input_shape=(3, 4))\nmodel.add(Merge(layers=[l1, l2], mode='sum'))\ninput = [np.random.random([2, 3, 4]), np.random.random([2, 3, 4])]\noutput = model.forward(input)  Input is:  [array([[[0.28764351, 0.0236015 , 0.78927442, 0.52646492],\n        [0.63922826, 0.45101604, 0.4555552 , 0.70105653],\n        [0.75790798, 0.78551523, 0.00686686, 0.61290369]],\n\n       [[0.00430865, 0.3303661 , 0.59915782, 0.90362298],\n        [0.26230717, 0.99383052, 0.50630521, 0.99119486],\n        [0.56138318, 0.68165639, 0.10644523, 0.51860127]]]),\n\n array([[[0.84365767, 0.8854741 , 0.84183673, 0.96322321],\n        [0.49354248, 0.97936826, 0.2266097 , 0.88083622],\n        [0.11011776, 0.65762034, 0.17446099, 0.76658969]],\n\n       [[0.58266689, 0.86322199, 0.87122999, 0.19031255],\n        [0.42275118, 0.76379413, 0.21355413, 0.81132937],\n        [0.97294728, 0.68601731, 0.39871792, 0.63172344]]])]  Output is  [[[1.1313012  0.90907556 1.6311111  1.4896882 ]\n  [1.1327708  1.4303843  0.6821649  1.5818927 ]\n  [0.8680257  1.4431355  0.18132785 1.3794935 ]]\n\n [[0.5869755  1.1935881  1.4703878  1.0939355 ]\n  [0.68505836 1.7576246  0.71985936 1.8025242 ]\n  [1.5343305  1.3676738  0.50516313 1.1503248 ]]]",
            "title": "Merge"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#masking",
            "text": "Use a mask value to skip timesteps for a sequence.  Scala:  Masking(maskValue = 0.0, inputShape = null)  Python:  Masking(mask_value=0.0, input_shape=None, name=None)  Parameters:   maskValue : Mask value. For each timestep in the input (the second dimension), if all the values in the input at that timestep are equal to 'maskValue', then the timestep will masked (skipped) in all downstream layers.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Masking}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Masking(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.00938185  -1.1461893  -1.0204586\n0.24702129  -2.2756217  0.010394359\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.00938185  -1.1461893  -1.0204586\n0.24702129  -2.2756217  0.010394359\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Masking\n\nmodel = Sequential()\nmodel.add(Masking(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.59540156 0.24933489 0.04434161]\n [0.89243422 0.68499562 0.36788333]]  Output is  [[0.5954016  0.24933489 0.04434161]\n [0.89243424 0.68499565 0.36788332]]",
            "title": "Masking"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/core/#maxoutdense",
            "text": "A dense maxout layer that takes the element-wise maximum of linear layers.  This allows the layer to learn a convex, piecewise linear activation function over the inputs.  The input of this layer should be 2D.  Scala:  MaxoutDense(outputDim, nbFeature = 4, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  MaxoutDense(output_dim, nb_feature=4, W_regularizer=None, b_regularizer=None, bias=True, input_dim=None, input_shape=None, name=None)  Parameters:   outputDim : The size of output dimension.  nbFeature : Number of Dense layers to use internally. Integer. Default is 4.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, MaxoutDense}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxoutDense(2, inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-1.3550005  -1.1668127  -1.2882779\n0.83600295  -1.94683    1.323666\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.71675766  1.2987505\n0.9871184   0.6634239\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxoutDense\n\nmodel = Sequential()\nmodel.add(MaxoutDense(2, input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.15996114 0.8391686  0.81922903]\n [0.52929427 0.35061754 0.88167693]]  Output is  [[0.4479192  0.4842512]\n [0.16833156 0.521764 ]]",
            "title": "MaxoutDense"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/",
            "text": "Convolution1D\n\n\nApplies convolution operator for filtering neighborhoods of 1-D inputs.\n\n\nYou can also use \nConv1D\n as an alias of this layer.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nConvolution1D(nbFilter, filterLength, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsampleLength = 1, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nConvolution1D(nb_filter, filter_length, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample_length=1, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nfilterLength\n: The extension (spatial or temporal) of each filter.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\nsubsampleLength\n: Factor by which to subsample output. Integer. Default is 1.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Convolution1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Convolution1D(8, 3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.4253887  -0.044403594    -1.1169672  -0.19499049\n0.85463065  0.6665206       0.21340805  0.56255895\n1.1126599   -0.3423326      0.09643264  -0.34345046\n\n(2,.,.) =\n-0.04046587 -0.2710401      0.10183265  1.4503858\n1.0639644   1.5317003       -0.18313104 -0.7098296\n0.612399    1.7357533       0.4641411   0.13530721\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.22175728  0.76192796  1.7907748   1.1534728   -1.5304534  0.07466106  -0.18292685 0.6038852\n\n(2,.,.) =\n0.85337734  0.43939286  -0.16770163 -0.8380078  0.7825804   -0.3485601  0.3017909   0.5823619\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Convolution1D\n\nmodel = Sequential()\nmodel.add(Convolution1D(8, 3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.06092268 0.0508438  0.47256153 0.80004565]\n  [0.48706905 0.65704781 0.04297214 0.42288264]\n  [0.92286158 0.85394381 0.46423248 0.87896669]]\n\n [[0.216527   0.13880484 0.93482372 0.44812419]\n  [0.95553331 0.27084259 0.58913626 0.01879454]\n  [0.6656435  0.1985877  0.94133745 0.57504128]]]\n\n\n\n\nOutput is\n\n\n[[[ 0.7461933  -2.3189526  -1.454972   -0.7323345   1.5272427  -0.87963724  0.6278059  -0.23403725]]\n\n [[ 1.2397771  -0.9249111  -1.1432207  -0.92868984  0.53766745 -1.0271561  -0.9593589  -0.4768026 ]]]\n\n\n\n\n\n\nConvolution2D\n\n\nApplies a 2D convolution over an input image composed of several input planes.\n\n\nYou can also use \nConv2D\n as an alias of this layer.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nConvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsample = (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nConvolution2D(nb_filter, nb_row, nb_col, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nnbRow\n: Number of rows in the convolution kernel.\n\n\nnbCol\n: Number of columns in the convolution kernel.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\nsubsample\n: Length 2 corresponding to the step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Convolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Convolution2D(4, 2, 2, activation = \"relu\", inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.6687597  1.3452173   -1.9608531  -0.30892205\n1.7459077   -0.443617   0.25789636  0.44496542\n-1.5395774  -0.37713575 -0.9973955  0.16208267\n\n(1,2,.,.) =\n0.593965    1.0544858   -1.0765858  0.22257836\n0.69452614  1.3700147   -0.886259   0.013910895\n-1.9819669  0.32151425  1.8303248   0.24231844\n\n(2,1,.,.) =\n-2.150859   -1.5894475  0.7543173   0.7713991\n-0.17536041 0.89053404  0.50489277  -0.098128\n0.11551995  1.3663125   0.76734704  0.28318745\n\n(2,2,.,.) =\n-0.9801306  0.39797616  -0.6403248  1.0090133\n-0.16866015 -1.426308   -2.4097774  0.26011375\n-2.5700948  1.0486397   -0.4585798  -0.94231766\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.6800046   0.0\n1.0366663   0.235024    0.0\n\n(1,2,.,.) =\n0.0         0.84696645  0.0\n0.9099177   0.0         0.0\n\n(1,3,.,.) =\n0.122891426 0.0         0.86579126\n0.0         0.0         0.0\n\n(1,4,.,.) =\n0.0         0.7185988   0.0\n1.0967548   0.48376864  0.0\n\n(2,1,.,.) =\n0.0         0.0         0.29164955\n0.06815311  0.0         0.0\n\n(2,2,.,.) =\n0.36370438  0.42393038  0.26948324\n1.1676859   0.5698308   0.44842285\n\n(2,3,.,.) =\n1.0797265   1.2410768   0.18289843\n0.0         0.0         0.18757495\n\n(2,4,.,.) =\n0.0         0.35713753  0.0\n0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Convolution2D\n\nmodel = Sequential()\nmodel.add(Convolution2D(4, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.05182015 0.91971256 0.81030852 0.64093699]\n   [0.60282957 0.16269049 0.79136121 0.05202386]\n   [0.62560999 0.00174107 0.75762976 0.93589574]]\n  [[0.13728558 0.85812609 0.39695457 0.81678788]\n   [0.7569393  0.61161632 0.60750583 0.6222684 ]\n   [0.53094821 0.38715199 0.0087283  0.05758945]]]\n\n [[[0.50030948 0.40179766 0.54900785 0.60950401]\n   [0.17464329 0.01506322 0.55273153 0.21567461]\n   [0.09037649 0.58831638 0.818708   0.96642448]]\n  [[0.77126628 0.58039509 0.91612417 0.12578268]\n   [0.6095838  0.15802154 0.78099004 0.63619778]\n   [0.70632951 0.91378968 0.84851605 0.7242516 ]]]]\n\n\n\n\nOutput is\n\n\n[[[[-0.45239753  0.2432243  -0.02717562]\n   [ 0.2698849  -0.09664132  0.92311716]]\n  [[ 0.9092748   0.7945191   0.8834159 ]\n   [ 0.4853364   0.6511425   0.52513427]]\n  [[ 0.5550465   0.8177169   0.43213058]\n   [ 0.4209347   0.7514105   0.27255327]]\n  [[-0.22105691  0.02853963  0.01092601]\n   [ 0.1258291  -0.41649136 -0.18953061]]]\n\n [[[-0.12111888  0.06418754  0.26331317]\n   [ 0.41674113  0.04221775  0.7313505 ]]\n  [[ 0.49442202  0.6964868   0.558412  ]\n   [ 0.25196168  0.8145504   0.69307953]]\n  [[ 0.5885831   0.59289575  0.71726865]\n   [ 0.46759683  0.520353    0.59305453]]\n  [[ 0.00594708  0.09721318  0.07852311]\n   [ 0.49868047  0.02704304  0.14635414]]]]\n\n\n\n\n\n\nConvolution3D\n\n\nApplies convolution operator for filtering windows of three-dimensional inputs.\n\n\nYou can also use \nConv3D\n as an alias of this layer.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nConvolution3D(nbFilter, kernelDim1, kernelDim2, kernelDim3, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsample = (1, 1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nConvolution3D(nb_filter, kernel_dim1, kernel_dim2, kernel_dim3, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nkernelDim1\n: Length of the first dimension in the convolution kernel.\n\n\nkernelDim2\n: Length of the second dimension in the convolution kernel.\n\n\nkernelDim3\n: Length of the third dimension in the convolution kernel.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\nsubsample\n: Length 3. Factor by which to subsample output. Also called strides elsewhere. Default is (1, 1, 1).\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Convolution3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Convolution3D(4, 2, 2, 2, inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.64140224  -1.4049392  0.4935015\n0.33096266  1.2768826   -0.57567996\n\n(1,1,2,.,.) =\n0.49570087  -2.0367618    -0.0032108661\n-0.24242361 -0.092683665  1.1579652\n\n(1,2,1,.,.) =\n-0.6730608  0.9149566   -1.7478822\n-0.1763675  -0.90117735 0.38452747\n\n(1,2,2,.,.) =\n0.5314353   1.4802488   -1.196325\n0.43506134  -0.56575996 -1.5489199\n\n(2,1,1,.,.) =\n0.074545994 -1.4092928  -0.57647055\n1.9998664   -0.19424418 -0.9296713\n\n(2,1,2,.,.) =\n-0.42966184 0.9247804   -0.21713361\n0.2723336   -1.3024703  1.278154\n\n(2,2,1,.,.) =\n1.1240695   1.1061385   -2.4662287\n-0.36022148 0.1620907   -1.1525819\n\n(2,2,2,.,.) =\n0.9885768   -0.526637   -0.40684605\n0.37813842  0.53998697  1.0001947\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.36078936  -0.6334647\n\n(1,2,1,.,.) =\n-0.19685572 0.4559337\n\n(1,3,1,.,.) =\n1.3750207   -2.4377227\n\n(1,4,1,.,.) =\n-0.82853335 -0.74145436\n\n(2,1,1,.,.) =\n-0.17973013 1.2930126\n\n(2,2,1,.,.) =\n0.69144577  0.44385013\n\n(2,3,1,.,.) =\n-0.5597819  0.5965629\n\n(2,4,1,.,.) =\n0.89755684  -0.6737796\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x1x1x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Convolution3D\n\nmodel = Sequential()\nmodel.add(Convolution3D(4, 2, 2, 2, input_shape=(2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.36798873 0.92635561 0.31834968]\n    [0.09001392 0.66762381 0.64477164]]\n   [[0.09760993 0.38067132 0.15069965]\n    [0.39052699 0.0223722  0.04786307]]]\n  [[[0.2867726  0.3674255  0.11852931]\n    [0.96436629 0.8012903  0.3211012 ]]\n   [[0.81738622 0.80606827 0.4060485 ]\n    [0.68010177 0.0934071  0.98479026]]]]\n\n [[[[0.71624597 0.37754442 0.07367964]\n    [0.60742428 0.38549046 0.78880978]]\n   [[0.97844361 0.11426373 0.55479659]\n    [0.06395313 0.86007246 0.34004405]]]\n  [[[0.94149643 0.8027673  0.19478027]\n    [0.17437108 0.754479   0.51055297]]\n   [[0.81933677 0.09040694 0.33775061]\n    [0.02582059 0.40027544 0.91809986]]]]]\n\n\n\n\n\nOutput is\n\n\n[[[[[ 1.6276866  -4.4106215]]]\n  [[[ 6.6988254  1.1409638]]]\n  [[[-5.7734865  -5.2575850]]]\n  [[[-1.8073934  -4.4056013]]]]\n\n [[[[-4.8580116  9.4352424]]]\n  [[[ 7.8890514  6.6063654]]]\n  [[[-7.3165756  -1.0116580]]]\n  [[[-1.3100024  1.0475740]]]]]\n\n\n\n\n\n\nAtrousConvolution1D\n\n\nApplies an atrous convolution operator for filtering neighborhoods of 1-D inputs.\n\n\nA.k.a dilated convolution or convolution with holes.\n\n\nBias will be included in this layer.\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nYou can also use \nAtrousConv1D\n as an alias of this layer.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nAtrousConvolution1D(nbFilter, filterLength, init = \"glorot_uniform\", activation = null, subsampleLength = 1, atrousRate = 1, wRegularizer = null, bRegularizer = null, inputShape = null)\n\n\n\n\nPython:\n\n\nAtrousConvolution1D(nb_filter, filter_length, init=\"glorot_uniform\", activation=None, border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution kernels to use.\n\n\nfilterLength\n: The extension (spatial or temporal) of each filter.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nsubsampleLength\n: Factor by which to subsample output. Integer. Default is 1.\n\n\natrousRate\n: Factor for kernel dilation. Also called filter_dilation elsewhere. Integer. Default is 1.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, AtrousConvolution1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AtrousConvolution1D(8, 3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.18167362 2.1452308   -0.39164552 -0.19750737\n-0.16184713 -1.3867316  -1.3447738  -0.6431075\n-0.42635638 -0.20490816 -2.5391808  -0.05881459\n\n(2,.,.) =\n-0.83197606 1.1706954   -0.80197126 -1.0663458\n0.36859998  -0.45194706 -1.2959619  -0.521925\n-1.133602   0.7700087   -1.2523394  1.1293458\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.1675637   0.9712032   -0.5615059  0.065867506 0.6681816   1.2097323   -1.0912716  0.8040266\n\n(2,.,.) =\n0.5009172   1.4765333   -0.14173388 0.060548827 0.752389    1.2912648   -1.0077878  0.06344204\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AtrousConvolution1D\n\nmodel = Sequential()\nmodel.add(AtrousConvolution1D(8, 3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.96952262 0.15211776 0.63026888 0.50572335]\n  [0.13218867 0.18807126 0.33509675 0.43385223]\n  [0.48027981 0.82222524 0.9630902  0.78855421]]\n\n [[0.49106312 0.16875464 0.54099084 0.2892753 ]\n  [0.03776569 0.51324722 0.95359981 0.52863015]\n  [0.69851295 0.29676433 0.59404524 0.90078511]]]\n\n\n\n\nOutput is\n\n\n[[[-0.62304074  0.6667814  -0.07074605 -0.03640915  0.11369559  0.3451041  -0.44238544  0.618591  ]]\n\n [[-0.5048915   0.9070808  -0.03285386  0.26761323 -0.08491824  0.36105093 -0.15240929  0.6145356 ]]]\n\n\n\n\n\n\nAtrousConvolution2D\n\n\nApplies an atrous convolution operator for filtering windows of 2-D inputs.\n\n\nA.k.a dilated convolution or convolution with holes.\n\n\nBias will be included in this layer.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nYou can also use \nAtrousConv2D\n as an alias of this layer.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nAtrousConvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, subsample = (1, 1), atrousRate= (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, inputShape = null)\n\n\n\n\nPython:\n\n\nAtrousConvolution2D(nb_filter, nb_row, nb_col, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), atrous_rate=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nnbRow\n: Number of rows in the convolution kernel.\n\n\nnbCol\n: Number of columns in the convolution kernel.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nsubsample\n: Length 2 . Factor by which to subsample output. Also called strides elsewhere. Default is (1, 1).\n\n\natrousRate\n: Length 2. Factor for kernel dilation. Also called filter_dilation elsewhere. Default is (1, 1).\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, AtrousConvolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AtrousConvolution2D(4, 2, 2, activation = \"relu\", inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.57626903     -0.56916714     0.46516004      -1.189643\n-0.117406875    -1.1139084      1.115328        0.23275337\n1.452733        -0.30968842     -0.6693723      -0.22098665\n\n(1,2,.,.) =\n0.06541251      -0.7000564      -0.460471       -0.5291468\n-0.6625642      0.6460361       -0.556095       1.6327276\n1.1914377       -0.69054496     -0.7461783      -1.0129389\n\n(2,1,.,.) =\n-0.19123174     0.06803144      -0.010993495    -0.79966563\n-0.010654963    2.0806832       1.972848        -1.8525643\n-0.84387285     1.2974458       -0.42781293     0.3540522\n\n(2,2,.,.) =\n1.6231914       0.52689505      0.47506556      -1.030227\n0.5143046       -0.9930063      -2.2869735      0.03994834\n-1.5566326      -1.0937842      0.82693833      -0.08408405\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.11401264  0.0         1.1396459\n0.0         0.0         0.88493514\n\n(1,2,.,.) =\n0.0         8.398667    1.1495202\n0.0         0.0         0.1630742\n\n(1,3,.,.) =\n0.0         0.92470163  0.0\n0.0         0.6321572   0.0\n\n(1,4,.,.) =\n0.0         1.1912066   0.0\n0.0         1.27647     0.13422263\n\n(2,1,.,.) =\n0.0         0.0         0.51365596\n0.0         0.4207713   1.1226959\n\n(2,2,.,.) =\n0.0         0.67600054  0.63635653\n0.40892223  2.0596464   1.7690754\n\n(2,3,.,.) =\n1.1899394   0.0         0.0\n1.7185769   0.39178902  0.0\n\n(2,4,.,.) =\n0.44333076  0.73385376  0.0\n2.516453    0.36223468  0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AtrousConvolution2D\n\nmodel = Sequential()\nmodel.add(AtrousConvolution2D(4, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.52102612 0.30683086 0.38543426 0.0026452 ]\n   [0.66805249 0.60656045 0.94601998 0.46574414]\n   [0.49391338 0.14274225 0.70473703 0.30427041]]\n  [[0.89066007 0.51782675 0.7063052  0.53440807]\n   [0.67377917 0.51541465 0.02137767 0.63357007]\n   [0.6614106  0.15849977 0.94459604 0.46852022]]]\n\n [[[0.79639026 0.94468413 0.73165819 0.54531867]\n   [0.97741046 0.64477619 0.52373183 0.06861999]\n   [0.37278645 0.53198045 0.95098245 0.86249644]]\n  [[0.47186038 0.81694951 0.78009033 0.20925898]\n   [0.69942883 0.37150324 0.58907364 0.88754231]\n   [0.64083971 0.4480097  0.91716521 0.66808943]]]]\n\n\n\n\nOutput is\n\n\n[[[[-0.32139003  -0.34667802 -0.35534883]\n   [-0.09653517  -0.35052428 -0.09859636]]\n  [[-0.3138999   -0.5563417  -0.6694119 ]\n   [-0.03151364  0.35521197  0.31497604]]\n  [[-0.34939283  -0.7537081  -0.3939833 ]\n   [-0.25708836  0.06015673  -0.16755156]]\n  [[-0.04791902  0.02060626  -0.5639752 ]\n   [ 0.16054101  0.22528952  -0.02460545]]]\n\n [[[-0.13129832  -0.5262137   -0.12281597]\n   [-0.36988598  -0.5532047   -0.43338764]]\n  [[-0.21627764  -0.17562683  0.23560521]\n   [ 0.23035726  -0.03152001  -0.46413773]]\n  [[-0.63740283  -0.33359224  0.15731882]\n   [-0.12795202  -0.25798583  -0.5261132 ]]\n  [[-0.01759483  -0.07666921  -0.00890112]\n   [ 0.27595833  -0.14117064  -0.3357542 ]]]]\n\n\n\n\n\n\nDeconvolution2D\n\n\nTransposed convolution operator for filtering windows of 2-D inputs.\n\n\nThe need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has\nthe shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nYou can also use \nDeconv2D\n as an alias of this layer.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nDeconvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, subsample = (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nDeconvolution2D(nb_filter, nb_row, nb_col, output_shape, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of transposed convolution filters to use.\n\n\nnbRow\n: Number of rows in the transposed convolution kernel.\n\n\nnbCol\n: Number of columns in the transposed convolution kernel.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nsubsample\n: Length 2 . The step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Deconvolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Deconvolution2D(2, 2, 2, activation = \"relu\", inputShape = Shape(2, 2, 3)))\nval input = Tensor[Float](1, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.1157457  -0.8626509  -0.7326707\n1.8340882   -1.1647098  -1.0159439\n\n(1,2,.,.) =\n-0.13360074 0.4507607   -0.5922559\n0.15494606  0.16541296  1.6870573\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.0         0.0     0.020009547\n0.0         0.0         0.0     0.0\n0.9656998   0.0         0.0     0.5543601\n\n(1,2,.,.) =\n0.0         0.0         0.0     0.07773054\n1.4971795   0.029338006 0.0     0.0\n0.0         0.45826393  0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Deconvolution2D\n\nmodel = Sequential()\nmodel.add(Deconvolution2D(2, 2, 2, (2, 3, 4), input_shape=(2, 2, 3)))\ninput = np.random.random([1, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.65315139 0.21904901 0.57943617]\n   [0.35141043 0.14628658 0.81862311]]\n\n  [[0.60094717 0.84649884 0.08338504]\n   [0.26753695 0.83676038 0.87466877]]]]\n\n\n\n\nOutput is\n\n\n[[[[-0.35380065  0.22048733  0.3084591   0.23341973]\n   [-0.11611718  0.5349988  -0.26301163  1.0291481 ]\n   [ 0.00479569  0.48814884  0.00127316  0.2546792 ]]\n\n  [[-0.02683929 -0.21759698 -0.8542665  -0.25376737]\n   [ 0.04426606  0.05486238 -0.9282576  -1.1576774 ]\n   [ 0.01637976  0.1838439  -0.01419228 -0.60704494]]]]\n\n\n\n\n\n\nSeparableConvolution2D\n\n\nApplies separable convolution operator for 2D inputs.\n\n\nSeparable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depthMultiplier argument controls how many output channels are generated per input channel in the depthwise step.\n\n\nYou can also use \nSeparableConv2D\n as an alias of this layer.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nSeparableConvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsample = (1, 1), depthMultiplier = 1, dimOrdering = \"th\", depthwiseRegularizer = null, pointwiseRegularizer= null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nSeparableConvolution2D(nb_filter, nb_row, nb_col, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), depth_multiplier=1, dim_ordering=\"th\", depthwise_regularizer=None, pointwise_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nnbRow\n: Number of rows in the convolution kernel.\n\n\nnbCol\n: Number of columns in the convolution kernel.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\nsubsample\n: Length 2 corresponding to the step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).\n\n\ndepthMultiplier\n: How many output channel to use per input channel for the depthwise convolution step. Integer. Default is 1.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ndepthwiseRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the depthwise weights matrices. Default is null.\n\n\npointwiseRegularizer\n: An instance of \nRegularizer\n, applied to the pointwise weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, SeparableConvolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SeparableConvolution2D(2, 2, 2, activation = \"relu\", inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.61846036  0.13724488     1.9047198    0.8788536\n0.74383116  -0.7590018     0.17210509   1.8095028\n-0.21476124 -0.010768774   0.5437478    0.97470677\n\n(1,2,.,.) =\n-0.22464052 -1.7141389     1.8457758    0.81563693\n-0.17250067 -1.2183974     -2.5329974   -1.3014348\n0.43760046  0.32672745     -0.6059157   0.31439257\n\n(2,1,.,.) =\n-0.32413644 -0.1871411     -0.13821407  -0.16577224\n-0.02138366 1.2260025      -0.48404458  -1.0251912\n-1.8844653  0.6796752      -0.5881143   2.1656246\n\n(2,2,.,.) =\n0.17234507  -0.6455974     1.9615031    0.6552883\n-0.05861185 1.8847446      -0.857622    -0.5949971\n-0.41135395 -0.92089206    0.13154007   -0.9326055\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.0         0.0\n0.029595211 0.0         0.34002993\n\n(1,2,.,.) =\n0.0         0.0         0.0\n0.073145226 0.0         0.5542682\n\n(2,1,.,.) =\n0.4973382   0.36478913  0.0\n0.0         0.0         0.0\n\n(2,2,.,.) =\n0.9668598   0.7102739   0.0\n0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SeparableConvolution2D\n\nmodel = Sequential()\nmodel.add(SeparableConvolution2D(2, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.39277921 0.36904141 0.16768533 0.41712068]\n   [0.62416696 0.19334139 0.83341541 0.16486488]\n   [0.57287259 0.47809379 0.11103843 0.01746644]]\n  [[0.24945342 0.05728102 0.19076369 0.70498077]\n   [0.39147172 0.08100018 0.74426575 0.74251056]\n   [0.61840056 0.00771785 0.65170218 0.04492181]]]\n\n [[[0.08337509 0.19320791 0.66757918 0.38905916]\n   [0.50237454 0.0996316  0.3981495  0.32274897]\n   [0.01598124 0.52896577 0.76068351 0.10099803]]\n  [[0.20396797 0.48682425 0.11302674 0.57491998]\n   [0.71529612 0.11720466 0.57783092 0.45790133]\n   [0.41573101 0.60269287 0.613528   0.32717263]]]]\n\n\n\n\nOutput is\n\n\n[[[[0.15971108 0.12109925 0.17461367]\n   [0.20024002 0.13661252 0.1871847 ]]\n  [[0.47139192 0.36838844 0.45902973]\n   [0.57752806 0.41371965 0.5079273 ]]]\n\n [[[0.11111417 0.10702941 0.2030398 ]\n   [0.13108528 0.15029006 0.18544158]]\n  [[0.27002305 0.31479427 0.57750916]\n   [0.3573216  0.40100253 0.5122235 ]]]]\n\n\n\n\n\n\nLocallyConnected1D\n\n\nLocally-connected layer for 1D inputs which works similarly to the TemporalConvolution layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nLocallyConnected1D(nbFilter, filterLength, activation = null, subsampleLength = 1, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nLocallyConnected1D(nb_filter, filter_length, activation=None, border_mode=\"valid\", subsample_length=1, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Dimensionality of the output.\n\n\nfilterLength\n: The extension (spatial or temporal) of each filter.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nsubsampleLength\n: Integer. Factor by which to subsample output.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, LocallyConnected1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LocallyConnected1D(6, 3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.6755046   0.47923228  -0.41470557 -1.4644535\n-1.580751   -0.36924785 -1.1507624  0.20131736\n-0.4983051  -2.0898817  0.1623063   0.8118141\n\n(2,.,.) =\n1.5955191   -1.1017833  1.6614468   1.7959124\n1.1084127   0.528379    -1.114553   -1.030853\n0.37758648  -2.5828059  1.0172523   -1.6773314\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.20011228 0.7842446   -0.57892114 0.2405633   -0.35126245 -0.5116563\n\n(2,.,.) =\n-0.33687726 0.7863857   0.30202985  0.33251244  -0.7414977  0.14271683\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x6]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LocallyConnected1D\n\nmodel = Sequential()\nmodel.add(LocallyConnected1D(6, 3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.67992353 0.88287213 0.98861104 0.17401607]\n  [0.23660068 0.02779148 0.52982599 0.19876749]\n  [0.38880073 0.6498778  0.81532701 0.91719509]]\n\n [[0.30532677 0.1574227  0.40535271 0.03174637]\n  [0.37303714 0.27821415 0.02314422 0.64516966]\n  [0.74813923 0.9884225  0.40667151 0.21894944]]]\n\n\n\n\nOutput is\n\n\n[[[ 0.66351205 -0.03819168 -0.48071918 -0.05209085 -0.07307816  0.94942856]]\n\n [[ 0.5890693   0.0179258  -0.31232932  0.4427027  -0.30954808  0.4486028 ]]]\n\n\n\n\n\n\nLocallyConnected2D\n\n\nLocally-connected layer for 2D inputs that works similarly to the SpatialConvolution layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nLocallyConnected2D(nbFilter, nbRow, nbCol, activation = null, borderMode = \"valid\", subsample = (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nLocallyConnected2D(nb_filter, nb_row, nb_col, activation=None, border_mode=\"valid\", subsample=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nnbRow\n: Number of rows in the convolution kernel.\n\n\nnbCol\n: Number of columns in the convolution kernel.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\nsubsample\n: Length 2 corresponding to the step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, LocallyConnected2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LocallyConnected2D(2, 2, 2, inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.3119988       -1.8982307      -0.13138956     1.0872058\n-0.11329581     -0.7087005      0.085274234     -0.94051\n1.04928         2.1579344       -1.4412278      -0.90965116\n\n(1,2,.,.) =\n-0.6119555      1.2226686       -0.10441754     -1.6240023\n0.5598073       -0.099059306    -1.543586       0.72533834\n-1.6674699      -1.0901593      -0.24129404     0.30954796\n\n(2,1,.,.) =\n-0.78856885     -0.5567014      -1.1273636      -0.98069143\n-0.40949664     0.92562497      -1.3729718      0.7423901\n-0.29498738     -0.044669412    1.0937366       0.90768206\n\n(2,2,.,.) =\n1.0948726       -0.23575573     -0.051821854    -0.58692485\n1.9133459       -1.0849183      2.1423934       0.6559134\n-0.8390565      -0.27111387     -0.8439365      -1.3939567\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.42428172 0.25790718  -0.5227444\n0.6963143   -0.34605533 -0.35524538\n\n(1,2,.,.) =\n0.61758286  0.8430548   0.1378907\n0.24116383  0.15782532  0.16882366\n\n(2,1,.,.) =\n-0.5603108  0.5107949   -0.112701565\n0.62288725  0.6909297   -0.9253155\n\n(2,2,.,.) =\n-0.2443612  0.9310517   -0.2417406\n-0.82973266 -1.0886648  0.19112866\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LocallyConnected2D\n\nmodel = Sequential()\nmodel.add(LocallyConnected2D(2, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.57424593 0.49505236 0.63711108 0.43693806]\n   [0.34655799 0.0058394  0.69310344 0.70403367]\n   [0.4620432  0.58679338 0.64529398 0.78130808]]\n  [[0.49651564 0.32201482 0.02470762 0.80535793]\n   [0.94485185 0.07150504 0.58789497 0.4562848 ]\n   [0.63595033 0.04600271 0.89771801 0.95419454]]]\n\n [[[0.69641827 0.21785002 0.15815588 0.8317213 ]\n   [0.84192366 0.3939658  0.64309395 0.3858968 ]\n   [0.16545408 0.58533897 0.99486481 0.84651898]]\n  [[0.05144159 0.94930242 0.26842063 0.6341632 ]\n   [0.442836   0.38544902 0.04266468 0.22600452]\n   [0.2705393  0.07313841 0.24295287 0.9573069 ]]]]\n\n\n\n\nOutput is\n\n\n[[[[ 0.1600316   0.178018   -0.07472821]\n   [ 0.0570091  -0.19973318  0.44483435]]\n  [[-0.20258084 -0.37692443 -0.27103102]\n   [-0.624092   -0.09749079 -0.00799894]]]\n\n [[[ 0.58953685  0.35287908 -0.2203412 ]\n   [ 0.13649486 -0.29554832  0.16932982]]\n  [[-0.00787066 -0.06614903 -0.2027885 ]\n   [-0.33434835 -0.33458236 -0.15103136]]]]\n\n\n\n\n\n\nCropping1D\n\n\nCropping layer for 1D input (e.g. temporal sequence).\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nCropping1D(cropping = (1, 1), inputShape = null)\n\n\n\n\nPython:\n\n\nCropping1D(cropping=(1, 1), input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ncropping\n: Length 2. How many units should be trimmed off at the beginning and end of the cropping dimension. Default is (1, 1).\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Cropping1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Cropping1D((1, 1), inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.0038188  -0.75265634 0.7417358   1.0674809\n-1.4702164  0.64112693  0.17750219  -0.21439286\n-0.93766433 -1.0809567  0.7706962   0.16380796\n\n(2,.,.) =\n0.45019576  -0.36689326 0.08852628  -0.21602148\n0.66039973  0.11638404  0.062985964 -1.0420738\n0.46727908  -0.85894865 1.9853845   0.059447426\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-1.4702164  0.64112693  0.17750219  -0.21439286\n\n(2,.,.) =\n0.66039973  0.11638404  0.062985964 -1.0420738\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Cropping1D\n\nmodel = Sequential()\nmodel.add(Cropping1D(input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.01030651 0.77603525 0.97263208 0.15933375]\n  [0.05135971 0.01139832 0.28809891 0.57260363]\n  [0.28128354 0.55290954 0.77011153 0.09879061]]\n\n [[0.75765909 0.55102462 0.42426818 0.14383546]\n  [0.85198966 0.3990277  0.13061313 0.10349525]\n  [0.69892804 0.30310119 0.2241441  0.05978997]]]\n\n\n\n\nOutput is\n\n\n[[[0.05135971 0.01139832 0.2880989  0.57260364]]\n\n [[0.8519896  0.3990277  0.13061313 0.10349525]]]\n\n\n\n\n\n\nCropping2D\n\n\nCropping layer for 2D input (e.g. picture).\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nCropping2D(cropping = ((0, 0), (0, 0)), dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nCropping2D(cropping=((0, 0), (0, 0)), dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ncropping\n: Int tuple of tuple of length 2. How many units should be trimmed off at the beginning and end of the 2 cropping dimensions (i.e. height and width). Default is ((0, 0), (0, 0)).\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Cropping2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Cropping2D(((0, 1), (1, 0)), inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.3613406  -0.03520738  -0.008660733  2.1150143\n0.18087284  1.8787018    0.30097032   -2.5634677\n-1.9463011  -0.18772388  1.5215846    -0.8047026\n\n(1,2,.,.) =\n-0.50510925 -1.1193116   0.6901347   -0.2625669\n-0.24307655 -0.77917117  -0.566465   1.0432123\n0.4877474   0.49704018   -1.5550427  1.5772455\n\n(2,1,.,.) =\n-1.6180872  0.011832007  1.2762135   0.5600022\n1.9009352   -0.11096256  1.1500957   -0.26341736\n1.0153246   0.88008636   0.0560876   -1.0235065\n\n(2,2,.,.) =\n0.1036221   1.08527      -0.52559805   -0.5091204\n1.3085281   -0.96346164  -0.09713245   -1.1010116\n0.08505145  1.9413263    2.0237558     -0.5978173\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.03520738 -0.008660733  2.1150143\n1.8787018   0.30097032    -2.5634677\n\n(1,2,.,.) =\n-1.1193116   0.6901347  -0.2625669\n-0.77917117  -0.566465  1.0432123\n\n(2,1,.,.) =\n0.011832007  1.2762135  0.5600022\n-0.11096256  1.1500957  -0.26341736\n\n(2,2,.,.) =\n1.08527      -0.52559805   -0.5091204\n-0.96346164  -0.09713245   -1.1010116\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Cropping2D\n\nmodel = Sequential()\nmodel.add(Cropping2D(((0, 1), (1, 0)), input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.03691489 0.60233732 0.96327319 0.99561146]\n   [0.85728883 0.77923287 0.41328434 0.87490199]\n   [0.3389653  0.94804499 0.72922732 0.21191413]]\n  [[0.28962322 0.30133445 0.58516862 0.22476588]\n   [0.95386045 0.72488497 0.12056255 0.01265548]\n   [0.48645173 0.34426033 0.09410422 0.86815053]]]\n\n [[[0.57444115 0.79141167 0.20755353 0.38616465]\n   [0.95793123 0.22366943 0.5080078  0.27193368]\n   [0.65402317 0.1023231  0.67207896 0.2229965 ]]\n  [[0.04160647 0.55577895 0.30907277 0.42227706]\n   [0.54489229 0.90423796 0.50782414 0.51441165]\n   [0.87544565 0.47791071 0.0341273  0.14728084]]]]\n\n\n\n\nOutput is\n\n\n[[[[0.6023373  0.96327317 0.9956115 ]\n   [0.77923286 0.41328433 0.874902  ]]\n  [[0.30133444 0.5851686  0.22476588]\n   [0.724885   0.12056255 0.01265548]]]\n\n [[[0.7914117  0.20755354 0.38616467]\n   [0.22366942 0.5080078  0.27193367]]\n\n  [[0.555779   0.30907276 0.42227706]\n   [0.904238   0.5078241  0.5144116 ]]]]\n\n\n\n\n\n\nCropping3D\n\n\nCropping layer for 3D data (e.g. spatial or spatio-temporal).\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nCropping3D(cropping = ((1, 1), (1, 1), (1, 1)), dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nCropping3D(cropping=((1, 1), (1, 1), (1, 1)), dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ncropping\n: Int tuple of tuple of length 3. How many units should be trimmed off at the beginning and end of the 3 cropping dimensions (i.e. kernel_dim1, kernel_dim2 and kernel_dim3). Default is ((1, 1), (1, 1), (1, 1)).\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Cropping3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Cropping3D(((1, 1), (1, 1), (1, 1)), inputShape = Shape(2, 3, 4, 5)))\nval input = Tensor[Float](2, 2, 3, 4, 5).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-0.12339484     0.25661087      0.04387503      -1.1047344      -1.1413815\n1.1830065       -0.07189157     -0.5418846      0.5576781       -0.5460917\n-0.5679186      -0.30854696     1.2614665       -0.6774269      -0.63295823\n0.5269464       -2.7981617      -0.056265026    -1.0814936      -1.0848739\n\n(1,1,2,.,.) =\n-1.9100302      0.461067        0.4014941       0.60723174      -0.40414023\n0.34300476      0.7107094       1.3142885       1.5696589       0.97591686\n0.38320687      0.07036536      -0.43628898     0.58050656      -0.57882625\n-0.43699506     -0.0094956765   0.15171598      0.038076796     -1.2433665\n\n(1,1,3,.,.) =\n0.39671394      0.880047        0.30971292      -0.3369089      0.13062176\n-0.27803114     -0.62177086     0.16659822      0.89428085      0.23684736\n1.6151237       -1.1479733      -0.2229254      1.1361892       0.79478127\n-1.8207864      1.6544164       0.07977915      -1.1316417      -0.25483203\n\n(1,2,1,.,.) =\n1.3165517       -0.9479057      -1.4662051      -0.3343554      -0.4522552\n-1.5829691      0.6378519       -0.16399206     1.4724066       1.2387054\n-1.1467208      -0.6325814      -1.2106491      -0.035734158    0.19871919\n2.285004        1.0482147       -2.0056705      -0.80917794     2.523167\n\n(1,2,2,.,.) =\n-0.57108706     -0.23606259     -0.45569882     -0.034214735    -1.9130942\n-0.2743481      1.61177         -0.7052599      0.17889105      -0.31241596\n0.22377247      1.5860337       -0.3226252      -0.1341058      0.9239994\n0.03615294      0.6233593       0.757827        -0.72271305     0.9429943\n\n(1,2,3,.,.) =\n-0.4409662      0.8867786       2.0036085       0.16242673      -0.3332395\n0.09082064      0.04958198      -0.27834833     1.8025815       -0.04848101\n0.2690667       -1.1263227      -0.95486647     0.09473259      0.98166656\n-0.9509363      -0.10084029     -0.35410827     0.29626986      0.97203517\n\n(2,1,1,.,.) =\n0.42096403      0.14016314      0.20216857      -0.678293       -1.0970931\n-0.4981112      0.12429344      1.7156922       -0.24384527     -0.010780937\n0.03672217      2.3021698       1.568247        -0.43173146     -0.5550057\n0.30469602      1.4772439       -0.21195345     0.04221814      -1.6883365\n\n(2,1,2,.,.) =\n0.22468264      0.72787744      -0.9597003      -0.28472963     -1.4575284\n1.0487963       0.4982454       -1.0186157      -1.9877508      -1.133779\n0.17539643      -0.35151628     -1.8955303      2.1854792       0.59556997\n0.6893949       -0.19556235     0.25862908      0.24450152      0.17786922\n\n(2,1,3,.,.) =\n1.147159        -0.8849993      0.9826487       0.95360875      -0.9210176\n1.3439047       0.6739913       0.06558858      0.91963255      -1.1758618\n1.747105        -0.7225308      -1.0160877      0.67554474      -0.7762811\n0.21184689      -0.43668815     -1.0738864      0.04661594      0.9613895\n\n(2,2,1,.,.) =\n-0.377159       -0.28094378     0.1081715       1.3683178       1.2572801\n0.47781375      0.4545212       0.55356956      1.0366637       -0.1962683\n-1.820227       -0.111765414    1.9194998       -1.6089902      -1.6960226\n0.14896627      0.9360371       0.49156702      0.08601956      -0.08815153\n\n(2,2,2,.,.) =\n0.056315728     -0.13061485     -0.49018836     -0.59103477     -1.6910721\n-0.023719765    -0.44977355     0.11218439      0.224829        1.400084\n0.31496882      -1.6386473      -0.6715097      0.14816228      0.3240011\n-0.80607724     -0.37951842     -0.2187672      1.1087769       0.43044603\n\n(2,2,3,.,.) =\n-1.6647842      -0.5720825      -1.5150099      0.42346838      1.495052\n-0.3567161      -1.4341534      -0.19422509     -1.2871891      -1.2758921\n-0.47077888     -0.42217267     0.67764246      1.2170314       0.8420698\n-0.4263702      1.2792329       0.38645822      -2.4653213      -1.512707\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4x5]\n\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.7107094   1.3142885   1.5696589\n0.07036536  -0.43628898 0.58050656\n\n(1,2,1,.,.) =\n1.61177     -0.7052599  0.17889105\n1.5860337   -0.3226252  -0.1341058\n\n(2,1,1,.,.) =\n0.4982454   -1.0186157  -1.9877508\n-0.35151628 -1.8955303  2.1854792\n\n(2,2,1,.,.) =\n-0.44977355 0.11218439  0.224829\n-1.6386473  -0.6715097  0.14816228\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Cropping3D\n\nmodel = Sequential()\nmodel.add(Cropping3D(((1, 1), (1, 1), (1, 1)), input_shape=(2, 3, 4, 5)))\ninput = np.random.random([2, 2, 3, 4, 5])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.17398425 0.68189365 0.77769123 0.53108205 0.64715435]\n    [0.14553671 0.56312657 0.68612354 0.69176945 0.30109699]\n    [0.09732807 0.37460879 0.19945361 0.86471357 0.66225896]\n    [0.23071766 0.7940814  0.20828491 0.05256511 0.39059369]]\n   [[0.61604377 0.08752888 0.0373393  0.2074062  0.60620641]\n    [0.72873275 0.86871873 0.89248703 0.9407502  0.71830713]\n    [0.23277175 0.75968678 0.2160847  0.76278034 0.27796526]\n    [0.45593022 0.31406512 0.83030059 0.17528758 0.56134316]]\n   [[0.65576189 0.41055457 0.90979203 0.76003643 0.26369912]\n    [0.20767533 0.60489496 0.44996379 0.20016757 0.39282226]\n    [0.14055952 0.15767185 0.70149107 0.88403803 0.77345544]\n    [0.34344548 0.03721154 0.86204782 0.45349481 0.69348787]]]\n  [[[0.55441874 0.59949813 0.4450893  0.2103161  0.6300366 ]\n    [0.71573331 0.32423206 0.06302588 0.91902299 0.30852669]\n    [0.73540519 0.20697542 0.20543135 0.44461869 0.89286638]\n    [0.41614996 0.48155318 0.51663767 0.23681825 0.34780746]]\n   [[0.34529962 0.81156897 0.77911935 0.65392321 0.45178564]\n    [0.39702465 0.36180668 0.37867952 0.24818676 0.84365902]\n    [0.67836434 0.24043224 0.59870659 0.81976809 0.95442206]\n    [0.15342281 0.48607751 0.11420129 0.68621285 0.09892679]]\n   [[0.61122758 0.40359022 0.99805441 0.76764677 0.6281926 ]\n    [0.44867213 0.81206033 0.40117858 0.98967612 0.76897064]\n    [0.90603977 0.17299288 0.68803644 0.75164168 0.4161878 ]\n    [0.18996933 0.93317759 0.77711184 0.50760022 0.77439241]]]]\n\n [[[[0.49974828 0.74486599 0.12447392 0.15415173 0.36715309]\n    [0.49334423 0.66699219 0.22202136 0.52689596 0.15497081]\n    [0.4117844  0.21886979 0.13096058 0.82589121 0.00621519]\n    [0.38257617 0.60924058 0.53549974 0.64299846 0.66315369]]\n   [[0.78048895 0.20350694 0.16485496 0.71243727 0.4581091 ]\n    [0.554526   0.66891789 0.90082079 0.76729771 0.40647459]\n    [0.72809646 0.68164733 0.83008334 0.90941546 0.1441997 ]\n    [0.44580521 0.78015871 0.63982938 0.26813225 0.15588673]]\n   [[0.85294056 0.0928758  0.37056251 0.82930655 0.27178195]\n    [0.95953427 0.60170629 0.69156911 0.27902576 0.55613879]\n    [0.97101437 0.49876892 0.36313494 0.11233855 0.24221145]\n    [0.28739626 0.2990425  0.68940864 0.95621615 0.6922569 ]]]\n  [[[0.90283303 0.51320503 0.78356741 0.79301195 0.17681709]\n    [0.61624755 0.95418399 0.68118889 0.69241549 0.17943311]\n    [0.71129437 0.55478761 0.34121912 0.86018439 0.03652437]\n    [0.39098173 0.87916544 0.39647239 0.00104663 0.01377085]]\n   [[0.28875017 0.03733266 0.47260498 0.2896268  0.55976704]\n    [0.08723092 0.45523634 0.98463086 0.56950302 0.98261442]\n    [0.20716971 0.52744283 0.39455719 0.57384754 0.76698272]\n    [0.3079253  0.88143353 0.85897125 0.0969679  0.43760548]]\n   [[0.44239165 0.56141652 0.30344311 0.05425044 0.34003295]\n    [0.31417344 0.39485584 0.47300811 0.38006721 0.23185974]\n    [0.06158527 0.95330693 0.63043506 0.9480669  0.93758737]\n    [0.05340179 0.2064604  0.97254971 0.60841205 0.89738937]]]]]\n\n\n\n\nOutput is\n\n\n[[[[[0.86871874 0.89248705 0.9407502 ]\n    [0.75968677 0.2160847  0.7627803 ]]]\n  [[[0.3618067  0.3786795  0.24818675]\n    [0.24043223 0.5987066  0.8197681 ]]]]\n\n [[[[0.6689179  0.9008208  0.7672977 ]\n    [0.68164736 0.8300834  0.9094155 ]]]\n  [[[0.45523635 0.9846309  0.569503  ]\n    [0.5274428  0.39455718 0.57384753]]]]]\n\n\n\n\n\n\nZeroPadding1D\n\n\nZero-padding layer for 1D input (e.g. temporal sequence).\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nZeroPadding1D(padding = 1, inputShape = null)\n\n\n\n\nPython:\n\n\nZeroPadding1D(padding=1, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npadding\n: How many zeros to add at the beginning and at the end of the padding dimension.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, ZeroPadding1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ZeroPadding1D(1, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.7421485   -0.13270181 -0.12605186 -0.7442475\n0.36977226  -0.90300065 -0.34193754 -0.035565257\n-0.23300397 0.8183156   0.7023575   -0.16938858\n\n(2,.,.) =\n-0.7785278  0.36642975  -1.0542017  -0.29036212\n-0.22632122 0.46808097  -0.68293047 1.2529073\n-0.8619831  1.3846883   1.0762612   1.1351995\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.0         0.0         0.0         0.0\n0.7421485   -0.13270181 -0.12605186 -0.7442475\n0.36977226  -0.90300065 -0.34193754 -0.035565257\n-0.23300397 0.8183156   0.7023575   -0.16938858\n0.0         0.0         0.0         0.0\n\n(2,.,.) =\n0.0         0.0         0.0         0.0\n-0.7785278  0.36642975  -1.0542017  -0.29036212\n-0.22632122 0.46808097  -0.68293047 1.2529073\n-0.8619831  1.3846883   1.0762612   1.1351995\n0.0         0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ZeroPadding1D\n\nmodel = Sequential()\nmodel.add(ZeroPadding1D(1, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.74177145 0.75805981 0.2091588  0.46929227]\n  [0.46041743 0.13213793 0.51065024 0.36081853]\n  [0.60803218 0.27764702 0.31788482 0.65445294]]\n\n [[0.96255443 0.74692762 0.50050961 0.88456158]\n  [0.55492653 0.50850271 0.17788885 0.91569285]\n  [0.27356035 0.74622588 0.39690752 0.75229177]]]\n\n\n\n\nOutput is\n\n\n[[[0.0        0.0        0.0        0.0       ]\n  [0.74177146 0.7580598  0.2091588  0.46929225]\n  [0.46041742 0.13213794 0.5106502  0.36081854]\n  [0.60803217 0.27764702 0.31788483 0.6544529 ]\n  [0.0        0.0        0.0        0.0       ]]\n\n [[0.0        0.0        0.0        0.0       ]\n  [0.96255445 0.7469276  0.5005096  0.8845616 ]\n  [0.5549265  0.5085027  0.17788884 0.91569287]\n  [0.27356035 0.7462259  0.39690754 0.75229174]\n  [0.0        0.0        0.0        0.0       ]]]\n\n\n\n\n\n\nZeroPadding2D\n\n\nZero-padding layer for 2D input (e.g. picture).\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nZeroPadding2D(padding = (1, 1), dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nZeroPadding2D(padding=(1, 1), dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npadding\n: How many zeros to add at the beginning and at the end of the 2 padding dimensions (rows and cols).\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, ZeroPadding2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ZeroPadding2D((1, 1), inputShape = Shape(2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.7201442   -1.0197405  1.3163399\n-0.23921064 0.7732504   -0.069928266\n\n(1,2,.,.) =\n0.46323594  -1.3043984  -0.67622787\n-1.610615   -0.39253974 -0.89652705\n\n(2,1,.,.) =\n-0.3784847  -0.6738694  0.30479854\n-0.49577644 1.0704983   0.6288544\n\n(2,2,.,.) =\n0.2821439   0.790223    0.34665197\n0.24190207  0.10775433  0.46225727\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     1.7201442   -1.0197405  1.3163399       0.0\n0.0     -0.23921064 0.7732504   -0.069928266    0.0\n0.0     0.0         0.0         0.0             0.0\n\n(1,2,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     0.46323594  -1.3043984  -0.67622787     0.0\n0.0     -1.610615   -0.39253974 -0.89652705     0.0\n0.0     0.0         0.0         0.0             0.0\n\n(2,1,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     -0.3784847  -0.6738694  0.30479854      0.0\n0.0     -0.49577644 1.0704983   0.6288544       0.0\n0.0     0.0         0.0         0.0             0.0\n\n(2,2,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     0.2821439   0.790223    0.34665197      0.0\n0.0     0.24190207  0.10775433  0.46225727      0.0\n0.0     0.0         0.0         0.0             0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x4x5]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ZeroPadding2D\n\nmodel = Sequential()\nmodel.add(ZeroPadding2D(input_shape=(2, 2, 3)))\ninput = np.random.random([2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.44048214 0.72494886 0.96654241]\n   [0.66254801 0.37409083 0.47681466]]\n\n  [[0.23204026 0.52762765 0.15072852]\n   [0.45052127 0.29016392 0.0133929 ]]]\n\n\n [[[0.09347565 0.4754528  0.63618458]\n   [0.08016674 0.21696158 0.83892852]]\n\n  [[0.81864575 0.90813398 0.08347963]\n   [0.57234761 0.76060611 0.65707858]]]]\n\n\n\n\nOutput is\n\n\n[[[[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.44048214 0.7249489  0.9665424  0.0 ]\n   [0.0   0.662548   0.37409082 0.47681466 0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]\n\n  [[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.23204026 0.52762765 0.15072852 0.0 ]\n   [0.0   0.45052126 0.29016393 0.0133929  0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]]\n\n\n [[[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.09347565 0.4754528  0.6361846  0.0 ]\n   [0.0   0.08016673 0.21696158 0.8389285  0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]\n\n  [[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.8186458  0.908134   0.08347963 0.0 ]\n   [0.0   0.5723476  0.7606061  0.65707856 0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]]]\n\n\n\n\n\n\nZeroPadding3D\n\n\nZero-padding layer for 3D data (spatial or spatio-temporal).\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nZeroPadding3D(padding = (1, 1, 1), dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nZeroPadding3D(padding=(1, 1, 1), dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npadding\n: How many zeros to add at the beginning and end of the 3 padding dimensions. Symmetric padding will be applied to each dimension.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, ZeroPadding3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ZeroPadding3D((1, 1, 1), inputShape = Shape(1, 2, 2, 2)))\nval input = Tensor[Float](1, 1, 2, 2, 2).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n1.086798    2.162806\n-0.50501716 -0.17430544\n\n(1,1,2,.,.) =\n-1.7388326  0.27966997\n1.6211525   1.1713351\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2x2]\n\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n\n(1,1,2,.,.) =\n0.0     0.0         0.0         0.0\n0.0     1.086798    2.162806    0.0\n0.0     -0.50501716 -0.17430544 0.0\n0.0     0.0         0.0         0.0\n\n(1,1,3,.,.) =\n0.0     0.0         0.0         0.0\n0.0     -1.7388326  0.27966997  0.0\n0.0     1.6211525   1.1713351   0.0\n0.0     0.0         0.0         0.0\n\n(1,1,4,.,.) =\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x4x4x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ZeroPadding3D\n\nmodel = Sequential()\nmodel.add(ZeroPadding3D((1, 1, 1), input_shape=(1, 2, 2, 2)))\ninput = np.random.random([1, 1, 2, 2, 2])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.12432462 0.19244616]\n    [0.39039533 0.88140855]]\n\n   [[0.71426182 0.86085132]\n    [0.04443494 0.679125  ]]]]]\n\n\n\n\nOutput is\n\n\n[[[[[0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]]\n   [[0.0   0.0        0.0        0.0 ]\n    [0.0   0.12432462 0.19244616 0.0 ]\n    [0.0   0.39039534 0.8814086  0.0 ]\n    [0.0   0.0        0.0        0.0 ]]\n   [[0.0   0.0        0.0        0.0 ]\n    [0.0   0.71426183 0.8608513  0.0 ]\n    [0.0   0.04443494 0.679125   0.0 ]\n    [0.0   0.0        0.0        0.0 ]]\n   [[0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]]]]]\n\n\n\n\n\n\nUpSampling1D\n\n\nUpSampling layer for 1D inputs.\n\n\nRepeats each temporal step 'length' times along the time axis.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nUpSampling1D(length = 2, inputShape = null)\n\n\n\n\nPython:\n\n\nUpSampling1D(length=2, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nlength\n: Integer. UpSampling factor. Default is 2.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, UpSampling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(UpSampling1D(2, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.80225134  -0.9644977   -0.71038723    -1.5673652\n0.67224514  -0.24330814  -0.082499735   0.2807591\n-0.9299857  -1.8893008   -1.1062661     -1.1637908\n\n(2,.,.) =\n-0.1831344  -0.6621819   -0.667329      -0.26960346\n-0.6601015  1.0819869    1.0307902      1.1801233\n-0.18303517 0.2565441    -0.39598823    0.23400643\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.80225134  -0.9644977   -0.71038723     -1.5673652\n0.80225134  -0.9644977   -0.71038723     -1.5673652\n0.67224514  -0.24330814  -0.082499735    0.2807591\n0.67224514  -0.24330814  -0.082499735    0.2807591\n-0.9299857  -1.8893008   -1.1062661     -1.1637908\n-0.9299857  -1.8893008   -1.1062661     -1.1637908\n\n(2,.,.) =\n-0.1831344  -0.6621819   -0.667329      -0.26960346\n-0.1831344  -0.6621819   -0.667329      -0.26960346\n-0.6601015  1.0819869    1.0307902      1.1801233\n-0.6601015  1.0819869    1.0307902      1.1801233\n-0.18303517 0.2565441    -0.39598823    0.23400643\n-0.18303517 0.2565441    -0.39598823    0.23400643\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import UpSampling1D\n\nmodel = Sequential()\nmodel.add(UpSampling1D(2, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.66227662 0.00663032 0.49010329 0.26836567]\n  [0.34225774 0.26000732 0.27628499 0.49861887]\n  [0.11619066 0.28123766 0.60770962 0.80773197]]\n\n [[0.477639   0.88906297 0.38577295 0.99058504]\n  [0.50690837 0.38107999 0.05881034 0.96402145]\n  [0.42226283 0.77350512 0.54961295 0.55315271]]]\n\n\n\n\nOutput is\n\n\n[[[0.6622766  0.00663032 0.4901033  0.26836568]\n  [0.6622766  0.00663032 0.4901033  0.26836568]\n  [0.34225774 0.26000732 0.276285   0.49861887]\n  [0.34225774 0.26000732 0.276285   0.49861887]\n  [0.11619066 0.28123766 0.60770965 0.807732  ]\n  [0.11619066 0.28123766 0.60770965 0.807732  ]]\n\n [[0.477639   0.88906294 0.38577294 0.990585  ]\n  [0.477639   0.88906294 0.38577294 0.990585  ]\n  [0.50690836 0.38107997 0.05881034 0.96402144]\n  [0.50690836 0.38107997 0.05881034 0.96402144]\n  [0.42226282 0.7735051  0.54961294 0.5531527 ]\n  [0.42226282 0.7735051  0.54961294 0.5531527 ]]]\n\n\n\n\n\n\nUpSampling2D\n\n\nUpSampling layer for 2D inputs.\n\n\nRepeats the rows and columns of the data by the specified size.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nUpSampling2D(size = (2, 2), dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nUpSampling2D(size=(2, 2), dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nsize\n: Length 2. UpSampling factors for rows and columns. Default is (2, 2).\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, UpSampling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(UpSampling2D((2, 2), inputShape = Shape(2, 2, 2)))\nval input = Tensor[Float](1, 2, 2, 2).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.07563081 -1.921836\n-1.7368479  0.1043008\n\n(1,2,.,.) =\n-1.825055   -0.096810855\n-0.89331573 0.72812295\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.07563081 -0.07563081  -1.921836   -1.921836\n-0.07563081 -0.07563081  -1.921836   -1.921836\n-1.7368479  -1.7368479   0.1043008   0.1043008\n-1.7368479  -1.7368479   0.1043008   0.1043008\n\n(1,2,.,.) =\n-1.825055    -1.825055    -0.096810855  -0.096810855\n-1.825055    -1.825055    -0.096810855  -0.096810855\n-0.89331573  -0.89331573  0.72812295    0.72812295\n-0.89331573  -0.89331573  0.72812295    0.72812295\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import UpSampling2D\n\nmodel = Sequential()\nmodel.add(UpSampling2D((2, 2), input_shape=(2, 2, 2)))\ninput = np.random.random([1, 2, 2, 2])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.55660253 0.21984387]\n   [0.36271854 0.57464162]]\n\n  [[0.55307278 0.33007518]\n   [0.31527167 0.87789644]]]]\n\n\n\n\nOutput is\n\n\n[[[[0.55660254 0.55660254 0.21984388 0.21984388]\n   [0.55660254 0.55660254 0.21984388 0.21984388]\n   [0.36271855 0.36271855 0.57464164 0.57464164]\n   [0.36271855 0.36271855 0.57464164 0.57464164]]\n\n  [[0.55307275 0.55307275 0.33007517 0.33007517]\n   [0.55307275 0.55307275 0.33007517 0.33007517]\n   [0.31527168 0.31527168 0.8778964  0.8778964 ]\n   [0.31527168 0.31527168 0.8778964  0.8778964 ]]]]\n\n\n\n\n\n\nUpSampling3D\n\n\nUpSampling layer for 3D inputs.\n\n\nRepeats the 1st, 2nd and 3rd dimensions of the data by the specified size.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nUpSampling3D(size = (2, 2, 2), dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nUpSampling3D(size=(2, 2, 2), dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nsize\n: Length 3. UpSampling factors for dim1, dim2 and dim3. Default is (2, 2, 2).\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, UpSampling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(UpSampling3D((2, 2, 2), inputShape = Shape(1, 1, 2, 2)))\nval input = Tensor[Float](1, 1, 1, 2, 2).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.9906968   -0.2451235\n1.5133694   -0.34076887\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1x2x2]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.9906968   0.9906968   -0.2451235  -0.2451235\n0.9906968   0.9906968   -0.2451235  -0.2451235\n1.5133694   1.5133694   -0.34076887 -0.34076887\n1.5133694   1.5133694   -0.34076887 -0.34076887\n\n(1,1,2,.,.) =\n0.9906968   0.9906968   -0.2451235  -0.2451235\n0.9906968   0.9906968   -0.2451235  -0.2451235\n1.5133694   1.5133694   -0.34076887 -0.34076887\n1.5133694   1.5133694   -0.34076887 -0.34076887\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x4x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import UpSampling3D\n\nmodel = Sequential()\nmodel.add(UpSampling3D((2, 2, 2), input_shape=(1, 1, 2, 2)))\ninput = np.random.random([1, 1, 1, 2, 2])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.58361205 0.2096227 ]\n    [0.51686662 0.70260105]]]]]\n\n\n\n\nOutput is\n\n\n[[[[[0.583612  0.583612  0.2096227 0.2096227]\n    [0.583612  0.583612  0.2096227 0.2096227]\n    [0.5168666 0.5168666 0.7026011 0.7026011]\n    [0.5168666 0.5168666 0.7026011 0.7026011]]\n\n   [[0.583612  0.583612  0.2096227 0.2096227]\n    [0.583612  0.583612  0.2096227 0.2096227]\n    [0.5168666 0.5168666 0.7026011 0.7026011]\n    [0.5168666 0.5168666 0.7026011 0.7026011]]]]]",
            "title": "Convolutional Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#convolution1d",
            "text": "Applies convolution operator for filtering neighborhoods of 1-D inputs.  You can also use  Conv1D  as an alias of this layer.  The input of this layer should be 3D.  Scala:  Convolution1D(nbFilter, filterLength, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsampleLength = 1, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  Convolution1D(nb_filter, filter_length, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample_length=1, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  filterLength : The extension (spatial or temporal) of each filter.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  subsampleLength : Factor by which to subsample output. Integer. Default is 1.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Convolution1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Convolution1D(8, 3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.4253887  -0.044403594    -1.1169672  -0.19499049\n0.85463065  0.6665206       0.21340805  0.56255895\n1.1126599   -0.3423326      0.09643264  -0.34345046\n\n(2,.,.) =\n-0.04046587 -0.2710401      0.10183265  1.4503858\n1.0639644   1.5317003       -0.18313104 -0.7098296\n0.612399    1.7357533       0.4641411   0.13530721\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.22175728  0.76192796  1.7907748   1.1534728   -1.5304534  0.07466106  -0.18292685 0.6038852\n\n(2,.,.) =\n0.85337734  0.43939286  -0.16770163 -0.8380078  0.7825804   -0.3485601  0.3017909   0.5823619\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Convolution1D\n\nmodel = Sequential()\nmodel.add(Convolution1D(8, 3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.06092268 0.0508438  0.47256153 0.80004565]\n  [0.48706905 0.65704781 0.04297214 0.42288264]\n  [0.92286158 0.85394381 0.46423248 0.87896669]]\n\n [[0.216527   0.13880484 0.93482372 0.44812419]\n  [0.95553331 0.27084259 0.58913626 0.01879454]\n  [0.6656435  0.1985877  0.94133745 0.57504128]]]  Output is  [[[ 0.7461933  -2.3189526  -1.454972   -0.7323345   1.5272427  -0.87963724  0.6278059  -0.23403725]]\n\n [[ 1.2397771  -0.9249111  -1.1432207  -0.92868984  0.53766745 -1.0271561  -0.9593589  -0.4768026 ]]]",
            "title": "Convolution1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#convolution2d",
            "text": "Applies a 2D convolution over an input image composed of several input planes.  You can also use  Conv2D  as an alias of this layer.  The input of this layer should be 4D.  Scala:  Convolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsample = (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  Convolution2D(nb_filter, nb_row, nb_col, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  nbRow : Number of rows in the convolution kernel.  nbCol : Number of columns in the convolution kernel.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  subsample : Length 2 corresponding to the step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Convolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Convolution2D(4, 2, 2, activation = \"relu\", inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.6687597  1.3452173   -1.9608531  -0.30892205\n1.7459077   -0.443617   0.25789636  0.44496542\n-1.5395774  -0.37713575 -0.9973955  0.16208267\n\n(1,2,.,.) =\n0.593965    1.0544858   -1.0765858  0.22257836\n0.69452614  1.3700147   -0.886259   0.013910895\n-1.9819669  0.32151425  1.8303248   0.24231844\n\n(2,1,.,.) =\n-2.150859   -1.5894475  0.7543173   0.7713991\n-0.17536041 0.89053404  0.50489277  -0.098128\n0.11551995  1.3663125   0.76734704  0.28318745\n\n(2,2,.,.) =\n-0.9801306  0.39797616  -0.6403248  1.0090133\n-0.16866015 -1.426308   -2.4097774  0.26011375\n-2.5700948  1.0486397   -0.4585798  -0.94231766\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.6800046   0.0\n1.0366663   0.235024    0.0\n\n(1,2,.,.) =\n0.0         0.84696645  0.0\n0.9099177   0.0         0.0\n\n(1,3,.,.) =\n0.122891426 0.0         0.86579126\n0.0         0.0         0.0\n\n(1,4,.,.) =\n0.0         0.7185988   0.0\n1.0967548   0.48376864  0.0\n\n(2,1,.,.) =\n0.0         0.0         0.29164955\n0.06815311  0.0         0.0\n\n(2,2,.,.) =\n0.36370438  0.42393038  0.26948324\n1.1676859   0.5698308   0.44842285\n\n(2,3,.,.) =\n1.0797265   1.2410768   0.18289843\n0.0         0.0         0.18757495\n\n(2,4,.,.) =\n0.0         0.35713753  0.0\n0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Convolution2D\n\nmodel = Sequential()\nmodel.add(Convolution2D(4, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.05182015 0.91971256 0.81030852 0.64093699]\n   [0.60282957 0.16269049 0.79136121 0.05202386]\n   [0.62560999 0.00174107 0.75762976 0.93589574]]\n  [[0.13728558 0.85812609 0.39695457 0.81678788]\n   [0.7569393  0.61161632 0.60750583 0.6222684 ]\n   [0.53094821 0.38715199 0.0087283  0.05758945]]]\n\n [[[0.50030948 0.40179766 0.54900785 0.60950401]\n   [0.17464329 0.01506322 0.55273153 0.21567461]\n   [0.09037649 0.58831638 0.818708   0.96642448]]\n  [[0.77126628 0.58039509 0.91612417 0.12578268]\n   [0.6095838  0.15802154 0.78099004 0.63619778]\n   [0.70632951 0.91378968 0.84851605 0.7242516 ]]]]  Output is  [[[[-0.45239753  0.2432243  -0.02717562]\n   [ 0.2698849  -0.09664132  0.92311716]]\n  [[ 0.9092748   0.7945191   0.8834159 ]\n   [ 0.4853364   0.6511425   0.52513427]]\n  [[ 0.5550465   0.8177169   0.43213058]\n   [ 0.4209347   0.7514105   0.27255327]]\n  [[-0.22105691  0.02853963  0.01092601]\n   [ 0.1258291  -0.41649136 -0.18953061]]]\n\n [[[-0.12111888  0.06418754  0.26331317]\n   [ 0.41674113  0.04221775  0.7313505 ]]\n  [[ 0.49442202  0.6964868   0.558412  ]\n   [ 0.25196168  0.8145504   0.69307953]]\n  [[ 0.5885831   0.59289575  0.71726865]\n   [ 0.46759683  0.520353    0.59305453]]\n  [[ 0.00594708  0.09721318  0.07852311]\n   [ 0.49868047  0.02704304  0.14635414]]]]",
            "title": "Convolution2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#convolution3d",
            "text": "Applies convolution operator for filtering windows of three-dimensional inputs.  You can also use  Conv3D  as an alias of this layer.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  The input of this layer should be 5D.  Scala:  Convolution3D(nbFilter, kernelDim1, kernelDim2, kernelDim3, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsample = (1, 1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  Convolution3D(nb_filter, kernel_dim1, kernel_dim2, kernel_dim3, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  kernelDim1 : Length of the first dimension in the convolution kernel.  kernelDim2 : Length of the second dimension in the convolution kernel.  kernelDim3 : Length of the third dimension in the convolution kernel.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  subsample : Length 3. Factor by which to subsample output. Also called strides elsewhere. Default is (1, 1, 1).  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Convolution3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Convolution3D(4, 2, 2, 2, inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.64140224  -1.4049392  0.4935015\n0.33096266  1.2768826   -0.57567996\n\n(1,1,2,.,.) =\n0.49570087  -2.0367618    -0.0032108661\n-0.24242361 -0.092683665  1.1579652\n\n(1,2,1,.,.) =\n-0.6730608  0.9149566   -1.7478822\n-0.1763675  -0.90117735 0.38452747\n\n(1,2,2,.,.) =\n0.5314353   1.4802488   -1.196325\n0.43506134  -0.56575996 -1.5489199\n\n(2,1,1,.,.) =\n0.074545994 -1.4092928  -0.57647055\n1.9998664   -0.19424418 -0.9296713\n\n(2,1,2,.,.) =\n-0.42966184 0.9247804   -0.21713361\n0.2723336   -1.3024703  1.278154\n\n(2,2,1,.,.) =\n1.1240695   1.1061385   -2.4662287\n-0.36022148 0.1620907   -1.1525819\n\n(2,2,2,.,.) =\n0.9885768   -0.526637   -0.40684605\n0.37813842  0.53998697  1.0001947\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.36078936  -0.6334647\n\n(1,2,1,.,.) =\n-0.19685572 0.4559337\n\n(1,3,1,.,.) =\n1.3750207   -2.4377227\n\n(1,4,1,.,.) =\n-0.82853335 -0.74145436\n\n(2,1,1,.,.) =\n-0.17973013 1.2930126\n\n(2,2,1,.,.) =\n0.69144577  0.44385013\n\n(2,3,1,.,.) =\n-0.5597819  0.5965629\n\n(2,4,1,.,.) =\n0.89755684  -0.6737796\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x1x1x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Convolution3D\n\nmodel = Sequential()\nmodel.add(Convolution3D(4, 2, 2, 2, input_shape=(2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[[0.36798873 0.92635561 0.31834968]\n    [0.09001392 0.66762381 0.64477164]]\n   [[0.09760993 0.38067132 0.15069965]\n    [0.39052699 0.0223722  0.04786307]]]\n  [[[0.2867726  0.3674255  0.11852931]\n    [0.96436629 0.8012903  0.3211012 ]]\n   [[0.81738622 0.80606827 0.4060485 ]\n    [0.68010177 0.0934071  0.98479026]]]]\n\n [[[[0.71624597 0.37754442 0.07367964]\n    [0.60742428 0.38549046 0.78880978]]\n   [[0.97844361 0.11426373 0.55479659]\n    [0.06395313 0.86007246 0.34004405]]]\n  [[[0.94149643 0.8027673  0.19478027]\n    [0.17437108 0.754479   0.51055297]]\n   [[0.81933677 0.09040694 0.33775061]\n    [0.02582059 0.40027544 0.91809986]]]]]  Output is  [[[[[ 1.6276866  -4.4106215]]]\n  [[[ 6.6988254  1.1409638]]]\n  [[[-5.7734865  -5.2575850]]]\n  [[[-1.8073934  -4.4056013]]]]\n\n [[[[-4.8580116  9.4352424]]]\n  [[[ 7.8890514  6.6063654]]]\n  [[[-7.3165756  -1.0116580]]]\n  [[[-1.3100024  1.0475740]]]]]",
            "title": "Convolution3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#atrousconvolution1d",
            "text": "Applies an atrous convolution operator for filtering neighborhoods of 1-D inputs.  A.k.a dilated convolution or convolution with holes.  Bias will be included in this layer.  Border mode currently supported for this layer is 'valid'.  You can also use  AtrousConv1D  as an alias of this layer.  The input of this layer should be 3D.  Scala:  AtrousConvolution1D(nbFilter, filterLength, init = \"glorot_uniform\", activation = null, subsampleLength = 1, atrousRate = 1, wRegularizer = null, bRegularizer = null, inputShape = null)  Python:  AtrousConvolution1D(nb_filter, filter_length, init=\"glorot_uniform\", activation=None, border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution kernels to use.  filterLength : The extension (spatial or temporal) of each filter.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  subsampleLength : Factor by which to subsample output. Integer. Default is 1.  atrousRate : Factor for kernel dilation. Also called filter_dilation elsewhere. Integer. Default is 1.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, AtrousConvolution1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AtrousConvolution1D(8, 3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.18167362 2.1452308   -0.39164552 -0.19750737\n-0.16184713 -1.3867316  -1.3447738  -0.6431075\n-0.42635638 -0.20490816 -2.5391808  -0.05881459\n\n(2,.,.) =\n-0.83197606 1.1706954   -0.80197126 -1.0663458\n0.36859998  -0.45194706 -1.2959619  -0.521925\n-1.133602   0.7700087   -1.2523394  1.1293458\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.1675637   0.9712032   -0.5615059  0.065867506 0.6681816   1.2097323   -1.0912716  0.8040266\n\n(2,.,.) =\n0.5009172   1.4765333   -0.14173388 0.060548827 0.752389    1.2912648   -1.0077878  0.06344204\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AtrousConvolution1D\n\nmodel = Sequential()\nmodel.add(AtrousConvolution1D(8, 3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.96952262 0.15211776 0.63026888 0.50572335]\n  [0.13218867 0.18807126 0.33509675 0.43385223]\n  [0.48027981 0.82222524 0.9630902  0.78855421]]\n\n [[0.49106312 0.16875464 0.54099084 0.2892753 ]\n  [0.03776569 0.51324722 0.95359981 0.52863015]\n  [0.69851295 0.29676433 0.59404524 0.90078511]]]  Output is  [[[-0.62304074  0.6667814  -0.07074605 -0.03640915  0.11369559  0.3451041  -0.44238544  0.618591  ]]\n\n [[-0.5048915   0.9070808  -0.03285386  0.26761323 -0.08491824  0.36105093 -0.15240929  0.6145356 ]]]",
            "title": "AtrousConvolution1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#atrousconvolution2d",
            "text": "Applies an atrous convolution operator for filtering windows of 2-D inputs.  A.k.a dilated convolution or convolution with holes.  Bias will be included in this layer.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'valid'.  You can also use  AtrousConv2D  as an alias of this layer.  The input of this layer should be 4D.  Scala:  AtrousConvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, subsample = (1, 1), atrousRate= (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, inputShape = null)  Python:  AtrousConvolution2D(nb_filter, nb_row, nb_col, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), atrous_rate=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  nbRow : Number of rows in the convolution kernel.  nbCol : Number of columns in the convolution kernel.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  subsample : Length 2 . Factor by which to subsample output. Also called strides elsewhere. Default is (1, 1).  atrousRate : Length 2. Factor for kernel dilation. Also called filter_dilation elsewhere. Default is (1, 1).  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, AtrousConvolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AtrousConvolution2D(4, 2, 2, activation = \"relu\", inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.57626903     -0.56916714     0.46516004      -1.189643\n-0.117406875    -1.1139084      1.115328        0.23275337\n1.452733        -0.30968842     -0.6693723      -0.22098665\n\n(1,2,.,.) =\n0.06541251      -0.7000564      -0.460471       -0.5291468\n-0.6625642      0.6460361       -0.556095       1.6327276\n1.1914377       -0.69054496     -0.7461783      -1.0129389\n\n(2,1,.,.) =\n-0.19123174     0.06803144      -0.010993495    -0.79966563\n-0.010654963    2.0806832       1.972848        -1.8525643\n-0.84387285     1.2974458       -0.42781293     0.3540522\n\n(2,2,.,.) =\n1.6231914       0.52689505      0.47506556      -1.030227\n0.5143046       -0.9930063      -2.2869735      0.03994834\n-1.5566326      -1.0937842      0.82693833      -0.08408405\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.11401264  0.0         1.1396459\n0.0         0.0         0.88493514\n\n(1,2,.,.) =\n0.0         8.398667    1.1495202\n0.0         0.0         0.1630742\n\n(1,3,.,.) =\n0.0         0.92470163  0.0\n0.0         0.6321572   0.0\n\n(1,4,.,.) =\n0.0         1.1912066   0.0\n0.0         1.27647     0.13422263\n\n(2,1,.,.) =\n0.0         0.0         0.51365596\n0.0         0.4207713   1.1226959\n\n(2,2,.,.) =\n0.0         0.67600054  0.63635653\n0.40892223  2.0596464   1.7690754\n\n(2,3,.,.) =\n1.1899394   0.0         0.0\n1.7185769   0.39178902  0.0\n\n(2,4,.,.) =\n0.44333076  0.73385376  0.0\n2.516453    0.36223468  0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AtrousConvolution2D\n\nmodel = Sequential()\nmodel.add(AtrousConvolution2D(4, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.52102612 0.30683086 0.38543426 0.0026452 ]\n   [0.66805249 0.60656045 0.94601998 0.46574414]\n   [0.49391338 0.14274225 0.70473703 0.30427041]]\n  [[0.89066007 0.51782675 0.7063052  0.53440807]\n   [0.67377917 0.51541465 0.02137767 0.63357007]\n   [0.6614106  0.15849977 0.94459604 0.46852022]]]\n\n [[[0.79639026 0.94468413 0.73165819 0.54531867]\n   [0.97741046 0.64477619 0.52373183 0.06861999]\n   [0.37278645 0.53198045 0.95098245 0.86249644]]\n  [[0.47186038 0.81694951 0.78009033 0.20925898]\n   [0.69942883 0.37150324 0.58907364 0.88754231]\n   [0.64083971 0.4480097  0.91716521 0.66808943]]]]  Output is  [[[[-0.32139003  -0.34667802 -0.35534883]\n   [-0.09653517  -0.35052428 -0.09859636]]\n  [[-0.3138999   -0.5563417  -0.6694119 ]\n   [-0.03151364  0.35521197  0.31497604]]\n  [[-0.34939283  -0.7537081  -0.3939833 ]\n   [-0.25708836  0.06015673  -0.16755156]]\n  [[-0.04791902  0.02060626  -0.5639752 ]\n   [ 0.16054101  0.22528952  -0.02460545]]]\n\n [[[-0.13129832  -0.5262137   -0.12281597]\n   [-0.36988598  -0.5532047   -0.43338764]]\n  [[-0.21627764  -0.17562683  0.23560521]\n   [ 0.23035726  -0.03152001  -0.46413773]]\n  [[-0.63740283  -0.33359224  0.15731882]\n   [-0.12795202  -0.25798583  -0.5261132 ]]\n  [[-0.01759483  -0.07666921  -0.00890112]\n   [ 0.27595833  -0.14117064  -0.3357542 ]]]]",
            "title": "AtrousConvolution2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#deconvolution2d",
            "text": "Transposed convolution operator for filtering windows of 2-D inputs.  The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has\nthe shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'valid'.  You can also use  Deconv2D  as an alias of this layer.  The input of this layer should be 4D.  Scala:  Deconvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, subsample = (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  Deconvolution2D(nb_filter, nb_row, nb_col, output_shape, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of transposed convolution filters to use.  nbRow : Number of rows in the transposed convolution kernel.  nbCol : Number of columns in the transposed convolution kernel.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  subsample : Length 2 . The step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Deconvolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Deconvolution2D(2, 2, 2, activation = \"relu\", inputShape = Shape(2, 2, 3)))\nval input = Tensor[Float](1, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.1157457  -0.8626509  -0.7326707\n1.8340882   -1.1647098  -1.0159439\n\n(1,2,.,.) =\n-0.13360074 0.4507607   -0.5922559\n0.15494606  0.16541296  1.6870573\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.0         0.0     0.020009547\n0.0         0.0         0.0     0.0\n0.9656998   0.0         0.0     0.5543601\n\n(1,2,.,.) =\n0.0         0.0         0.0     0.07773054\n1.4971795   0.029338006 0.0     0.0\n0.0         0.45826393  0.0     0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Deconvolution2D\n\nmodel = Sequential()\nmodel.add(Deconvolution2D(2, 2, 2, (2, 3, 4), input_shape=(2, 2, 3)))\ninput = np.random.random([1, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[0.65315139 0.21904901 0.57943617]\n   [0.35141043 0.14628658 0.81862311]]\n\n  [[0.60094717 0.84649884 0.08338504]\n   [0.26753695 0.83676038 0.87466877]]]]  Output is  [[[[-0.35380065  0.22048733  0.3084591   0.23341973]\n   [-0.11611718  0.5349988  -0.26301163  1.0291481 ]\n   [ 0.00479569  0.48814884  0.00127316  0.2546792 ]]\n\n  [[-0.02683929 -0.21759698 -0.8542665  -0.25376737]\n   [ 0.04426606  0.05486238 -0.9282576  -1.1576774 ]\n   [ 0.01637976  0.1838439  -0.01419228 -0.60704494]]]]",
            "title": "Deconvolution2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#separableconvolution2d",
            "text": "Applies separable convolution operator for 2D inputs.  Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depthMultiplier argument controls how many output channels are generated per input channel in the depthwise step.  You can also use  SeparableConv2D  as an alias of this layer.  The input of this layer should be 4D.  Scala:  SeparableConvolution2D(nbFilter, nbRow, nbCol, init = \"glorot_uniform\", activation = null, borderMode = \"valid\", subsample = (1, 1), depthMultiplier = 1, dimOrdering = \"th\", depthwiseRegularizer = null, pointwiseRegularizer= null, bRegularizer = null, bias = true, inputShape = null)  Python:  SeparableConvolution2D(nb_filter, nb_row, nb_col, init=\"glorot_uniform\", activation=None, border_mode=\"valid\", subsample=(1, 1), depth_multiplier=1, dim_ordering=\"th\", depthwise_regularizer=None, pointwise_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  nbRow : Number of rows in the convolution kernel.  nbCol : Number of columns in the convolution kernel.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is 'glorot_uniform'.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  subsample : Length 2 corresponding to the step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).  depthMultiplier : How many output channel to use per input channel for the depthwise convolution step. Integer. Default is 1.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  depthwiseRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the depthwise weights matrices. Default is null.  pointwiseRegularizer : An instance of  Regularizer , applied to the pointwise weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, SeparableConvolution2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SeparableConvolution2D(2, 2, 2, activation = \"relu\", inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.61846036  0.13724488     1.9047198    0.8788536\n0.74383116  -0.7590018     0.17210509   1.8095028\n-0.21476124 -0.010768774   0.5437478    0.97470677\n\n(1,2,.,.) =\n-0.22464052 -1.7141389     1.8457758    0.81563693\n-0.17250067 -1.2183974     -2.5329974   -1.3014348\n0.43760046  0.32672745     -0.6059157   0.31439257\n\n(2,1,.,.) =\n-0.32413644 -0.1871411     -0.13821407  -0.16577224\n-0.02138366 1.2260025      -0.48404458  -1.0251912\n-1.8844653  0.6796752      -0.5881143   2.1656246\n\n(2,2,.,.) =\n0.17234507  -0.6455974     1.9615031    0.6552883\n-0.05861185 1.8847446      -0.857622    -0.5949971\n-0.41135395 -0.92089206    0.13154007   -0.9326055\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.0         0.0\n0.029595211 0.0         0.34002993\n\n(1,2,.,.) =\n0.0         0.0         0.0\n0.073145226 0.0         0.5542682\n\n(2,1,.,.) =\n0.4973382   0.36478913  0.0\n0.0         0.0         0.0\n\n(2,2,.,.) =\n0.9668598   0.7102739   0.0\n0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SeparableConvolution2D\n\nmodel = Sequential()\nmodel.add(SeparableConvolution2D(2, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.39277921 0.36904141 0.16768533 0.41712068]\n   [0.62416696 0.19334139 0.83341541 0.16486488]\n   [0.57287259 0.47809379 0.11103843 0.01746644]]\n  [[0.24945342 0.05728102 0.19076369 0.70498077]\n   [0.39147172 0.08100018 0.74426575 0.74251056]\n   [0.61840056 0.00771785 0.65170218 0.04492181]]]\n\n [[[0.08337509 0.19320791 0.66757918 0.38905916]\n   [0.50237454 0.0996316  0.3981495  0.32274897]\n   [0.01598124 0.52896577 0.76068351 0.10099803]]\n  [[0.20396797 0.48682425 0.11302674 0.57491998]\n   [0.71529612 0.11720466 0.57783092 0.45790133]\n   [0.41573101 0.60269287 0.613528   0.32717263]]]]  Output is  [[[[0.15971108 0.12109925 0.17461367]\n   [0.20024002 0.13661252 0.1871847 ]]\n  [[0.47139192 0.36838844 0.45902973]\n   [0.57752806 0.41371965 0.5079273 ]]]\n\n [[[0.11111417 0.10702941 0.2030398 ]\n   [0.13108528 0.15029006 0.18544158]]\n  [[0.27002305 0.31479427 0.57750916]\n   [0.3573216  0.40100253 0.5122235 ]]]]",
            "title": "SeparableConvolution2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#locallyconnected1d",
            "text": "Locally-connected layer for 1D inputs which works similarly to the TemporalConvolution layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.  Border mode currently supported for this layer is 'valid'.  The input of this layer should be 3D.  Scala:  LocallyConnected1D(nbFilter, filterLength, activation = null, subsampleLength = 1, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  LocallyConnected1D(nb_filter, filter_length, activation=None, border_mode=\"valid\", subsample_length=1, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Dimensionality of the output.  filterLength : The extension (spatial or temporal) of each filter.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  subsampleLength : Integer. Factor by which to subsample output.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, LocallyConnected1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LocallyConnected1D(6, 3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.6755046   0.47923228  -0.41470557 -1.4644535\n-1.580751   -0.36924785 -1.1507624  0.20131736\n-0.4983051  -2.0898817  0.1623063   0.8118141\n\n(2,.,.) =\n1.5955191   -1.1017833  1.6614468   1.7959124\n1.1084127   0.528379    -1.114553   -1.030853\n0.37758648  -2.5828059  1.0172523   -1.6773314\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.20011228 0.7842446   -0.57892114 0.2405633   -0.35126245 -0.5116563\n\n(2,.,.) =\n-0.33687726 0.7863857   0.30202985  0.33251244  -0.7414977  0.14271683\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x6]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LocallyConnected1D\n\nmodel = Sequential()\nmodel.add(LocallyConnected1D(6, 3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.67992353 0.88287213 0.98861104 0.17401607]\n  [0.23660068 0.02779148 0.52982599 0.19876749]\n  [0.38880073 0.6498778  0.81532701 0.91719509]]\n\n [[0.30532677 0.1574227  0.40535271 0.03174637]\n  [0.37303714 0.27821415 0.02314422 0.64516966]\n  [0.74813923 0.9884225  0.40667151 0.21894944]]]  Output is  [[[ 0.66351205 -0.03819168 -0.48071918 -0.05209085 -0.07307816  0.94942856]]\n\n [[ 0.5890693   0.0179258  -0.31232932  0.4427027  -0.30954808  0.4486028 ]]]",
            "title": "LocallyConnected1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#locallyconnected2d",
            "text": "Locally-connected layer for 2D inputs that works similarly to the SpatialConvolution layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.  The input of this layer should be 4D.  Scala:  LocallyConnected2D(nbFilter, nbRow, nbCol, activation = null, borderMode = \"valid\", subsample = (1, 1), dimOrdering = \"th\", wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  LocallyConnected2D(nb_filter, nb_row, nb_col, activation=None, border_mode=\"valid\", subsample=(1, 1), dim_ordering=\"th\", W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  nbRow : Number of rows in the convolution kernel.  nbCol : Number of columns in the convolution kernel.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  subsample : Length 2 corresponding to the step of the convolution in the height and width dimension. Also called strides elsewhere. Default is (1, 1).  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, LocallyConnected2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LocallyConnected2D(2, 2, 2, inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.3119988       -1.8982307      -0.13138956     1.0872058\n-0.11329581     -0.7087005      0.085274234     -0.94051\n1.04928         2.1579344       -1.4412278      -0.90965116\n\n(1,2,.,.) =\n-0.6119555      1.2226686       -0.10441754     -1.6240023\n0.5598073       -0.099059306    -1.543586       0.72533834\n-1.6674699      -1.0901593      -0.24129404     0.30954796\n\n(2,1,.,.) =\n-0.78856885     -0.5567014      -1.1273636      -0.98069143\n-0.40949664     0.92562497      -1.3729718      0.7423901\n-0.29498738     -0.044669412    1.0937366       0.90768206\n\n(2,2,.,.) =\n1.0948726       -0.23575573     -0.051821854    -0.58692485\n1.9133459       -1.0849183      2.1423934       0.6559134\n-0.8390565      -0.27111387     -0.8439365      -1.3939567\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.42428172 0.25790718  -0.5227444\n0.6963143   -0.34605533 -0.35524538\n\n(1,2,.,.) =\n0.61758286  0.8430548   0.1378907\n0.24116383  0.15782532  0.16882366\n\n(2,1,.,.) =\n-0.5603108  0.5107949   -0.112701565\n0.62288725  0.6909297   -0.9253155\n\n(2,2,.,.) =\n-0.2443612  0.9310517   -0.2417406\n-0.82973266 -1.0886648  0.19112866\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LocallyConnected2D\n\nmodel = Sequential()\nmodel.add(LocallyConnected2D(2, 2, 2, input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.57424593 0.49505236 0.63711108 0.43693806]\n   [0.34655799 0.0058394  0.69310344 0.70403367]\n   [0.4620432  0.58679338 0.64529398 0.78130808]]\n  [[0.49651564 0.32201482 0.02470762 0.80535793]\n   [0.94485185 0.07150504 0.58789497 0.4562848 ]\n   [0.63595033 0.04600271 0.89771801 0.95419454]]]\n\n [[[0.69641827 0.21785002 0.15815588 0.8317213 ]\n   [0.84192366 0.3939658  0.64309395 0.3858968 ]\n   [0.16545408 0.58533897 0.99486481 0.84651898]]\n  [[0.05144159 0.94930242 0.26842063 0.6341632 ]\n   [0.442836   0.38544902 0.04266468 0.22600452]\n   [0.2705393  0.07313841 0.24295287 0.9573069 ]]]]  Output is  [[[[ 0.1600316   0.178018   -0.07472821]\n   [ 0.0570091  -0.19973318  0.44483435]]\n  [[-0.20258084 -0.37692443 -0.27103102]\n   [-0.624092   -0.09749079 -0.00799894]]]\n\n [[[ 0.58953685  0.35287908 -0.2203412 ]\n   [ 0.13649486 -0.29554832  0.16932982]]\n  [[-0.00787066 -0.06614903 -0.2027885 ]\n   [-0.33434835 -0.33458236 -0.15103136]]]]",
            "title": "LocallyConnected2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#cropping1d",
            "text": "Cropping layer for 1D input (e.g. temporal sequence).  The input of this layer should be 3D.  Scala:  Cropping1D(cropping = (1, 1), inputShape = null)  Python:  Cropping1D(cropping=(1, 1), input_shape=None, name=None)  Parameters:   cropping : Length 2. How many units should be trimmed off at the beginning and end of the cropping dimension. Default is (1, 1).  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Cropping1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Cropping1D((1, 1), inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.0038188  -0.75265634 0.7417358   1.0674809\n-1.4702164  0.64112693  0.17750219  -0.21439286\n-0.93766433 -1.0809567  0.7706962   0.16380796\n\n(2,.,.) =\n0.45019576  -0.36689326 0.08852628  -0.21602148\n0.66039973  0.11638404  0.062985964 -1.0420738\n0.46727908  -0.85894865 1.9853845   0.059447426\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-1.4702164  0.64112693  0.17750219  -0.21439286\n\n(2,.,.) =\n0.66039973  0.11638404  0.062985964 -1.0420738\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x1x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Cropping1D\n\nmodel = Sequential()\nmodel.add(Cropping1D(input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.01030651 0.77603525 0.97263208 0.15933375]\n  [0.05135971 0.01139832 0.28809891 0.57260363]\n  [0.28128354 0.55290954 0.77011153 0.09879061]]\n\n [[0.75765909 0.55102462 0.42426818 0.14383546]\n  [0.85198966 0.3990277  0.13061313 0.10349525]\n  [0.69892804 0.30310119 0.2241441  0.05978997]]]  Output is  [[[0.05135971 0.01139832 0.2880989  0.57260364]]\n\n [[0.8519896  0.3990277  0.13061313 0.10349525]]]",
            "title": "Cropping1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#cropping2d",
            "text": "Cropping layer for 2D input (e.g. picture).  The input of this layer should be 4D.  Scala:  Cropping2D(cropping = ((0, 0), (0, 0)), dimOrdering = \"th\", inputShape = null)  Python:  Cropping2D(cropping=((0, 0), (0, 0)), dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   cropping : Int tuple of tuple of length 2. How many units should be trimmed off at the beginning and end of the 2 cropping dimensions (i.e. height and width). Default is ((0, 0), (0, 0)).  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Cropping2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Cropping2D(((0, 1), (1, 0)), inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-1.3613406  -0.03520738  -0.008660733  2.1150143\n0.18087284  1.8787018    0.30097032   -2.5634677\n-1.9463011  -0.18772388  1.5215846    -0.8047026\n\n(1,2,.,.) =\n-0.50510925 -1.1193116   0.6901347   -0.2625669\n-0.24307655 -0.77917117  -0.566465   1.0432123\n0.4877474   0.49704018   -1.5550427  1.5772455\n\n(2,1,.,.) =\n-1.6180872  0.011832007  1.2762135   0.5600022\n1.9009352   -0.11096256  1.1500957   -0.26341736\n1.0153246   0.88008636   0.0560876   -1.0235065\n\n(2,2,.,.) =\n0.1036221   1.08527      -0.52559805   -0.5091204\n1.3085281   -0.96346164  -0.09713245   -1.1010116\n0.08505145  1.9413263    2.0237558     -0.5978173\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.03520738 -0.008660733  2.1150143\n1.8787018   0.30097032    -2.5634677\n\n(1,2,.,.) =\n-1.1193116   0.6901347  -0.2625669\n-0.77917117  -0.566465  1.0432123\n\n(2,1,.,.) =\n0.011832007  1.2762135  0.5600022\n-0.11096256  1.1500957  -0.26341736\n\n(2,2,.,.) =\n1.08527      -0.52559805   -0.5091204\n-0.96346164  -0.09713245   -1.1010116\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Cropping2D\n\nmodel = Sequential()\nmodel.add(Cropping2D(((0, 1), (1, 0)), input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.03691489 0.60233732 0.96327319 0.99561146]\n   [0.85728883 0.77923287 0.41328434 0.87490199]\n   [0.3389653  0.94804499 0.72922732 0.21191413]]\n  [[0.28962322 0.30133445 0.58516862 0.22476588]\n   [0.95386045 0.72488497 0.12056255 0.01265548]\n   [0.48645173 0.34426033 0.09410422 0.86815053]]]\n\n [[[0.57444115 0.79141167 0.20755353 0.38616465]\n   [0.95793123 0.22366943 0.5080078  0.27193368]\n   [0.65402317 0.1023231  0.67207896 0.2229965 ]]\n  [[0.04160647 0.55577895 0.30907277 0.42227706]\n   [0.54489229 0.90423796 0.50782414 0.51441165]\n   [0.87544565 0.47791071 0.0341273  0.14728084]]]]  Output is  [[[[0.6023373  0.96327317 0.9956115 ]\n   [0.77923286 0.41328433 0.874902  ]]\n  [[0.30133444 0.5851686  0.22476588]\n   [0.724885   0.12056255 0.01265548]]]\n\n [[[0.7914117  0.20755354 0.38616467]\n   [0.22366942 0.5080078  0.27193367]]\n\n  [[0.555779   0.30907276 0.42227706]\n   [0.904238   0.5078241  0.5144116 ]]]]",
            "title": "Cropping2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#cropping3d",
            "text": "Cropping layer for 3D data (e.g. spatial or spatio-temporal).  The input of this layer should be 5D.  Scala:  Cropping3D(cropping = ((1, 1), (1, 1), (1, 1)), dimOrdering = \"th\", inputShape = null)  Python:  Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   cropping : Int tuple of tuple of length 3. How many units should be trimmed off at the beginning and end of the 3 cropping dimensions (i.e. kernel_dim1, kernel_dim2 and kernel_dim3). Default is ((1, 1), (1, 1), (1, 1)).  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Cropping3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Cropping3D(((1, 1), (1, 1), (1, 1)), inputShape = Shape(2, 3, 4, 5)))\nval input = Tensor[Float](2, 2, 3, 4, 5).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-0.12339484     0.25661087      0.04387503      -1.1047344      -1.1413815\n1.1830065       -0.07189157     -0.5418846      0.5576781       -0.5460917\n-0.5679186      -0.30854696     1.2614665       -0.6774269      -0.63295823\n0.5269464       -2.7981617      -0.056265026    -1.0814936      -1.0848739\n\n(1,1,2,.,.) =\n-1.9100302      0.461067        0.4014941       0.60723174      -0.40414023\n0.34300476      0.7107094       1.3142885       1.5696589       0.97591686\n0.38320687      0.07036536      -0.43628898     0.58050656      -0.57882625\n-0.43699506     -0.0094956765   0.15171598      0.038076796     -1.2433665\n\n(1,1,3,.,.) =\n0.39671394      0.880047        0.30971292      -0.3369089      0.13062176\n-0.27803114     -0.62177086     0.16659822      0.89428085      0.23684736\n1.6151237       -1.1479733      -0.2229254      1.1361892       0.79478127\n-1.8207864      1.6544164       0.07977915      -1.1316417      -0.25483203\n\n(1,2,1,.,.) =\n1.3165517       -0.9479057      -1.4662051      -0.3343554      -0.4522552\n-1.5829691      0.6378519       -0.16399206     1.4724066       1.2387054\n-1.1467208      -0.6325814      -1.2106491      -0.035734158    0.19871919\n2.285004        1.0482147       -2.0056705      -0.80917794     2.523167\n\n(1,2,2,.,.) =\n-0.57108706     -0.23606259     -0.45569882     -0.034214735    -1.9130942\n-0.2743481      1.61177         -0.7052599      0.17889105      -0.31241596\n0.22377247      1.5860337       -0.3226252      -0.1341058      0.9239994\n0.03615294      0.6233593       0.757827        -0.72271305     0.9429943\n\n(1,2,3,.,.) =\n-0.4409662      0.8867786       2.0036085       0.16242673      -0.3332395\n0.09082064      0.04958198      -0.27834833     1.8025815       -0.04848101\n0.2690667       -1.1263227      -0.95486647     0.09473259      0.98166656\n-0.9509363      -0.10084029     -0.35410827     0.29626986      0.97203517\n\n(2,1,1,.,.) =\n0.42096403      0.14016314      0.20216857      -0.678293       -1.0970931\n-0.4981112      0.12429344      1.7156922       -0.24384527     -0.010780937\n0.03672217      2.3021698       1.568247        -0.43173146     -0.5550057\n0.30469602      1.4772439       -0.21195345     0.04221814      -1.6883365\n\n(2,1,2,.,.) =\n0.22468264      0.72787744      -0.9597003      -0.28472963     -1.4575284\n1.0487963       0.4982454       -1.0186157      -1.9877508      -1.133779\n0.17539643      -0.35151628     -1.8955303      2.1854792       0.59556997\n0.6893949       -0.19556235     0.25862908      0.24450152      0.17786922\n\n(2,1,3,.,.) =\n1.147159        -0.8849993      0.9826487       0.95360875      -0.9210176\n1.3439047       0.6739913       0.06558858      0.91963255      -1.1758618\n1.747105        -0.7225308      -1.0160877      0.67554474      -0.7762811\n0.21184689      -0.43668815     -1.0738864      0.04661594      0.9613895\n\n(2,2,1,.,.) =\n-0.377159       -0.28094378     0.1081715       1.3683178       1.2572801\n0.47781375      0.4545212       0.55356956      1.0366637       -0.1962683\n-1.820227       -0.111765414    1.9194998       -1.6089902      -1.6960226\n0.14896627      0.9360371       0.49156702      0.08601956      -0.08815153\n\n(2,2,2,.,.) =\n0.056315728     -0.13061485     -0.49018836     -0.59103477     -1.6910721\n-0.023719765    -0.44977355     0.11218439      0.224829        1.400084\n0.31496882      -1.6386473      -0.6715097      0.14816228      0.3240011\n-0.80607724     -0.37951842     -0.2187672      1.1087769       0.43044603\n\n(2,2,3,.,.) =\n-1.6647842      -0.5720825      -1.5150099      0.42346838      1.495052\n-0.3567161      -1.4341534      -0.19422509     -1.2871891      -1.2758921\n-0.47077888     -0.42217267     0.67764246      1.2170314       0.8420698\n-0.4263702      1.2792329       0.38645822      -2.4653213      -1.512707\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4x5]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.7107094   1.3142885   1.5696589\n0.07036536  -0.43628898 0.58050656\n\n(1,2,1,.,.) =\n1.61177     -0.7052599  0.17889105\n1.5860337   -0.3226252  -0.1341058\n\n(2,1,1,.,.) =\n0.4982454   -1.0186157  -1.9877508\n-0.35151628 -1.8955303  2.1854792\n\n(2,2,1,.,.) =\n-0.44977355 0.11218439  0.224829\n-1.6386473  -0.6715097  0.14816228\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Cropping3D\n\nmodel = Sequential()\nmodel.add(Cropping3D(((1, 1), (1, 1), (1, 1)), input_shape=(2, 3, 4, 5)))\ninput = np.random.random([2, 2, 3, 4, 5])\noutput = model.forward(input)  Input is:  [[[[[0.17398425 0.68189365 0.77769123 0.53108205 0.64715435]\n    [0.14553671 0.56312657 0.68612354 0.69176945 0.30109699]\n    [0.09732807 0.37460879 0.19945361 0.86471357 0.66225896]\n    [0.23071766 0.7940814  0.20828491 0.05256511 0.39059369]]\n   [[0.61604377 0.08752888 0.0373393  0.2074062  0.60620641]\n    [0.72873275 0.86871873 0.89248703 0.9407502  0.71830713]\n    [0.23277175 0.75968678 0.2160847  0.76278034 0.27796526]\n    [0.45593022 0.31406512 0.83030059 0.17528758 0.56134316]]\n   [[0.65576189 0.41055457 0.90979203 0.76003643 0.26369912]\n    [0.20767533 0.60489496 0.44996379 0.20016757 0.39282226]\n    [0.14055952 0.15767185 0.70149107 0.88403803 0.77345544]\n    [0.34344548 0.03721154 0.86204782 0.45349481 0.69348787]]]\n  [[[0.55441874 0.59949813 0.4450893  0.2103161  0.6300366 ]\n    [0.71573331 0.32423206 0.06302588 0.91902299 0.30852669]\n    [0.73540519 0.20697542 0.20543135 0.44461869 0.89286638]\n    [0.41614996 0.48155318 0.51663767 0.23681825 0.34780746]]\n   [[0.34529962 0.81156897 0.77911935 0.65392321 0.45178564]\n    [0.39702465 0.36180668 0.37867952 0.24818676 0.84365902]\n    [0.67836434 0.24043224 0.59870659 0.81976809 0.95442206]\n    [0.15342281 0.48607751 0.11420129 0.68621285 0.09892679]]\n   [[0.61122758 0.40359022 0.99805441 0.76764677 0.6281926 ]\n    [0.44867213 0.81206033 0.40117858 0.98967612 0.76897064]\n    [0.90603977 0.17299288 0.68803644 0.75164168 0.4161878 ]\n    [0.18996933 0.93317759 0.77711184 0.50760022 0.77439241]]]]\n\n [[[[0.49974828 0.74486599 0.12447392 0.15415173 0.36715309]\n    [0.49334423 0.66699219 0.22202136 0.52689596 0.15497081]\n    [0.4117844  0.21886979 0.13096058 0.82589121 0.00621519]\n    [0.38257617 0.60924058 0.53549974 0.64299846 0.66315369]]\n   [[0.78048895 0.20350694 0.16485496 0.71243727 0.4581091 ]\n    [0.554526   0.66891789 0.90082079 0.76729771 0.40647459]\n    [0.72809646 0.68164733 0.83008334 0.90941546 0.1441997 ]\n    [0.44580521 0.78015871 0.63982938 0.26813225 0.15588673]]\n   [[0.85294056 0.0928758  0.37056251 0.82930655 0.27178195]\n    [0.95953427 0.60170629 0.69156911 0.27902576 0.55613879]\n    [0.97101437 0.49876892 0.36313494 0.11233855 0.24221145]\n    [0.28739626 0.2990425  0.68940864 0.95621615 0.6922569 ]]]\n  [[[0.90283303 0.51320503 0.78356741 0.79301195 0.17681709]\n    [0.61624755 0.95418399 0.68118889 0.69241549 0.17943311]\n    [0.71129437 0.55478761 0.34121912 0.86018439 0.03652437]\n    [0.39098173 0.87916544 0.39647239 0.00104663 0.01377085]]\n   [[0.28875017 0.03733266 0.47260498 0.2896268  0.55976704]\n    [0.08723092 0.45523634 0.98463086 0.56950302 0.98261442]\n    [0.20716971 0.52744283 0.39455719 0.57384754 0.76698272]\n    [0.3079253  0.88143353 0.85897125 0.0969679  0.43760548]]\n   [[0.44239165 0.56141652 0.30344311 0.05425044 0.34003295]\n    [0.31417344 0.39485584 0.47300811 0.38006721 0.23185974]\n    [0.06158527 0.95330693 0.63043506 0.9480669  0.93758737]\n    [0.05340179 0.2064604  0.97254971 0.60841205 0.89738937]]]]]  Output is  [[[[[0.86871874 0.89248705 0.9407502 ]\n    [0.75968677 0.2160847  0.7627803 ]]]\n  [[[0.3618067  0.3786795  0.24818675]\n    [0.24043223 0.5987066  0.8197681 ]]]]\n\n [[[[0.6689179  0.9008208  0.7672977 ]\n    [0.68164736 0.8300834  0.9094155 ]]]\n  [[[0.45523635 0.9846309  0.569503  ]\n    [0.5274428  0.39455718 0.57384753]]]]]",
            "title": "Cropping3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#zeropadding1d",
            "text": "Zero-padding layer for 1D input (e.g. temporal sequence).  The input of this layer should be 3D.  Scala:  ZeroPadding1D(padding = 1, inputShape = null)  Python:  ZeroPadding1D(padding=1, input_shape=None, name=None)  Parameters:   padding : How many zeros to add at the beginning and at the end of the padding dimension.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, ZeroPadding1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ZeroPadding1D(1, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.7421485   -0.13270181 -0.12605186 -0.7442475\n0.36977226  -0.90300065 -0.34193754 -0.035565257\n-0.23300397 0.8183156   0.7023575   -0.16938858\n\n(2,.,.) =\n-0.7785278  0.36642975  -1.0542017  -0.29036212\n-0.22632122 0.46808097  -0.68293047 1.2529073\n-0.8619831  1.3846883   1.0762612   1.1351995\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.0         0.0         0.0         0.0\n0.7421485   -0.13270181 -0.12605186 -0.7442475\n0.36977226  -0.90300065 -0.34193754 -0.035565257\n-0.23300397 0.8183156   0.7023575   -0.16938858\n0.0         0.0         0.0         0.0\n\n(2,.,.) =\n0.0         0.0         0.0         0.0\n-0.7785278  0.36642975  -1.0542017  -0.29036212\n-0.22632122 0.46808097  -0.68293047 1.2529073\n-0.8619831  1.3846883   1.0762612   1.1351995\n0.0         0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x5x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ZeroPadding1D\n\nmodel = Sequential()\nmodel.add(ZeroPadding1D(1, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.74177145 0.75805981 0.2091588  0.46929227]\n  [0.46041743 0.13213793 0.51065024 0.36081853]\n  [0.60803218 0.27764702 0.31788482 0.65445294]]\n\n [[0.96255443 0.74692762 0.50050961 0.88456158]\n  [0.55492653 0.50850271 0.17788885 0.91569285]\n  [0.27356035 0.74622588 0.39690752 0.75229177]]]  Output is  [[[0.0        0.0        0.0        0.0       ]\n  [0.74177146 0.7580598  0.2091588  0.46929225]\n  [0.46041742 0.13213794 0.5106502  0.36081854]\n  [0.60803217 0.27764702 0.31788483 0.6544529 ]\n  [0.0        0.0        0.0        0.0       ]]\n\n [[0.0        0.0        0.0        0.0       ]\n  [0.96255445 0.7469276  0.5005096  0.8845616 ]\n  [0.5549265  0.5085027  0.17788884 0.91569287]\n  [0.27356035 0.7462259  0.39690754 0.75229174]\n  [0.0        0.0        0.0        0.0       ]]]",
            "title": "ZeroPadding1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#zeropadding2d",
            "text": "Zero-padding layer for 2D input (e.g. picture).  The input of this layer should be 4D.  Scala:  ZeroPadding2D(padding = (1, 1), dimOrdering = \"th\", inputShape = null)  Python:  ZeroPadding2D(padding=(1, 1), dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   padding : How many zeros to add at the beginning and at the end of the 2 padding dimensions (rows and cols).  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, ZeroPadding2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ZeroPadding2D((1, 1), inputShape = Shape(2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.7201442   -1.0197405  1.3163399\n-0.23921064 0.7732504   -0.069928266\n\n(1,2,.,.) =\n0.46323594  -1.3043984  -0.67622787\n-1.610615   -0.39253974 -0.89652705\n\n(2,1,.,.) =\n-0.3784847  -0.6738694  0.30479854\n-0.49577644 1.0704983   0.6288544\n\n(2,2,.,.) =\n0.2821439   0.790223    0.34665197\n0.24190207  0.10775433  0.46225727\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     1.7201442   -1.0197405  1.3163399       0.0\n0.0     -0.23921064 0.7732504   -0.069928266    0.0\n0.0     0.0         0.0         0.0             0.0\n\n(1,2,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     0.46323594  -1.3043984  -0.67622787     0.0\n0.0     -1.610615   -0.39253974 -0.89652705     0.0\n0.0     0.0         0.0         0.0             0.0\n\n(2,1,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     -0.3784847  -0.6738694  0.30479854      0.0\n0.0     -0.49577644 1.0704983   0.6288544       0.0\n0.0     0.0         0.0         0.0             0.0\n\n(2,2,.,.) =\n0.0     0.0         0.0         0.0             0.0\n0.0     0.2821439   0.790223    0.34665197      0.0\n0.0     0.24190207  0.10775433  0.46225727      0.0\n0.0     0.0         0.0         0.0             0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x4x5]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ZeroPadding2D\n\nmodel = Sequential()\nmodel.add(ZeroPadding2D(input_shape=(2, 2, 3)))\ninput = np.random.random([2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[0.44048214 0.72494886 0.96654241]\n   [0.66254801 0.37409083 0.47681466]]\n\n  [[0.23204026 0.52762765 0.15072852]\n   [0.45052127 0.29016392 0.0133929 ]]]\n\n\n [[[0.09347565 0.4754528  0.63618458]\n   [0.08016674 0.21696158 0.83892852]]\n\n  [[0.81864575 0.90813398 0.08347963]\n   [0.57234761 0.76060611 0.65707858]]]]  Output is  [[[[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.44048214 0.7249489  0.9665424  0.0 ]\n   [0.0   0.662548   0.37409082 0.47681466 0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]\n\n  [[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.23204026 0.52762765 0.15072852 0.0 ]\n   [0.0   0.45052126 0.29016393 0.0133929  0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]]\n\n\n [[[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.09347565 0.4754528  0.6361846  0.0 ]\n   [0.0   0.08016673 0.21696158 0.8389285  0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]\n\n  [[0.0   0.0        0.0        0.0        0.0 ]\n   [0.0   0.8186458  0.908134   0.08347963 0.0 ]\n   [0.0   0.5723476  0.7606061  0.65707856 0.0 ]\n   [0.0   0.0        0.0        0.0        0.0 ]]]]",
            "title": "ZeroPadding2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#zeropadding3d",
            "text": "Zero-padding layer for 3D data (spatial or spatio-temporal).  The input of this layer should be 5D.  Scala:  ZeroPadding3D(padding = (1, 1, 1), dimOrdering = \"th\", inputShape = null)  Python:  ZeroPadding3D(padding=(1, 1, 1), dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   padding : How many zeros to add at the beginning and end of the 3 padding dimensions. Symmetric padding will be applied to each dimension.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, ZeroPadding3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ZeroPadding3D((1, 1, 1), inputShape = Shape(1, 2, 2, 2)))\nval input = Tensor[Float](1, 1, 2, 2, 2).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n1.086798    2.162806\n-0.50501716 -0.17430544\n\n(1,1,2,.,.) =\n-1.7388326  0.27966997\n1.6211525   1.1713351\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2x2]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n\n(1,1,2,.,.) =\n0.0     0.0         0.0         0.0\n0.0     1.086798    2.162806    0.0\n0.0     -0.50501716 -0.17430544 0.0\n0.0     0.0         0.0         0.0\n\n(1,1,3,.,.) =\n0.0     0.0         0.0         0.0\n0.0     -1.7388326  0.27966997  0.0\n0.0     1.6211525   1.1713351   0.0\n0.0     0.0         0.0         0.0\n\n(1,1,4,.,.) =\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n0.0     0.0         0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x4x4x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ZeroPadding3D\n\nmodel = Sequential()\nmodel.add(ZeroPadding3D((1, 1, 1), input_shape=(1, 2, 2, 2)))\ninput = np.random.random([1, 1, 2, 2, 2])\noutput = model.forward(input)  Input is:  [[[[[0.12432462 0.19244616]\n    [0.39039533 0.88140855]]\n\n   [[0.71426182 0.86085132]\n    [0.04443494 0.679125  ]]]]]  Output is  [[[[[0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]]\n   [[0.0   0.0        0.0        0.0 ]\n    [0.0   0.12432462 0.19244616 0.0 ]\n    [0.0   0.39039534 0.8814086  0.0 ]\n    [0.0   0.0        0.0        0.0 ]]\n   [[0.0   0.0        0.0        0.0 ]\n    [0.0   0.71426183 0.8608513  0.0 ]\n    [0.0   0.04443494 0.679125   0.0 ]\n    [0.0   0.0        0.0        0.0 ]]\n   [[0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]\n    [0.0   0.0        0.0        0.0 ]]]]]",
            "title": "ZeroPadding3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#upsampling1d",
            "text": "UpSampling layer for 1D inputs.  Repeats each temporal step 'length' times along the time axis.  The input of this layer should be 3D.  Scala:  UpSampling1D(length = 2, inputShape = null)  Python:  UpSampling1D(length=2, input_shape=None, name=None)  Parameters:   length : Integer. UpSampling factor. Default is 2.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, UpSampling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(UpSampling1D(2, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.80225134  -0.9644977   -0.71038723    -1.5673652\n0.67224514  -0.24330814  -0.082499735   0.2807591\n-0.9299857  -1.8893008   -1.1062661     -1.1637908\n\n(2,.,.) =\n-0.1831344  -0.6621819   -0.667329      -0.26960346\n-0.6601015  1.0819869    1.0307902      1.1801233\n-0.18303517 0.2565441    -0.39598823    0.23400643\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.80225134  -0.9644977   -0.71038723     -1.5673652\n0.80225134  -0.9644977   -0.71038723     -1.5673652\n0.67224514  -0.24330814  -0.082499735    0.2807591\n0.67224514  -0.24330814  -0.082499735    0.2807591\n-0.9299857  -1.8893008   -1.1062661     -1.1637908\n-0.9299857  -1.8893008   -1.1062661     -1.1637908\n\n(2,.,.) =\n-0.1831344  -0.6621819   -0.667329      -0.26960346\n-0.1831344  -0.6621819   -0.667329      -0.26960346\n-0.6601015  1.0819869    1.0307902      1.1801233\n-0.6601015  1.0819869    1.0307902      1.1801233\n-0.18303517 0.2565441    -0.39598823    0.23400643\n-0.18303517 0.2565441    -0.39598823    0.23400643\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x6x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import UpSampling1D\n\nmodel = Sequential()\nmodel.add(UpSampling1D(2, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.66227662 0.00663032 0.49010329 0.26836567]\n  [0.34225774 0.26000732 0.27628499 0.49861887]\n  [0.11619066 0.28123766 0.60770962 0.80773197]]\n\n [[0.477639   0.88906297 0.38577295 0.99058504]\n  [0.50690837 0.38107999 0.05881034 0.96402145]\n  [0.42226283 0.77350512 0.54961295 0.55315271]]]  Output is  [[[0.6622766  0.00663032 0.4901033  0.26836568]\n  [0.6622766  0.00663032 0.4901033  0.26836568]\n  [0.34225774 0.26000732 0.276285   0.49861887]\n  [0.34225774 0.26000732 0.276285   0.49861887]\n  [0.11619066 0.28123766 0.60770965 0.807732  ]\n  [0.11619066 0.28123766 0.60770965 0.807732  ]]\n\n [[0.477639   0.88906294 0.38577294 0.990585  ]\n  [0.477639   0.88906294 0.38577294 0.990585  ]\n  [0.50690836 0.38107997 0.05881034 0.96402144]\n  [0.50690836 0.38107997 0.05881034 0.96402144]\n  [0.42226282 0.7735051  0.54961294 0.5531527 ]\n  [0.42226282 0.7735051  0.54961294 0.5531527 ]]]",
            "title": "UpSampling1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#upsampling2d",
            "text": "UpSampling layer for 2D inputs.  Repeats the rows and columns of the data by the specified size.  The input of this layer should be 4D.  Scala:  UpSampling2D(size = (2, 2), dimOrdering = \"th\", inputShape = null)  Python:  UpSampling2D(size=(2, 2), dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   size : Length 2. UpSampling factors for rows and columns. Default is (2, 2).  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, UpSampling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(UpSampling2D((2, 2), inputShape = Shape(2, 2, 2)))\nval input = Tensor[Float](1, 2, 2, 2).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.07563081 -1.921836\n-1.7368479  0.1043008\n\n(1,2,.,.) =\n-1.825055   -0.096810855\n-0.89331573 0.72812295\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.07563081 -0.07563081  -1.921836   -1.921836\n-0.07563081 -0.07563081  -1.921836   -1.921836\n-1.7368479  -1.7368479   0.1043008   0.1043008\n-1.7368479  -1.7368479   0.1043008   0.1043008\n\n(1,2,.,.) =\n-1.825055    -1.825055    -0.096810855  -0.096810855\n-1.825055    -1.825055    -0.096810855  -0.096810855\n-0.89331573  -0.89331573  0.72812295    0.72812295\n-0.89331573  -0.89331573  0.72812295    0.72812295\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x4x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import UpSampling2D\n\nmodel = Sequential()\nmodel.add(UpSampling2D((2, 2), input_shape=(2, 2, 2)))\ninput = np.random.random([1, 2, 2, 2])\noutput = model.forward(input)  Input is:  [[[[0.55660253 0.21984387]\n   [0.36271854 0.57464162]]\n\n  [[0.55307278 0.33007518]\n   [0.31527167 0.87789644]]]]  Output is  [[[[0.55660254 0.55660254 0.21984388 0.21984388]\n   [0.55660254 0.55660254 0.21984388 0.21984388]\n   [0.36271855 0.36271855 0.57464164 0.57464164]\n   [0.36271855 0.36271855 0.57464164 0.57464164]]\n\n  [[0.55307275 0.55307275 0.33007517 0.33007517]\n   [0.55307275 0.55307275 0.33007517 0.33007517]\n   [0.31527168 0.31527168 0.8778964  0.8778964 ]\n   [0.31527168 0.31527168 0.8778964  0.8778964 ]]]]",
            "title": "UpSampling2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/convolutional/#upsampling3d",
            "text": "UpSampling layer for 3D inputs.  Repeats the 1st, 2nd and 3rd dimensions of the data by the specified size.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  The input of this layer should be 5D.  Scala:  UpSampling3D(size = (2, 2, 2), dimOrdering = \"th\", inputShape = null)  Python:  UpSampling3D(size=(2, 2, 2), dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   size : Length 3. UpSampling factors for dim1, dim2 and dim3. Default is (2, 2, 2).  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, UpSampling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(UpSampling3D((2, 2, 2), inputShape = Shape(1, 1, 2, 2)))\nval input = Tensor[Float](1, 1, 1, 2, 2).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.9906968   -0.2451235\n1.5133694   -0.34076887\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x1x2x2]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.9906968   0.9906968   -0.2451235  -0.2451235\n0.9906968   0.9906968   -0.2451235  -0.2451235\n1.5133694   1.5133694   -0.34076887 -0.34076887\n1.5133694   1.5133694   -0.34076887 -0.34076887\n\n(1,1,2,.,.) =\n0.9906968   0.9906968   -0.2451235  -0.2451235\n0.9906968   0.9906968   -0.2451235  -0.2451235\n1.5133694   1.5133694   -0.34076887 -0.34076887\n1.5133694   1.5133694   -0.34076887 -0.34076887\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x4x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import UpSampling3D\n\nmodel = Sequential()\nmodel.add(UpSampling3D((2, 2, 2), input_shape=(1, 1, 2, 2)))\ninput = np.random.random([1, 1, 1, 2, 2])\noutput = model.forward(input)  Input is:  [[[[[0.58361205 0.2096227 ]\n    [0.51686662 0.70260105]]]]]  Output is  [[[[[0.583612  0.583612  0.2096227 0.2096227]\n    [0.583612  0.583612  0.2096227 0.2096227]\n    [0.5168666 0.5168666 0.7026011 0.7026011]\n    [0.5168666 0.5168666 0.7026011 0.7026011]]\n\n   [[0.583612  0.583612  0.2096227 0.2096227]\n    [0.583612  0.583612  0.2096227 0.2096227]\n    [0.5168666 0.5168666 0.7026011 0.7026011]\n    [0.5168666 0.5168666 0.7026011 0.7026011]]]]]",
            "title": "UpSampling3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/",
            "text": "MaxPooling1D\n\n\nMax pooling operation for temporal data.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nMaxPooling1D(poolLength = 2, stride = -1, borderMode = \"valid\", inputShape = null)\n\n\n\n\nPython:\n\n\nMaxPooling1D(pool_length=2, stride=None, border_mode=\"valid\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npoolLength\n: Size of the region to which max pooling is applied. Integer. Default is 2.\n\n\nstride\n: Factor by which to downscale. 2 will halve the input. If not specified, it will default to poolLength.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, MaxPooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxPooling1D(poolLength = 3, inputShape = Shape(4, 5)))\nval input = Tensor[Float](3, 4, 5).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.32697344   -1.901702    0.9338836    0.4988416    -1.4769285\n0.82112324   -1.749153    -1.2225364   0.17479241   -0.1569928\n-1.9349245   -0.7208759   -2.6186085   0.7094514    0.02309827\n0.06299127   -0.28094748  -1.679667    -0.19593267  -0.6486389\n\n(2,.,.) =\n0.5059762    -0.27661985  1.3978469    -0.13661754  0.9121702\n1.20289      -1.2779995   -1.221474    1.6933655    0.06884759\n-0.8358409   -1.5242177   0.38067985   0.1758138    -2.0869224\n-0.052700672 -1.2065598   0.65831304   -2.7004414   -1.5840155\n\n(3,.,.) =\n-1.5877407   -0.23685509  -1.1487285   0.6082965    0.5463596\n-0.6323151   1.6099663    0.16473362   -0.6759079   -0.22952202\n0.07198518   1.0313594    1.4555247    0.7538992    -1.2048378\n1.2034347    0.11312642   -0.14845283  -1.3795642   1.1672769\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4x5]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.82112324  -0.7208759  0.9338836   0.7094514   0.02309827\n\n(2,.,.) =\n1.20289     -0.27661985 1.3978469   1.6933655   0.9121702\n\n(3,.,.) =\n0.07198518  1.6099663   1.4555247   0.7538992   0.5463596\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x5]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxPooling1D\n\nmodel = Sequential()\nmodel.add(MaxPooling1D(pool_length = 3, input_shape = (4, 5)))\ninput = np.random.random([3, 4, 5])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.40580359 0.91869648 0.52699134 0.96507862 0.45316868]\n  [0.55665601 0.91599093 0.68640946 0.55788983 0.79788871]\n  [0.63706076 0.86559853 0.2157637  0.56051023 0.48453306]\n  [0.68673896 0.35445905 0.98369363 0.05747027 0.54176785]]\n\n [[0.00154654 0.02109022 0.69103023 0.08356977 0.51230376]\n  [0.01498106 0.32251403 0.98859889 0.6393191  0.59248678]\n  [0.43467219 0.97269656 0.82172126 0.62731276 0.19477236]\n  [0.44162847 0.50752131 0.43099026 0.07546448 0.97122237]]\n\n [[0.9526254  0.82221173 0.13355431 0.19929353 0.95937559]\n  [0.53449677 0.8041899  0.45077759 0.40048272 0.31712774]\n  [0.83603459 0.72547619 0.61066729 0.09561956 0.32530191]\n  [0.10199395 0.77512743 0.69522612 0.7456257  0.73544269]]]\n\n\n\n\nOutput is:\n\n\n[[[0.63706076 0.91869646 0.6864095  0.9650786  0.7978887 ]]\n\n [[0.43467218 0.97269654 0.9885989  0.6393191  0.5924868 ]]\n\n [[0.9526254  0.82221174 0.6106673  0.4004827  0.95937556]]]\n\n\n\n\n\n\nMaxPooling2D\n\n\nMax pooling operation for spatial data.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nMaxPooling2D(poolSize = (2, 2), strides = null, borderMode = \"valid\", dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nMaxPooling2D(pool_size=(2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npoolSize\n: Length 2 corresponding to the downscale vertically and horizontally. Default is (2, 2), which will halve the image in each dimension.\n\n\nstrides\n: Length 2. Stride values. Default is null, and in this case it will be equal to poolSize.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, MaxPooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxPooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n2.5301383   0.10926374  0.6072471   0.658932\n-0.3569041  0.32731345  -1.4209954  -0.4969882\n0.70455354  -2.7349844  0.66514283  -1.0055662\n\n(1,2,.,.) =\n-0.29669985 0.054489832 -1.1771511  -0.37510478\n1.2857671   -1.1703448  0.39755398  -1.6102049\n-0.42201662 1.2561954   1.1706035   0.20676066\n\n(2,1,.,.) =\n2.2395058   0.36936793  -1.0407287  0.46479732\n0.08024679  -1.3457166  -0.7048267  -0.017787607\n-0.66454273 -1.5704913  -1.7375602  -2.417642\n\n(2,2,.,.) =\n-1.5279706  -1.0108438  1.0017345   -0.5810244\n-1.5944351  0.11111861  0.4439802   -0.48056543\n-2.4090567  -1.459287   0.67291117  0.24757418\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n2.5301383   0.658932\n\n(1,2,.,.) =\n1.2857671   0.39755398\n\n(2,1,.,.) =\n2.2395058   0.46479732\n\n(2,2,.,.) =\n0.11111861  1.0017345\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxPooling2D\n\nmodel = Sequential()\nmodel.add(MaxPooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.82279705 0.62487892 0.37391352 0.22834848]\n   [0.68709158 0.40902972 0.73191486 0.40095294]\n   [0.651977   0.93330601 0.45785981 0.45939351]]\n  [[0.372833   0.39871945 0.13426243 0.83083849]\n   [0.24290548 0.04446027 0.58070741 0.37752852]\n   [0.13116942 0.59339663 0.94669915 0.02460278]]]\n\n [[[0.46505904 0.96103464 0.75846419 0.77357123]\n   [0.37835688 0.88438048 0.5679742  0.74607276]\n   [0.41415466 0.73945737 0.39188398 0.52736799]]\n  [[0.51772064 0.19857965 0.15476197 0.64569767]\n   [0.21794751 0.74455093 0.48423447 0.15482331]\n   [0.38363071 0.78733222 0.2542284  0.88671892]]]]\n\n\n\n\nOutput is:\n\n\n[[[[0.82279706 0.7319149 ]]\n  [[0.39871946 0.8308385 ]]]\n\n [[[0.96103466 0.77357125]]\n  [[0.74455094 0.64569765]]]]\n\n\n\n\n\n\nMaxPooling3D\n\n\nApplies max pooling operation for 3D data (spatial or spatio-temporal).\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nMaxPooling3D(poolSize = (2, 2, 2), strides = null, dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nMaxPooling3D(pool_size=(2, 2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npoolSize\n: Length 3. Factors by which to downscale (dim1, dim2, dim3). Default is (2, 2, 2), which will halve the image in each dimension.\n\n\nstrides\n: Length 3. Stride values. Default is null, and in this case it will be equal to poolSize.\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, MaxPooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxPooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-0.5052603  0.8938585   0.44785392\n-0.48919395 0.35026026  0.541859\n\n(1,1,2,.,.) =\n1.5306468   0.24512683  1.71524\n-0.49025944 2.1886358   0.15880944\n\n(1,2,1,.,.) =\n-0.5133986  -0.16549884 -0.2971134\n1.5887301   1.8269571   1.3843931\n\n(1,2,2,.,.) =\n0.07515256  1.6993935   -0.3392596\n1.2611006   0.20215735  1.3105171\n\n(2,1,1,.,.) =\n-2.0070438  0.35554957  0.21326075\n-0.4078646  -1.5748956  -1.1007504\n\n(2,1,2,.,.) =\n1.0571382   -1.6031493  1.4638771\n-0.25891435 1.4923956   -0.24045596\n\n(2,2,1,.,.) =\n-0.57790893 0.14577095  1.3165486\n0.81937057  -0.3797079  1.2544848\n\n(2,2,2,.,.) =\n-0.42183575 -0.63774794 -2.0576336\n0.43662143  1.9010457   -0.061519064\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n2.1886358\n\n(1,2,1,.,.) =\n1.8269571\n\n(2,1,1,.,.) =\n1.4923956\n\n(2,2,1,.,.) =\n1.9010457\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x1x1]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxPooling3D\n\nmodel = Sequential()\nmodel.add(MaxPooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.73349746 0.9811588  0.86071417]\n    [0.33287621 0.37991739 0.87029317]]\n   [[0.62537904 0.48099174 0.06194759]\n    [0.38747972 0.05175308 0.36096032]]]\n  [[[0.63260385 0.69990236 0.63353249]\n    [0.19081261 0.56210617 0.75985185]]\n   [[0.8624058  0.47224318 0.26524027]\n    [0.75317792 0.39251436 0.98938982]]]]\n\n [[[[0.00556086 0.18833728 0.80340438]\n    [0.9317538  0.88142596 0.90724509]]\n   [[0.90243612 0.04594116 0.43662143]\n    [0.24205094 0.58687822 0.57977055]]]\n  [[[0.17240398 0.18346483 0.02520754]\n    [0.06968248 0.02442692 0.56078895]]\n   [[0.69503427 0.09528588 0.46104647]\n    [0.16752596 0.88175901 0.71032998]]]]]\n\n\n\n\nOutput is:\n\n\n[[[[[0.9811588]]]\n  [[[0.8624058]]]]\n\n [[[[0.9317538]]]\n  [[[0.881759 ]]]]]\n\n\n\n\n\n\nAveragePooling1D\n\n\nAverage pooling for temporal data.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nAveragePooling1D(poolLength = 2, stride = -1, borderMode = \"valid\", inputShape = null)\n\n\n\n\nPython:\n\n\nAveragePooling1D(pool_length=2, stride=None, border_mode=\"valid\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npoolLength\n: Size of the region to which average pooling is applied. Integer. Default is 2.\n\n\nstride\n: Factor by which to downscale. 2 will halve the input. If not specified, it will default to poolLength.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, AveragePooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AveragePooling1D(poolLength = 3, inputShape = Shape(4, 5)))\nval input = Tensor[Float](3, 4, 5).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.1377933   0.21371832   1.7306958    -1.9029621   -1.4344455\n0.30368176   1.4264593    -2.1044374   1.9331307    0.064429775\n-0.20389123  1.0778805    -0.6283651   1.3097609    -0.13545972\n0.2993623    0.6173592    0.36244655   0.79175955   0.79752296\n\n(2,.,.) =\n-0.2151101   -0.016314683 0.42787352   0.8266788    1.3463322\n-0.5822824   -0.80566406  1.8474609    1.0040557    0.058591228\n1.1027422    -1.3031522   -0.17601672  1.0220417    -0.26774135\n0.5274945    0.33779684   -0.85662115  0.057247106  -0.26438802\n\n(3,.,.) =\n-0.069942534 0.9225811    -0.46108767  2.4335458    0.101546675\n-0.12930758  0.7706995    -0.1920893   -0.23971881  0.72432745\n0.55851805   -0.5315623   0.7103099    -0.5954772   1.1504582\n0.6810412    2.08239      0.5578813    -0.21148366  0.6381254\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4x5]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.34600094 0.9060194   -0.33403555 0.4466432   -0.50182515\n\n(2,.,.) =\n0.10178324  -0.70837694 0.69977254  0.95092535  0.37906072\n\n(3,.,.) =\n0.11975598  0.38723943  0.01904432  0.5327832   0.6587774\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x5]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AveragePooling1D\n\nmodel = Sequential()\nmodel.add(AveragePooling1D(pool_length = 3, input_shape = (4, 5)))\ninput = np.random.random([3, 4, 5])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.70712635 0.03582033 0.06284402 0.32036469 0.81474437]\n  [0.79836044 0.90868531 0.28817162 0.82013972 0.31323463]\n  [0.22585456 0.36409411 0.22224669 0.93922795 0.19095179]\n  [0.39250119 0.64782355 0.85172164 0.28749378 0.1088145 ]]\n\n [[0.04373644 0.29722078 0.3117768  0.64487829 0.4810562 ]\n  [0.3168246  0.08202731 0.58480522 0.72992227 0.64433289]\n  [0.49511033 0.09427843 0.80680702 0.23613413 0.70898751]\n  [0.50461138 0.26695611 0.34203601 0.09773049 0.19039967]]\n\n [[0.75294793 0.55036481 0.26584527 0.98080601 0.43339867]\n  [0.50389323 0.07068883 0.78938881 0.96551069 0.15544646]\n  [0.12795345 0.23093578 0.22171131 0.54183322 0.39152313]\n  [0.53546306 0.66279754 0.52490436 0.14028357 0.40409458]]]\n\n\n\n\nOutput is:\n\n\n[[[0.57711375 0.4361999  0.19108744 0.69324416 0.43964362]]\n\n [[0.2852238  0.15784217 0.56779635 0.53697824 0.61145884]]\n\n [[0.4615982  0.28399646 0.42564845 0.8293833  0.3267894 ]]]\n\n\n\n\n\n\nAveragePooling2D\n\n\nAverage pooling operation for spatial data.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nAveragePooling2D(poolSize = (2, 2), strides = null, borderMode = \"valid\", dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nAveragePooling2D(pool_size=(2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npoolSize\n: Length 2 corresponding to the downscale vertically and horizontally. Default is (2, 2), which will halve the image in each dimension.\n\n\nstrides\n: Length 2. Stride values. Default is null, and in this case it will be equal to poolSize.\n\n\nborderMode\n: Either 'valid' or 'same'. Default is 'valid'.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, AveragePooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AveragePooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.9929163   0.73885435  -0.34893242 1.1493853\n-0.45246652 -0.32470804 1.2192643   0.30351913\n1.3251832   0.52051955  -1.1398637  -2.427732\n\n(1,2,.,.) =\n-0.5123787  -0.5055035  0.3858232   0.71986055\n0.9580216   0.36081943  1.4867425   0.9852266\n-0.6051215  -0.15555465 -1.4472512  0.51882136\n\n(2,1,.,.) =\n-1.5209191  0.006158142 1.5162845   -0.06919313\n0.56743985  -0.499725   -0.44013703 -0.12666322\n0.78009427  1.9432178   1.4082893   -0.6143322\n\n(2,2,.,.) =\n-1.387891   0.023748515 -0.8295103  -0.9282333\n1.1375008   -1.4631946  -0.67415875 -0.7773346\n-2.297338   1.0384767   1.7125391   -1.7680352\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.23864903  0.5808091\n\n(1,2,.,.) =\n0.075239725 0.89441323\n\n(2,1,.,.) =\n-0.36176154 0.22007278\n\n(2,2,.,.) =\n-0.4224591  -0.8023093\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AveragePooling2D\n\nmodel = Sequential()\nmodel.add(AveragePooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.23128514 0.69922098 0.52158685 0.43063779]\n   [0.89149649 0.33910949 0.4402748  0.08933058]\n   [0.71712488 0.21574851 0.76768248 0.57027882]]\n  [[0.08349921 0.85318742 0.49922456 0.6256355 ]\n   [0.22331336 0.78402155 0.91424506 0.18895412]\n   [0.89722286 0.31067545 0.82655572 0.37775551]]]\n\n [[[0.9706926  0.28398186 0.36623623 0.23701637]\n   [0.49936358 0.50951663 0.48116156 0.89941571]\n   [0.06519683 0.34624179 0.2462403  0.48512833]]\n  [[0.58408752 0.68318898 0.67886418 0.43403476]\n   [0.87328453 0.8412756  0.59168164 0.49972216]\n   [0.82188585 0.63685579 0.50966912 0.51439279]]]]\n\n\n\n\nOutput is:\n\n\n[[[[0.540278   0.3704575 ]]\n  [[0.48600537 0.5570148 ]]]\n\n [[[0.56588864 0.49595746]]\n  [[0.7454592  0.5510757 ]]]]\n\n\n\n\n\n\nAveragePooling3D\n\n\nApplies average pooling operation for 3D data (spatial or spatio-temporal).\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nAveragePooling3D(poolSize = (2, 2, 2), strides = null, dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nAveragePooling3D(pool_size=(2, 2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\npoolSize\n: Length 3. Factors by which to downscale (dim1, dim2, dim3). Default is (2, 2, 2), which will halve the image in each dimension.\n\n\nstrides\n: Length 3. Stride values. Default is null, and in this case it will be equal to poolSize.\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, AveragePooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AveragePooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-1.2491689  -0.72333497 0.076971635\n-1.2854124  -0.026098572    2.1735003\n\n(1,1,2,.,.) =\n1.4519714   1.1690209   2.3558137\n0.53576165  0.544173    1.1044264\n\n(1,2,1,.,.) =\n-1.2603238  -0.5580594  -0.91401285\n-0.18393324 -0.34946147 -0.5833402\n\n(1,2,2,.,.) =\n-0.2528762  0.5091298   0.2399745\n1.4895978   -1.3734508  -1.0218369\n\n(2,1,1,.,.) =\n-1.7266496  -0.04624697 0.47165343\n0.16339892  0.9384256   1.0018257\n\n(2,1,2,.,.) =\n-0.45763373 0.41072395  0.3123065\n-1.1914686  0.90784425  -2.8544335\n\n(2,2,1,.,.) =\n0.81638193  -1.2425674  1.9570643\n1.444956    0.37828556  -1.7336447\n\n(2,2,2,.,.) =\n-0.43858975 0.91795254  0.3359727\n0.20638026  -0.07622202 -2.1452882\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.052114002\n\n(1,2,1,.,.) =\n-0.24742219\n\n(2,1,1,.,.) =\n-0.12520081\n\n(2,2,1,.,.) =\n0.25082216\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x1x1]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AveragePooling3D\n\nmodel = Sequential()\nmodel.add(AveragePooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.97536696 0.01017816 0.77256405]\n    [0.53928594 0.80710453 0.71903394]]\n   [[0.67067647 0.38680811 0.05431165]\n    [0.98320357 0.8602754  0.12535218]]]\n  [[[0.32317928 0.17129988 0.51584225]\n    [0.70060648 0.36568169 0.36353108]]\n   [[0.90675921 0.68967216 0.29854921]\n    [0.72568459 0.34304905 0.9501725 ]]]]\n\n [[[[0.96295459 0.51457555 0.15752579]\n    [0.29569757 0.73166152 0.24217882]]\n   [[0.69938844 0.98315048 0.36022304]\n    [0.97079866 0.03950786 0.18505114]]]\n  [[[0.10255992 0.87988966 0.13163776]\n    [0.286857   0.56472867 0.73914834]]\n   [[0.51970598 0.19869426 0.47845175]\n    [0.86776147 0.60381965 0.88064078]]]]]\n\n\n\n\nOutput is:\n\n\n[[[[[0.6541124 ]]]\n  [[[0.5282415 ]]]]\n\n [[[[0.64971685]]]\n  [[[0.5030021 ]]]]]\n\n\n\n\n\n\n\nGlobalMaxPooling1D\n\n\nGlobal max pooling operation for temporal data.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nGlobalMaxPooling1D(inputShape = null)\n\n\n\n\nPython:\n\n\nGlobalMaxPooling1D(input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalMaxPooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalMaxPooling1D(inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.0603509  2.0034506   -1.9884554\n-1.187076   -1.3023934  -0.8058352\n\n(2,.,.) =\n-0.9960039  -2.5800185  -0.01848254\n-0.66063184 -1.451372   1.3490999\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-1.0603509  2.0034506   -0.8058352\n-0.66063184 -1.451372   1.3490999\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalMaxPooling1D\n\nmodel = Sequential()\nmodel.add(GlobalMaxPooling1D(input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.70782546 0.2064717  0.45753652]\n  [0.41262607 0.72777349 0.21662695]]\n\n [[0.0937254  0.16749913 0.65395922]\n  [0.51027108 0.67591602 0.41025529]]]\n\n\n\n\nOutput is:\n\n\n[[0.7078255  0.7277735  0.45753652]\n [0.5102711  0.675916   0.6539592 ]]\n\n\n\n\n\n\nGlobalMaxPooling2D\n\n\nGlobal max pooling operation for spatial data.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nGlobalMaxPooling2D(dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nGlobalMaxPooling2D(dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalMaxPooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalMaxPooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.33040258 -0.94677037 0.14672963   0.1252591\n-0.3141728  0.68040586  1.3511285    -0.29235613\n0.03947779  -1.1260709  0.16128083   -1.1656744\n\n(1,2,.,.) =\n1.0182372   -0.6030568  -1.5335841   0.37804475\n0.26944965  -1.6720067  0.2405665    -0.95661074\n-0.31286374 0.109459646 -1.6644431   -1.9295278\n\n(2,1,.,.) =\n1.0210015   -0.69647574 -0.629564    1.6719679\n-0.7825565  -0.48921636 0.1892077    0.17827414\n0.76913565  0.17354056  -0.5749589   -1.736962\n\n(2,2,.,.) =\n0.82071537  -0.22566034 0.12415939   0.02941268\n0.34600595  0.86877316  0.9797952    -1.7793267\n0.025843443 -1.6373945  -0.093925744 -0.22479358\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n1.3511285   1.0182372\n1.6719679   0.9797952\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalMaxPooling2D\n\nmodel = Sequential()\nmodel.add(GlobalMaxPooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.4037502  0.02214535 0.83987632 0.63656679]\n   [0.57757778 0.41884406 0.94446159 0.55776242]\n   [0.40213234 0.4463349  0.04457756 0.99123233]]\n  [[0.71114675 0.88609155 0.40299526 0.01648121]\n   [0.23102758 0.89673552 0.07030409 0.79674961]\n   [0.84665248 0.18257089 0.87211872 0.22697933]]]\n\n [[[0.08033122 0.26298654 0.10863184 0.57894922]\n   [0.03999134 0.90867755 0.80473921 0.79913378]\n   [0.60443084 0.92055786 0.17994007 0.87414516]]\n  [[0.50193442 0.52639178 0.72124789 0.41776979]\n   [0.09495006 0.91797563 0.48755794 0.50458372]\n   [0.47387433 0.93445126 0.83216554 0.67275364]]]]\n\n\n\n\nOutput is:\n\n\n[[0.99123234 0.8967355]\n [0.92055786 0.9344513]]\n\n\n\n\n\n\nGlobalMaxPooling3D\n\n\nApplies global max pooling operation for 3D data.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nGlobalMaxPooling3D(dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nGlobalMaxPooling3D(dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalMaxPooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalMaxPooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.5882856      0.6403546    -0.42084476\n-0.097307995   -0.52530056  -0.36757985\n\n(1,1,2,.,.) =\n0.60983604  -0.24730012 -0.07744695\n0.36276138  0.34722528  0.19874145\n\n(1,2,1,.,.) =\n-0.912143   0.14050196  2.1116483\n-0.67692965 -0.5708391  -2.1040971\n\n(1,2,2,.,.) =\n2.1500692   1.1697202   1.364164\n1.2241726   -0.12069768 1.2471954\n\n(2,1,1,.,.) =\n0.39550102  -0.7435119  0.47669584\n-0.17335615 0.2690476   -0.8462402\n\n(2,1,2,.,.) =\n-1.0553921  -0.35153934 0.8036665\n-1.029019   -0.64534503 0.94537926\n\n(2,2,1,.,.) =\n0.5388452   -0.27233714 1.5837694\n1.0976856   -0.20959699 1.6285672\n\n(2,2,2,.,.) =\n-0.7736055  0.58593166  -1.2158531\n1.2194971   1.4081163   1.2056179\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.6403546   2.1500692\n0.94537926  1.6285672\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalMaxPooling3D\n\nmodel = Sequential()\nmodel.add(GlobalMaxPooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.35670186 0.12860978 0.99958102]\n    [0.17345679 0.19473725 0.41235665]]\n   [[0.98948429 0.26686797 0.4632353 ]\n    [0.15791828 0.07452679 0.73215605]]]\n  [[[0.80305568 0.73208753 0.31179214]\n    [0.43452576 0.563038   0.65955869]]\n   [[0.31947806 0.00899334 0.55208827]\n    [0.57471665 0.10157217 0.42698318]]]]\n\n [[[[0.59277903 0.35379325 0.5311834 ]\n    [0.91781414 0.10407255 0.58049721]]\n   [[0.14371521 0.24279466 0.26071055]\n    [0.89431752 0.66817043 0.61662462]]]\n  [[[0.6672706  0.38855847 0.88462881]\n    [0.38859986 0.80439572 0.27661295]]\n   [[0.41453042 0.11527795 0.75953012]\n    [0.77940987 0.26283438 0.97745039]]]]]\n\n\n\n\nOutput is:\n\n\n[[0.99958104 0.8030557]\n [0.91781414 0.9774504]]\n\n\n\n\n\n\nGlobalAveragePooling1D\n\n\nGlobal average pooling operation for temporal data.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nGlobalAveragePooling1D(inputShape = null)\n\n\n\n\nPython:\n\n\nGlobalAveragePooling1D(input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalAveragePooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalAveragePooling1D(inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.52390736  -0.2733816  0.124149635\n-1.351596   -1.1435038  -1.5176618\n\n(2,.,.) =\n1.0428048   -0.65227276 -0.44158915\n-0.23790422 0.4179904   -0.12358317\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.41384432 -0.7084427  -0.69675606\n0.40245032  -0.11714119 -0.28258616\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalAveragePooling1D\n\nmodel = Sequential()\nmodel.add(GlobalAveragePooling1D(input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.93869359 0.22245741 0.9744004 ]\n  [0.89151128 0.8211663  0.73579694]]\n\n [[0.37929716 0.509159   0.21713254]\n  [0.81838451 0.72323228 0.0370643 ]]]\n\n\n\n\nOutput is:\n\n\n[[0.9151024   0.52181184 0.85509866]\n [0.59884083  0.6161956  0.12709841]]\n\n\n\n\n\n\nGlobalAveragePooling2D\n\n\nGlobal average pooling operation for spatial data.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nGlobalAveragePooling2D(dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nGlobalAveragePooling2D(dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalAveragePooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalAveragePooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.110688895    -0.95155084  1.8221924   -2.0326483\n-0.013243215   -1.2766567   -0.16704278 -0.97121066\n-2.4606674     -0.24223651  -0.5687073  0.69842345\n\n(1,2,.,.) =\n0.14165956     0.17032783   2.5329256   0.011501087\n-0.3236992     1.1332442    0.18139894  -2.3126595\n0.1546373      0.35264283   -0.04404357 -0.70906943\n\n(2,1,.,.) =\n-0.08527824    0.29270124   -0.7355773  -0.6026267\n-0.71629876    0.83938205   0.5129336   0.118145116\n0.17555784     -0.8842884   0.12628363  -0.5556226\n\n(2,2,.,.) =\n0.6230317      0.64954233   -1.3002442  -0.44802713\n-0.7294096     0.29014868   -0.55649257 2.1427174\n0.0146621745   0.67039204   0.12979278  1.8543824\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.5043883  0.10740545\n-0.12622404 0.27837467\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalAveragePooling2D\n\nmodel = Sequential()\nmodel.add(GlobalAveragePooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.44851152 0.94140516 0.45500829 0.07239139]\n   [0.58724461 0.7386701  0.69641719 0.70497337]\n   [0.15950558 0.56006247 0.82534941 0.59303245]]\n  [[0.94628326 0.75747177 0.92495215 0.16233194]\n   [0.21553426 0.65968036 0.72130258 0.8929379 ]\n   [0.91295078 0.36362834 0.04734189 0.32399088]]]\n\n [[[0.74069289 0.8804913  0.38783329 0.82279268]\n   [0.29561186 0.86405938 0.21608269 0.618583  ]\n   [0.16823803 0.65690701 0.85394726 0.94541932]]\n  [[0.33876558 0.47517543 0.25908204 0.81933296]\n   [0.16176792 0.57166    0.28295922 0.95254489]\n   [0.10532106 0.98495855 0.41048516 0.86755462]]]]\n\n\n\n\nOutput is:\n\n\n[[0.5652142 0.5773672]\n [0.6208883 0.519134 ]]\n\n\n\n\n\n\nGlobalAveragePooling3D\n\n\nApplies global average pooling operation for 3D data.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'valid'.\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nGlobalAveragePooling3D(dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nGlobalAveragePooling3D(dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalAveragePooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalAveragePooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n1.8996966    -0.20018125  -0.3271749\n0.27594963   -1.0520669   0.86003053\n\n(1,1,2,.,.) =\n-0.7652662   0.72945994   0.9008456\n0.8692407    -1.1327444   2.0664887\n\n(1,2,1,.,.) =\n0.10636215   -0.812925    -0.3757974\n0.48761207   0.017417012  -2.395701\n\n(1,2,2,.,.) =\n-1.3122851   -0.5942121   -0.6180062\n-0.032230377 -0.27521232  -0.3567782\n\n(2,1,1,.,.) =\n1.8668615    -0.4244298   1.0701258\n0.63794065   -1.023562    0.16939393\n\n(2,1,2,.,.) =\n0.20582832   0.5321886    -1.5412451\n-0.38068503  1.4506307    -0.47838798\n\n(2,2,1,.,.) =\n-0.7344984   -0.28647164  2.410416\n-1.8175911   -1.1973995   1.001777\n\n(2,2,2,.,.) =\n-0.09646813  0.11988298   1.4687495\n1.493955     0.16738588   1.133337\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.34368983  -0.51347965\n0.17372166  0.30525622\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalAveragePooling3D\n\nmodel = Sequential()\nmodel.add(GlobalAveragePooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.38403874 0.30696173 0.25682854]\n    [0.53124253 0.62668969 0.21927777]]\n   [[0.33040063 0.37388563 0.75210039]\n    [0.08358634 0.80063745 0.13251887]]]\n  [[[0.41724617 0.2241106  0.55527267]\n    [0.69493785 0.71098284 0.54058444]]\n   [[0.4773658  0.92236993 0.76933649]\n    [0.45217032 0.61153948 0.01976393]]]]\n\n [[[[0.27256789 0.56008397 0.19898919]\n    [0.44973465 0.66605998 0.77117999]]\n   [[0.07868799 0.94786045 0.2240451 ]\n    [0.92261946 0.4053334  0.2572511 ]]]\n  [[[0.33754374 0.28838802 0.79900278]\n    [0.26374789 0.25610211 0.9320699 ]]\n   [[0.19518511 0.80707822 0.29660536]\n    [0.56917623 0.07653736 0.77836375]]]]]\n\n\n\n\nOutput is:\n\n\n[[0.3998474  0.53297335]\n [0.47953442 0.46665   ]]",
            "title": "Pooling Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#maxpooling1d",
            "text": "Max pooling operation for temporal data.  The input of this layer should be 3D.  Scala:  MaxPooling1D(poolLength = 2, stride = -1, borderMode = \"valid\", inputShape = null)  Python:  MaxPooling1D(pool_length=2, stride=None, border_mode=\"valid\", input_shape=None, name=None)  Parameters:   poolLength : Size of the region to which max pooling is applied. Integer. Default is 2.  stride : Factor by which to downscale. 2 will halve the input. If not specified, it will default to poolLength.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, MaxPooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxPooling1D(poolLength = 3, inputShape = Shape(4, 5)))\nval input = Tensor[Float](3, 4, 5).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.32697344   -1.901702    0.9338836    0.4988416    -1.4769285\n0.82112324   -1.749153    -1.2225364   0.17479241   -0.1569928\n-1.9349245   -0.7208759   -2.6186085   0.7094514    0.02309827\n0.06299127   -0.28094748  -1.679667    -0.19593267  -0.6486389\n\n(2,.,.) =\n0.5059762    -0.27661985  1.3978469    -0.13661754  0.9121702\n1.20289      -1.2779995   -1.221474    1.6933655    0.06884759\n-0.8358409   -1.5242177   0.38067985   0.1758138    -2.0869224\n-0.052700672 -1.2065598   0.65831304   -2.7004414   -1.5840155\n\n(3,.,.) =\n-1.5877407   -0.23685509  -1.1487285   0.6082965    0.5463596\n-0.6323151   1.6099663    0.16473362   -0.6759079   -0.22952202\n0.07198518   1.0313594    1.4555247    0.7538992    -1.2048378\n1.2034347    0.11312642   -0.14845283  -1.3795642   1.1672769\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4x5]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.82112324  -0.7208759  0.9338836   0.7094514   0.02309827\n\n(2,.,.) =\n1.20289     -0.27661985 1.3978469   1.6933655   0.9121702\n\n(3,.,.) =\n0.07198518  1.6099663   1.4555247   0.7538992   0.5463596\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x5]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxPooling1D\n\nmodel = Sequential()\nmodel.add(MaxPooling1D(pool_length = 3, input_shape = (4, 5)))\ninput = np.random.random([3, 4, 5])\noutput = model.forward(input)  Input is:  [[[0.40580359 0.91869648 0.52699134 0.96507862 0.45316868]\n  [0.55665601 0.91599093 0.68640946 0.55788983 0.79788871]\n  [0.63706076 0.86559853 0.2157637  0.56051023 0.48453306]\n  [0.68673896 0.35445905 0.98369363 0.05747027 0.54176785]]\n\n [[0.00154654 0.02109022 0.69103023 0.08356977 0.51230376]\n  [0.01498106 0.32251403 0.98859889 0.6393191  0.59248678]\n  [0.43467219 0.97269656 0.82172126 0.62731276 0.19477236]\n  [0.44162847 0.50752131 0.43099026 0.07546448 0.97122237]]\n\n [[0.9526254  0.82221173 0.13355431 0.19929353 0.95937559]\n  [0.53449677 0.8041899  0.45077759 0.40048272 0.31712774]\n  [0.83603459 0.72547619 0.61066729 0.09561956 0.32530191]\n  [0.10199395 0.77512743 0.69522612 0.7456257  0.73544269]]]  Output is:  [[[0.63706076 0.91869646 0.6864095  0.9650786  0.7978887 ]]\n\n [[0.43467218 0.97269654 0.9885989  0.6393191  0.5924868 ]]\n\n [[0.9526254  0.82221174 0.6106673  0.4004827  0.95937556]]]",
            "title": "MaxPooling1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#maxpooling2d",
            "text": "Max pooling operation for spatial data.  The input of this layer should be 4D.  Scala:  MaxPooling2D(poolSize = (2, 2), strides = null, borderMode = \"valid\", dimOrdering = \"th\", inputShape = null)  Python:  MaxPooling2D(pool_size=(2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   poolSize : Length 2 corresponding to the downscale vertically and horizontally. Default is (2, 2), which will halve the image in each dimension.  strides : Length 2. Stride values. Default is null, and in this case it will be equal to poolSize.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, MaxPooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxPooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n2.5301383   0.10926374  0.6072471   0.658932\n-0.3569041  0.32731345  -1.4209954  -0.4969882\n0.70455354  -2.7349844  0.66514283  -1.0055662\n\n(1,2,.,.) =\n-0.29669985 0.054489832 -1.1771511  -0.37510478\n1.2857671   -1.1703448  0.39755398  -1.6102049\n-0.42201662 1.2561954   1.1706035   0.20676066\n\n(2,1,.,.) =\n2.2395058   0.36936793  -1.0407287  0.46479732\n0.08024679  -1.3457166  -0.7048267  -0.017787607\n-0.66454273 -1.5704913  -1.7375602  -2.417642\n\n(2,2,.,.) =\n-1.5279706  -1.0108438  1.0017345   -0.5810244\n-1.5944351  0.11111861  0.4439802   -0.48056543\n-2.4090567  -1.459287   0.67291117  0.24757418\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n2.5301383   0.658932\n\n(1,2,.,.) =\n1.2857671   0.39755398\n\n(2,1,.,.) =\n2.2395058   0.46479732\n\n(2,2,.,.) =\n0.11111861  1.0017345\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxPooling2D\n\nmodel = Sequential()\nmodel.add(MaxPooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.82279705 0.62487892 0.37391352 0.22834848]\n   [0.68709158 0.40902972 0.73191486 0.40095294]\n   [0.651977   0.93330601 0.45785981 0.45939351]]\n  [[0.372833   0.39871945 0.13426243 0.83083849]\n   [0.24290548 0.04446027 0.58070741 0.37752852]\n   [0.13116942 0.59339663 0.94669915 0.02460278]]]\n\n [[[0.46505904 0.96103464 0.75846419 0.77357123]\n   [0.37835688 0.88438048 0.5679742  0.74607276]\n   [0.41415466 0.73945737 0.39188398 0.52736799]]\n  [[0.51772064 0.19857965 0.15476197 0.64569767]\n   [0.21794751 0.74455093 0.48423447 0.15482331]\n   [0.38363071 0.78733222 0.2542284  0.88671892]]]]  Output is:  [[[[0.82279706 0.7319149 ]]\n  [[0.39871946 0.8308385 ]]]\n\n [[[0.96103466 0.77357125]]\n  [[0.74455094 0.64569765]]]]",
            "title": "MaxPooling2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#maxpooling3d",
            "text": "Applies max pooling operation for 3D data (spatial or spatio-temporal).  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'valid'.  The input of this layer should be 5D.  Scala:  MaxPooling3D(poolSize = (2, 2, 2), strides = null, dimOrdering = \"th\", inputShape = null)  Python:  MaxPooling3D(pool_size=(2, 2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   poolSize : Length 3. Factors by which to downscale (dim1, dim2, dim3). Default is (2, 2, 2), which will halve the image in each dimension.  strides : Length 3. Stride values. Default is null, and in this case it will be equal to poolSize.  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, MaxPooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(MaxPooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-0.5052603  0.8938585   0.44785392\n-0.48919395 0.35026026  0.541859\n\n(1,1,2,.,.) =\n1.5306468   0.24512683  1.71524\n-0.49025944 2.1886358   0.15880944\n\n(1,2,1,.,.) =\n-0.5133986  -0.16549884 -0.2971134\n1.5887301   1.8269571   1.3843931\n\n(1,2,2,.,.) =\n0.07515256  1.6993935   -0.3392596\n1.2611006   0.20215735  1.3105171\n\n(2,1,1,.,.) =\n-2.0070438  0.35554957  0.21326075\n-0.4078646  -1.5748956  -1.1007504\n\n(2,1,2,.,.) =\n1.0571382   -1.6031493  1.4638771\n-0.25891435 1.4923956   -0.24045596\n\n(2,2,1,.,.) =\n-0.57790893 0.14577095  1.3165486\n0.81937057  -0.3797079  1.2544848\n\n(2,2,2,.,.) =\n-0.42183575 -0.63774794 -2.0576336\n0.43662143  1.9010457   -0.061519064\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n2.1886358\n\n(1,2,1,.,.) =\n1.8269571\n\n(2,1,1,.,.) =\n1.4923956\n\n(2,2,1,.,.) =\n1.9010457\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x1x1]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import MaxPooling3D\n\nmodel = Sequential()\nmodel.add(MaxPooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[[0.73349746 0.9811588  0.86071417]\n    [0.33287621 0.37991739 0.87029317]]\n   [[0.62537904 0.48099174 0.06194759]\n    [0.38747972 0.05175308 0.36096032]]]\n  [[[0.63260385 0.69990236 0.63353249]\n    [0.19081261 0.56210617 0.75985185]]\n   [[0.8624058  0.47224318 0.26524027]\n    [0.75317792 0.39251436 0.98938982]]]]\n\n [[[[0.00556086 0.18833728 0.80340438]\n    [0.9317538  0.88142596 0.90724509]]\n   [[0.90243612 0.04594116 0.43662143]\n    [0.24205094 0.58687822 0.57977055]]]\n  [[[0.17240398 0.18346483 0.02520754]\n    [0.06968248 0.02442692 0.56078895]]\n   [[0.69503427 0.09528588 0.46104647]\n    [0.16752596 0.88175901 0.71032998]]]]]  Output is:  [[[[[0.9811588]]]\n  [[[0.8624058]]]]\n\n [[[[0.9317538]]]\n  [[[0.881759 ]]]]]",
            "title": "MaxPooling3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#averagepooling1d",
            "text": "Average pooling for temporal data.  The input of this layer should be 3D.  Scala:  AveragePooling1D(poolLength = 2, stride = -1, borderMode = \"valid\", inputShape = null)  Python:  AveragePooling1D(pool_length=2, stride=None, border_mode=\"valid\", input_shape=None, name=None)  Parameters:   poolLength : Size of the region to which average pooling is applied. Integer. Default is 2.  stride : Factor by which to downscale. 2 will halve the input. If not specified, it will default to poolLength.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, AveragePooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AveragePooling1D(poolLength = 3, inputShape = Shape(4, 5)))\nval input = Tensor[Float](3, 4, 5).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.1377933   0.21371832   1.7306958    -1.9029621   -1.4344455\n0.30368176   1.4264593    -2.1044374   1.9331307    0.064429775\n-0.20389123  1.0778805    -0.6283651   1.3097609    -0.13545972\n0.2993623    0.6173592    0.36244655   0.79175955   0.79752296\n\n(2,.,.) =\n-0.2151101   -0.016314683 0.42787352   0.8266788    1.3463322\n-0.5822824   -0.80566406  1.8474609    1.0040557    0.058591228\n1.1027422    -1.3031522   -0.17601672  1.0220417    -0.26774135\n0.5274945    0.33779684   -0.85662115  0.057247106  -0.26438802\n\n(3,.,.) =\n-0.069942534 0.9225811    -0.46108767  2.4335458    0.101546675\n-0.12930758  0.7706995    -0.1920893   -0.23971881  0.72432745\n0.55851805   -0.5315623   0.7103099    -0.5954772   1.1504582\n0.6810412    2.08239      0.5578813    -0.21148366  0.6381254\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4x5]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.34600094 0.9060194   -0.33403555 0.4466432   -0.50182515\n\n(2,.,.) =\n0.10178324  -0.70837694 0.69977254  0.95092535  0.37906072\n\n(3,.,.) =\n0.11975598  0.38723943  0.01904432  0.5327832   0.6587774\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x1x5]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AveragePooling1D\n\nmodel = Sequential()\nmodel.add(AveragePooling1D(pool_length = 3, input_shape = (4, 5)))\ninput = np.random.random([3, 4, 5])\noutput = model.forward(input)  Input is:  [[[0.70712635 0.03582033 0.06284402 0.32036469 0.81474437]\n  [0.79836044 0.90868531 0.28817162 0.82013972 0.31323463]\n  [0.22585456 0.36409411 0.22224669 0.93922795 0.19095179]\n  [0.39250119 0.64782355 0.85172164 0.28749378 0.1088145 ]]\n\n [[0.04373644 0.29722078 0.3117768  0.64487829 0.4810562 ]\n  [0.3168246  0.08202731 0.58480522 0.72992227 0.64433289]\n  [0.49511033 0.09427843 0.80680702 0.23613413 0.70898751]\n  [0.50461138 0.26695611 0.34203601 0.09773049 0.19039967]]\n\n [[0.75294793 0.55036481 0.26584527 0.98080601 0.43339867]\n  [0.50389323 0.07068883 0.78938881 0.96551069 0.15544646]\n  [0.12795345 0.23093578 0.22171131 0.54183322 0.39152313]\n  [0.53546306 0.66279754 0.52490436 0.14028357 0.40409458]]]  Output is:  [[[0.57711375 0.4361999  0.19108744 0.69324416 0.43964362]]\n\n [[0.2852238  0.15784217 0.56779635 0.53697824 0.61145884]]\n\n [[0.4615982  0.28399646 0.42564845 0.8293833  0.3267894 ]]]",
            "title": "AveragePooling1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#averagepooling2d",
            "text": "Average pooling operation for spatial data.  The input of this layer should be 4D.  Scala:  AveragePooling2D(poolSize = (2, 2), strides = null, borderMode = \"valid\", dimOrdering = \"th\", inputShape = null)  Python:  AveragePooling2D(pool_size=(2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   poolSize : Length 2 corresponding to the downscale vertically and horizontally. Default is (2, 2), which will halve the image in each dimension.  strides : Length 2. Stride values. Default is null, and in this case it will be equal to poolSize.  borderMode : Either 'valid' or 'same'. Default is 'valid'.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, AveragePooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AveragePooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.9929163   0.73885435  -0.34893242 1.1493853\n-0.45246652 -0.32470804 1.2192643   0.30351913\n1.3251832   0.52051955  -1.1398637  -2.427732\n\n(1,2,.,.) =\n-0.5123787  -0.5055035  0.3858232   0.71986055\n0.9580216   0.36081943  1.4867425   0.9852266\n-0.6051215  -0.15555465 -1.4472512  0.51882136\n\n(2,1,.,.) =\n-1.5209191  0.006158142 1.5162845   -0.06919313\n0.56743985  -0.499725   -0.44013703 -0.12666322\n0.78009427  1.9432178   1.4082893   -0.6143322\n\n(2,2,.,.) =\n-1.387891   0.023748515 -0.8295103  -0.9282333\n1.1375008   -1.4631946  -0.67415875 -0.7773346\n-2.297338   1.0384767   1.7125391   -1.7680352\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.23864903  0.5808091\n\n(1,2,.,.) =\n0.075239725 0.89441323\n\n(2,1,.,.) =\n-0.36176154 0.22007278\n\n(2,2,.,.) =\n-0.4224591  -0.8023093\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AveragePooling2D\n\nmodel = Sequential()\nmodel.add(AveragePooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.23128514 0.69922098 0.52158685 0.43063779]\n   [0.89149649 0.33910949 0.4402748  0.08933058]\n   [0.71712488 0.21574851 0.76768248 0.57027882]]\n  [[0.08349921 0.85318742 0.49922456 0.6256355 ]\n   [0.22331336 0.78402155 0.91424506 0.18895412]\n   [0.89722286 0.31067545 0.82655572 0.37775551]]]\n\n [[[0.9706926  0.28398186 0.36623623 0.23701637]\n   [0.49936358 0.50951663 0.48116156 0.89941571]\n   [0.06519683 0.34624179 0.2462403  0.48512833]]\n  [[0.58408752 0.68318898 0.67886418 0.43403476]\n   [0.87328453 0.8412756  0.59168164 0.49972216]\n   [0.82188585 0.63685579 0.50966912 0.51439279]]]]  Output is:  [[[[0.540278   0.3704575 ]]\n  [[0.48600537 0.5570148 ]]]\n\n [[[0.56588864 0.49595746]]\n  [[0.7454592  0.5510757 ]]]]",
            "title": "AveragePooling2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#averagepooling3d",
            "text": "Applies average pooling operation for 3D data (spatial or spatio-temporal).  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'valid'.  The input of this layer should be 5D.  Scala:  AveragePooling3D(poolSize = (2, 2, 2), strides = null, dimOrdering = \"th\", inputShape = null)  Python:  AveragePooling3D(pool_size=(2, 2, 2), strides=None, border_mode=\"valid\", dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   poolSize : Length 3. Factors by which to downscale (dim1, dim2, dim3). Default is (2, 2, 2), which will halve the image in each dimension.  strides : Length 3. Stride values. Default is null, and in this case it will be equal to poolSize.  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, AveragePooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(AveragePooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-1.2491689  -0.72333497 0.076971635\n-1.2854124  -0.026098572    2.1735003\n\n(1,1,2,.,.) =\n1.4519714   1.1690209   2.3558137\n0.53576165  0.544173    1.1044264\n\n(1,2,1,.,.) =\n-1.2603238  -0.5580594  -0.91401285\n-0.18393324 -0.34946147 -0.5833402\n\n(1,2,2,.,.) =\n-0.2528762  0.5091298   0.2399745\n1.4895978   -1.3734508  -1.0218369\n\n(2,1,1,.,.) =\n-1.7266496  -0.04624697 0.47165343\n0.16339892  0.9384256   1.0018257\n\n(2,1,2,.,.) =\n-0.45763373 0.41072395  0.3123065\n-1.1914686  0.90784425  -2.8544335\n\n(2,2,1,.,.) =\n0.81638193  -1.2425674  1.9570643\n1.444956    0.37828556  -1.7336447\n\n(2,2,2,.,.) =\n-0.43858975 0.91795254  0.3359727\n0.20638026  -0.07622202 -2.1452882\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.052114002\n\n(1,2,1,.,.) =\n-0.24742219\n\n(2,1,1,.,.) =\n-0.12520081\n\n(2,2,1,.,.) =\n0.25082216\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x1x1x1]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import AveragePooling3D\n\nmodel = Sequential()\nmodel.add(AveragePooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[[0.97536696 0.01017816 0.77256405]\n    [0.53928594 0.80710453 0.71903394]]\n   [[0.67067647 0.38680811 0.05431165]\n    [0.98320357 0.8602754  0.12535218]]]\n  [[[0.32317928 0.17129988 0.51584225]\n    [0.70060648 0.36568169 0.36353108]]\n   [[0.90675921 0.68967216 0.29854921]\n    [0.72568459 0.34304905 0.9501725 ]]]]\n\n [[[[0.96295459 0.51457555 0.15752579]\n    [0.29569757 0.73166152 0.24217882]]\n   [[0.69938844 0.98315048 0.36022304]\n    [0.97079866 0.03950786 0.18505114]]]\n  [[[0.10255992 0.87988966 0.13163776]\n    [0.286857   0.56472867 0.73914834]]\n   [[0.51970598 0.19869426 0.47845175]\n    [0.86776147 0.60381965 0.88064078]]]]]  Output is:  [[[[[0.6541124 ]]]\n  [[[0.5282415 ]]]]\n\n [[[[0.64971685]]]\n  [[[0.5030021 ]]]]]",
            "title": "AveragePooling3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#globalmaxpooling1d",
            "text": "Global max pooling operation for temporal data.  The input of this layer should be 3D.  Scala:  GlobalMaxPooling1D(inputShape = null)  Python:  GlobalMaxPooling1D(input_shape=None, name=None)  Parameters:   inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalMaxPooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalMaxPooling1D(inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.0603509  2.0034506   -1.9884554\n-1.187076   -1.3023934  -0.8058352\n\n(2,.,.) =\n-0.9960039  -2.5800185  -0.01848254\n-0.66063184 -1.451372   1.3490999\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-1.0603509  2.0034506   -0.8058352\n-0.66063184 -1.451372   1.3490999\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalMaxPooling1D\n\nmodel = Sequential()\nmodel.add(GlobalMaxPooling1D(input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)  Input is:  [[[0.70782546 0.2064717  0.45753652]\n  [0.41262607 0.72777349 0.21662695]]\n\n [[0.0937254  0.16749913 0.65395922]\n  [0.51027108 0.67591602 0.41025529]]]  Output is:  [[0.7078255  0.7277735  0.45753652]\n [0.5102711  0.675916   0.6539592 ]]",
            "title": "GlobalMaxPooling1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#globalmaxpooling2d",
            "text": "Global max pooling operation for spatial data.  The input of this layer should be 4D.  Scala:  GlobalMaxPooling2D(dimOrdering = \"th\", inputShape = null)  Python:  GlobalMaxPooling2D(dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalMaxPooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalMaxPooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n-0.33040258 -0.94677037 0.14672963   0.1252591\n-0.3141728  0.68040586  1.3511285    -0.29235613\n0.03947779  -1.1260709  0.16128083   -1.1656744\n\n(1,2,.,.) =\n1.0182372   -0.6030568  -1.5335841   0.37804475\n0.26944965  -1.6720067  0.2405665    -0.95661074\n-0.31286374 0.109459646 -1.6644431   -1.9295278\n\n(2,1,.,.) =\n1.0210015   -0.69647574 -0.629564    1.6719679\n-0.7825565  -0.48921636 0.1892077    0.17827414\n0.76913565  0.17354056  -0.5749589   -1.736962\n\n(2,2,.,.) =\n0.82071537  -0.22566034 0.12415939   0.02941268\n0.34600595  0.86877316  0.9797952    -1.7793267\n0.025843443 -1.6373945  -0.093925744 -0.22479358\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n1.3511285   1.0182372\n1.6719679   0.9797952\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalMaxPooling2D\n\nmodel = Sequential()\nmodel.add(GlobalMaxPooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.4037502  0.02214535 0.83987632 0.63656679]\n   [0.57757778 0.41884406 0.94446159 0.55776242]\n   [0.40213234 0.4463349  0.04457756 0.99123233]]\n  [[0.71114675 0.88609155 0.40299526 0.01648121]\n   [0.23102758 0.89673552 0.07030409 0.79674961]\n   [0.84665248 0.18257089 0.87211872 0.22697933]]]\n\n [[[0.08033122 0.26298654 0.10863184 0.57894922]\n   [0.03999134 0.90867755 0.80473921 0.79913378]\n   [0.60443084 0.92055786 0.17994007 0.87414516]]\n  [[0.50193442 0.52639178 0.72124789 0.41776979]\n   [0.09495006 0.91797563 0.48755794 0.50458372]\n   [0.47387433 0.93445126 0.83216554 0.67275364]]]]  Output is:  [[0.99123234 0.8967355]\n [0.92055786 0.9344513]]",
            "title": "GlobalMaxPooling2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#globalmaxpooling3d",
            "text": "Applies global max pooling operation for 3D data.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'valid'.  The input of this layer should be 5D.  Scala:  GlobalMaxPooling3D(dimOrdering = \"th\", inputShape = null)  Python:  GlobalMaxPooling3D(dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalMaxPooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalMaxPooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.5882856      0.6403546    -0.42084476\n-0.097307995   -0.52530056  -0.36757985\n\n(1,1,2,.,.) =\n0.60983604  -0.24730012 -0.07744695\n0.36276138  0.34722528  0.19874145\n\n(1,2,1,.,.) =\n-0.912143   0.14050196  2.1116483\n-0.67692965 -0.5708391  -2.1040971\n\n(1,2,2,.,.) =\n2.1500692   1.1697202   1.364164\n1.2241726   -0.12069768 1.2471954\n\n(2,1,1,.,.) =\n0.39550102  -0.7435119  0.47669584\n-0.17335615 0.2690476   -0.8462402\n\n(2,1,2,.,.) =\n-1.0553921  -0.35153934 0.8036665\n-1.029019   -0.64534503 0.94537926\n\n(2,2,1,.,.) =\n0.5388452   -0.27233714 1.5837694\n1.0976856   -0.20959699 1.6285672\n\n(2,2,2,.,.) =\n-0.7736055  0.58593166  -1.2158531\n1.2194971   1.4081163   1.2056179\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.6403546   2.1500692\n0.94537926  1.6285672\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalMaxPooling3D\n\nmodel = Sequential()\nmodel.add(GlobalMaxPooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[[0.35670186 0.12860978 0.99958102]\n    [0.17345679 0.19473725 0.41235665]]\n   [[0.98948429 0.26686797 0.4632353 ]\n    [0.15791828 0.07452679 0.73215605]]]\n  [[[0.80305568 0.73208753 0.31179214]\n    [0.43452576 0.563038   0.65955869]]\n   [[0.31947806 0.00899334 0.55208827]\n    [0.57471665 0.10157217 0.42698318]]]]\n\n [[[[0.59277903 0.35379325 0.5311834 ]\n    [0.91781414 0.10407255 0.58049721]]\n   [[0.14371521 0.24279466 0.26071055]\n    [0.89431752 0.66817043 0.61662462]]]\n  [[[0.6672706  0.38855847 0.88462881]\n    [0.38859986 0.80439572 0.27661295]]\n   [[0.41453042 0.11527795 0.75953012]\n    [0.77940987 0.26283438 0.97745039]]]]]  Output is:  [[0.99958104 0.8030557]\n [0.91781414 0.9774504]]",
            "title": "GlobalMaxPooling3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#globalaveragepooling1d",
            "text": "Global average pooling operation for temporal data.  The input of this layer should be 3D.  Scala:  GlobalAveragePooling1D(inputShape = null)  Python:  GlobalAveragePooling1D(input_shape=None, name=None)  Parameters:   inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalAveragePooling1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalAveragePooling1D(inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.52390736  -0.2733816  0.124149635\n-1.351596   -1.1435038  -1.5176618\n\n(2,.,.) =\n1.0428048   -0.65227276 -0.44158915\n-0.23790422 0.4179904   -0.12358317\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.41384432 -0.7084427  -0.69675606\n0.40245032  -0.11714119 -0.28258616\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalAveragePooling1D\n\nmodel = Sequential()\nmodel.add(GlobalAveragePooling1D(input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)  Input is:  [[[0.93869359 0.22245741 0.9744004 ]\n  [0.89151128 0.8211663  0.73579694]]\n\n [[0.37929716 0.509159   0.21713254]\n  [0.81838451 0.72323228 0.0370643 ]]]  Output is:  [[0.9151024   0.52181184 0.85509866]\n [0.59884083  0.6161956  0.12709841]]",
            "title": "GlobalAveragePooling1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#globalaveragepooling2d",
            "text": "Global average pooling operation for spatial data.  The input of this layer should be 4D.  Scala:  GlobalAveragePooling2D(dimOrdering = \"th\", inputShape = null)  Python:  GlobalAveragePooling2D(dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalAveragePooling2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalAveragePooling2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.110688895    -0.95155084  1.8221924   -2.0326483\n-0.013243215   -1.2766567   -0.16704278 -0.97121066\n-2.4606674     -0.24223651  -0.5687073  0.69842345\n\n(1,2,.,.) =\n0.14165956     0.17032783   2.5329256   0.011501087\n-0.3236992     1.1332442    0.18139894  -2.3126595\n0.1546373      0.35264283   -0.04404357 -0.70906943\n\n(2,1,.,.) =\n-0.08527824    0.29270124   -0.7355773  -0.6026267\n-0.71629876    0.83938205   0.5129336   0.118145116\n0.17555784     -0.8842884   0.12628363  -0.5556226\n\n(2,2,.,.) =\n0.6230317      0.64954233   -1.3002442  -0.44802713\n-0.7294096     0.29014868   -0.55649257 2.1427174\n0.0146621745   0.67039204   0.12979278  1.8543824\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.5043883  0.10740545\n-0.12622404 0.27837467\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalAveragePooling2D\n\nmodel = Sequential()\nmodel.add(GlobalAveragePooling2D(input_shape = (2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.44851152 0.94140516 0.45500829 0.07239139]\n   [0.58724461 0.7386701  0.69641719 0.70497337]\n   [0.15950558 0.56006247 0.82534941 0.59303245]]\n  [[0.94628326 0.75747177 0.92495215 0.16233194]\n   [0.21553426 0.65968036 0.72130258 0.8929379 ]\n   [0.91295078 0.36362834 0.04734189 0.32399088]]]\n\n [[[0.74069289 0.8804913  0.38783329 0.82279268]\n   [0.29561186 0.86405938 0.21608269 0.618583  ]\n   [0.16823803 0.65690701 0.85394726 0.94541932]]\n  [[0.33876558 0.47517543 0.25908204 0.81933296]\n   [0.16176792 0.57166    0.28295922 0.95254489]\n   [0.10532106 0.98495855 0.41048516 0.86755462]]]]  Output is:  [[0.5652142 0.5773672]\n [0.6208883 0.519134 ]]",
            "title": "GlobalAveragePooling2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/pooling/#globalaveragepooling3d",
            "text": "Applies global average pooling operation for 3D data.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'valid'.  The input of this layer should be 5D.  Scala:  GlobalAveragePooling3D(dimOrdering = \"th\", inputShape = null)  Python:  GlobalAveragePooling3D(dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GlobalAveragePooling3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GlobalAveragePooling3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n1.8996966    -0.20018125  -0.3271749\n0.27594963   -1.0520669   0.86003053\n\n(1,1,2,.,.) =\n-0.7652662   0.72945994   0.9008456\n0.8692407    -1.1327444   2.0664887\n\n(1,2,1,.,.) =\n0.10636215   -0.812925    -0.3757974\n0.48761207   0.017417012  -2.395701\n\n(1,2,2,.,.) =\n-1.3122851   -0.5942121   -0.6180062\n-0.032230377 -0.27521232  -0.3567782\n\n(2,1,1,.,.) =\n1.8668615    -0.4244298   1.0701258\n0.63794065   -1.023562    0.16939393\n\n(2,1,2,.,.) =\n0.20582832   0.5321886    -1.5412451\n-0.38068503  1.4506307    -0.47838798\n\n(2,2,1,.,.) =\n-0.7344984   -0.28647164  2.410416\n-1.8175911   -1.1973995   1.001777\n\n(2,2,2,.,.) =\n-0.09646813  0.11988298   1.4687495\n1.493955     0.16738588   1.133337\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.34368983  -0.51347965\n0.17372166  0.30525622\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GlobalAveragePooling3D\n\nmodel = Sequential()\nmodel.add(GlobalAveragePooling3D(input_shape = (2, 2, 2, 3)))\ninput = np.random.random([2, 2, 2, 2, 3])\noutput = model.forward(input)  Input is:  [[[[[0.38403874 0.30696173 0.25682854]\n    [0.53124253 0.62668969 0.21927777]]\n   [[0.33040063 0.37388563 0.75210039]\n    [0.08358634 0.80063745 0.13251887]]]\n  [[[0.41724617 0.2241106  0.55527267]\n    [0.69493785 0.71098284 0.54058444]]\n   [[0.4773658  0.92236993 0.76933649]\n    [0.45217032 0.61153948 0.01976393]]]]\n\n [[[[0.27256789 0.56008397 0.19898919]\n    [0.44973465 0.66605998 0.77117999]]\n   [[0.07868799 0.94786045 0.2240451 ]\n    [0.92261946 0.4053334  0.2572511 ]]]\n  [[[0.33754374 0.28838802 0.79900278]\n    [0.26374789 0.25610211 0.9320699 ]]\n   [[0.19518511 0.80707822 0.29660536]\n    [0.56917623 0.07653736 0.77836375]]]]]  Output is:  [[0.3998474  0.53297335]\n [0.47953442 0.46665   ]]",
            "title": "GlobalAveragePooling3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/recurrent/",
            "text": "SimpleRNN\n\n\nA fully-connected recurrent neural network cell. The output is to be fed back to input.\n\n\nThe input of this layer should be 3D, i.e. (batch, time steps, input dim).\n\n\nScala:\n\n\nSimpleRNN(outputDim, activation = \"tanh\", returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)\n\n\n\n\nPython:\n\n\nSimpleRNN(output_dim, activation=\"tanh\", return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\noutputDim\n: Hidden unit size. Dimension of internal projections and final output.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is 'tanh'.\n\n\nreturnSequences\n: Whether to return the full sequence or only return the last output in the output sequence. Default is false.\n\n\ngoBackwards\n: Whether the input sequence will be processed backwards. Default is false.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nuRegularizer\n: An instance of \nRegularizer\n, applied the recurrent weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, SimpleRNN}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SimpleRNN(8, activation = \"relu\", inputShape = Shape(4, 5)))\nval input = Tensor[Float](2, 4, 5).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.71328646  0.24269831  -0.75013286 -1.6663225  0.35494477\n0.073439054 -1.1181073  -0.6577777  1.3154761   0.15396282\n0.41183218  -1.2667576  -0.11167632 0.946616    0.06427766\n0.013886308 -0.20620999 1.1173447   1.9083043   1.7680032\n\n(2,.,.) =\n-2.3510098  -0.8492037  0.042268332 -0.43801674 -0.010638754\n1.298793    -0.24814601 0.31325665  -0.19119295 -2.072075\n-0.11629801 0.27296612  0.94443846  0.37293285  -0.82289046\n0.6044998   0.93386084  -1.3502276  -1.7753356  1.6173482\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x5]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.0  0.020557694  0.0   0.39700085  0.622244  0.0   0.36524248  0.88961613\n0.0  1.4797685    0.0   0.0         0.0       0.0   0.0         0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SimpleRNN\n\nmodel = Sequential()\nmodel.add(SimpleRNN(8, activation = \"relu\", input_shape = (4, 5)))\ninput = np.random.random([2, 4, 5])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.43400622 0.65452575 0.94952774 0.96210478 0.05286231]\n  [0.2162183  0.33225502 0.09725628 0.80813221 0.29556109]\n  [0.19720487 0.35077585 0.80904872 0.80576513 0.82035253]\n  [0.36175687 0.63291153 0.08437936 0.71581099 0.790709  ]]\n\n [[0.35387003 0.36532078 0.9834315  0.07562338 0.05600369]\n  [0.65927201 0.14652252 0.10848068 0.88225065 0.88871385]\n  [0.23627135 0.72620104 0.60391828 0.51571874 0.73550574]\n  [0.80773506 0.35121494 0.66889362 0.530684   0.52066982]]]\n\n\n\n\nOutput is:\n\n\n[[0.77534926 0.23742369 0.14946866 0.0        0.16289112 0.0  0.71689016 0.24594748]\n [0.8987881  0.06123672 0.3312829  0.29757586 0.0        0.0  1.0179179  0.23447856]]\n\n\n\n\n\n\nLSTM\n\n\nLong Short Term Memory unit architecture.\n\n\nThe input of this layer should be 3D, i.e. (batch, time steps, input dim).\n\n\nScala:\n\n\nLSTM(outputDim, activation = \"tanh\", innerActivation = \"hard_sigmoid\", returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)\n\n\n\n\nPython:\n\n\nLSTM(output_dim, activation=\"tanh\", inner_activation=\"hard_sigmoid\", return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\noutputDim\n: Hidden unit size. Dimension of internal projections and final output.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is 'tanh'.\n\n\ninnerActivation\n: String representation of the activation function for inner cells. See \nhere\n for available activation strings. Default is 'hard_sigmoid'.\n\n\nreturnSequences\n: Whether to return the full sequence or only return the last output in the output sequence. Default is false.\n\n\ngoBackwards\n: Whether the input sequence will be processed backwards. Default is false.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nuRegularizer\n: An instance of \nRegularizer\n, applied the recurrent weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, LSTM}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LSTM(8, inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.3485646   0.38385049  0.676986\n0.13189854  0.30926105  0.4539456\n\n(2,.,.) =\n-1.7166822  -0.71257055 -0.477679\n-0.36572325 -0.5534503  -0.018431915\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.20168768 -0.20359062 -0.11801678 -0.08987579 0.20480658  -0.05170132 -0.048530716    0.08447949\n-0.07134238 -0.11233686 0.073534355 0.047955263 0.13415548  0.12862797  -0.07839044     0.28296617\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(8, input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.84004043 0.2081865  0.76093342]\n  [0.06878797 0.13804673 0.23251666]]\n\n [[0.24651173 0.5650254  0.41424478]\n  [0.49338729 0.40505622 0.01497762]]]\n\n\n\n\nOutput is:\n\n\n[[ 0.01089199  0.02563154 -0.04335827  0.03037791  0.11265078 -0.17756112\n   0.14166507  0.01017009]\n [ 0.0144811   0.03360332  0.00676281 -0.01473055  0.09639315 -0.16620669\n   0.07391933  0.01746811]]\n\n\n\n\n\n\nGRU\n\n\nGated Recurrent Unit architecture.\n\n\nThe input of this layer should be 3D, i.e. (batch, time steps, input dim).\n\n\nScala:\n\n\nGRU(outputDim, activation = \"tanh\", innerActivation = \"hard_sigmoid\", returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)\n\n\n\n\nPython:\n\n\nGRU(output_dim, activation=\"tanh\", inner_activation=\"hard_sigmoid\", return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\noutputDim\n: Hidden unit size. Dimension of internal projections and final output.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is 'tanh'.\n\n\ninnerActivation\n: String representation of the activation function for inner cells. See \nhere\n for available activation strings. Default is 'hard_sigmoid'.\n\n\nreturnSequences\n: Whether to return the full sequence or only return the last output in the output sequence. Default is false.\n\n\ngoBackwards\n: Whether the input sequence will be processed backwards. Default is false.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nuRegularizer\n: An instance of \nRegularizer\n, applied the recurrent weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GRU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GRU(8, inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.010477358 -1.1201298  -0.86472356\n0.12688802   -0.6696582  0.08027417\n\n(2,.,.) =\n0.1724209    -0.52319324 -0.8808063\n0.17918338   -0.552886   -0.11891741\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.12018716  -0.31560755    0.2867627   0.6728765   0.13287778  0.2112865   0.13381396  -0.4267934\n-0.18521798  -0.30512968    0.14875418  0.63962734  0.1841841   0.25272882  0.016909363 -0.38463163\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GRU\n\nmodel = Sequential()\nmodel.add(GRU(8, input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.25026651 0.35433442 0.01417391]\n  [0.77236921 0.97315472 0.66090386]]\n\n [[0.76037554 0.41029034 0.68725938]\n  [0.17888889 0.67670088 0.70580547]]]\n\n\n\n\nOutput is:\n\n\n[[-0.03584666  0.07984452 -0.06159414 -0.13331707  0.34015405 -0.07107028  0.12444386 -0.06606203]\n [ 0.02881907  0.04856917 -0.15306929 -0.24991018  0.23814955  0.0303434   0.06634206 -0.15335503]]\n\n\n\n\n\n\nHighway\n\n\nDensely connected highway network.\n\n\nHighway layers are a natural extension of LSTMs to feedforward networks.\n\n\nThe input of this layer should be 2D, i.e. (batch, input dim).\n\n\nScala:\n\n\nHighway(activation = null, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)\n\n\n\n\nPython:\n\n\nHighway(activation=None, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is null.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nbias\n: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Highway}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Highway(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.26041138 0.4286919   1.723103\n1.4516269   0.5557163   -0.1149741\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.006746907    -0.109112576    1.3375516\n0.6065166   0.41575465  -0.06849813\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Highway\n\nmodel = Sequential()\nmodel.add(Highway(input_shape = (3)))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.5762107  0.45679288 0.00370956]\n [0.24133312 0.38104653 0.05249192]]\n\n\n\n\nOutput is:\n\n\n[[0.5762107  0.4567929  0.00370956]\n [0.24133313 0.38104653 0.05249191]]\n\n\n\n\n\n\nConvLSTM2D\n\n\nConvolutional LSTM.\n\n\nData format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').\n\n\nBorder mode currently supported for this layer is 'same'.\n\n\nThe convolution kernel for this layer is a square kernel with equal strides 'subsample'.\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nConvLSTM2D(nbFilter, nbKernel, activation = \"tanh\", innerActivation = \"hard_sigmoid\", dimOrdering = \"th\", subsample = 1, wRegularizer = null, uRegularizer = null, bRegularizer = null, returnSequences = false, goBackwards = false, inputShape = null)\n\n\n\n\nPython:\n\n\nConvLSTM2D(nb_filter, nb_row, nb_col, activation=\"tanh\", inner_activation=\"hard_sigmoid\", dim_ordering=\"th\", border_mode=\"same\", subsample=(1, 1), W_regularizer=None, U_regularizer=None, b_regularizer=None, return_sequences=False, go_backwards=False, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nnbFilter\n: Number of convolution filters to use.\n\n\nnbKernel\n: Number of rows/columns in the convolution kernel. Square kernel. In Python, require nb_row==nb_col.\n\n\nactivation\n: String representation of the activation function to use. See \nhere\n for available activation strings. Default is 'tanh'.\n\n\ninnerActivation\n: String representation of the activation function to use for inner cells. See \nhere\n for available activation strings. Default is 'hard_sigmoid'.\n\n\ndimOrdering\n: Format of input data. Only 'th' (Channel First) is supported for now.\n\n\nsubsample\n: Factor by which to subsample output. Also called strides elsewhere.\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\nuRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default is null.\n\n\nbRegularizer\n: An instance of \nRegularizer\n, applied to the bias. Default is null.\n\n\nreturnSequences\n: Whether to return the full sequence or the last output in the output sequence. Default is false.\n\n\ngoBackwards\n: Whether the input sequence will be processed backwards. Default is false.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, ConvLSTM2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ConvLSTM2D(2, 2, inputShape = Shape(1, 2, 2, 2)))\nval input = Tensor[Float](1, 1, 2, 2, 2).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-0.3935159  -2.0734277\n0.16473202  -1.0574125\n\n(1,1,2,.,.) =\n1.2325795   0.510846\n-0.4246685  -0.109434046\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2x2]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.12613402     0.035963967\n0.046498444     0.03568305\n\n(1,2,.,.) =\n-0.1547083      -0.046905644\n-0.115438126    -0.08817647\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ConvLSTM2D\n\nmodel = Sequential()\nmodel.add(ConvLSTM2D(2, 2, 2, input_shape=(1, 2, 2, 2)))\ninput = np.random.random([1, 1, 2, 2, 2])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.53293431 0.02606896]\n    [0.50916001 0.6927234 ]]\n\n   [[0.44282168 0.05963464]\n    [0.22863441 0.45312165]]]]]\n\n\n\n\nOutput is\n\n\n[[[[ 0.09322705  0.09817358]\n   [ 0.12197719  0.11264911]]\n\n  [[ -0.03922357 -0.11715978]\n   [ -0.01915754 -0.03141996]]]]",
            "title": "Recurrent Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/recurrent/#simplernn",
            "text": "A fully-connected recurrent neural network cell. The output is to be fed back to input.  The input of this layer should be 3D, i.e. (batch, time steps, input dim).  Scala:  SimpleRNN(outputDim, activation = \"tanh\", returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)  Python:  SimpleRNN(output_dim, activation=\"tanh\", return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, name=None)  Parameters:   outputDim : Hidden unit size. Dimension of internal projections and final output.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is 'tanh'.  returnSequences : Whether to return the full sequence or only return the last output in the output sequence. Default is false.  goBackwards : Whether the input sequence will be processed backwards. Default is false.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  uRegularizer : An instance of  Regularizer , applied the recurrent weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, SimpleRNN}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SimpleRNN(8, activation = \"relu\", inputShape = Shape(4, 5)))\nval input = Tensor[Float](2, 4, 5).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.71328646  0.24269831  -0.75013286 -1.6663225  0.35494477\n0.073439054 -1.1181073  -0.6577777  1.3154761   0.15396282\n0.41183218  -1.2667576  -0.11167632 0.946616    0.06427766\n0.013886308 -0.20620999 1.1173447   1.9083043   1.7680032\n\n(2,.,.) =\n-2.3510098  -0.8492037  0.042268332 -0.43801674 -0.010638754\n1.298793    -0.24814601 0.31325665  -0.19119295 -2.072075\n-0.11629801 0.27296612  0.94443846  0.37293285  -0.82289046\n0.6044998   0.93386084  -1.3502276  -1.7753356  1.6173482\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x5]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.0  0.020557694  0.0   0.39700085  0.622244  0.0   0.36524248  0.88961613\n0.0  1.4797685    0.0   0.0         0.0       0.0   0.0         0.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SimpleRNN\n\nmodel = Sequential()\nmodel.add(SimpleRNN(8, activation = \"relu\", input_shape = (4, 5)))\ninput = np.random.random([2, 4, 5])\noutput = model.forward(input)  Input is:  [[[0.43400622 0.65452575 0.94952774 0.96210478 0.05286231]\n  [0.2162183  0.33225502 0.09725628 0.80813221 0.29556109]\n  [0.19720487 0.35077585 0.80904872 0.80576513 0.82035253]\n  [0.36175687 0.63291153 0.08437936 0.71581099 0.790709  ]]\n\n [[0.35387003 0.36532078 0.9834315  0.07562338 0.05600369]\n  [0.65927201 0.14652252 0.10848068 0.88225065 0.88871385]\n  [0.23627135 0.72620104 0.60391828 0.51571874 0.73550574]\n  [0.80773506 0.35121494 0.66889362 0.530684   0.52066982]]]  Output is:  [[0.77534926 0.23742369 0.14946866 0.0        0.16289112 0.0  0.71689016 0.24594748]\n [0.8987881  0.06123672 0.3312829  0.29757586 0.0        0.0  1.0179179  0.23447856]]",
            "title": "SimpleRNN"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/recurrent/#lstm",
            "text": "Long Short Term Memory unit architecture.  The input of this layer should be 3D, i.e. (batch, time steps, input dim).  Scala:  LSTM(outputDim, activation = \"tanh\", innerActivation = \"hard_sigmoid\", returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)  Python:  LSTM(output_dim, activation=\"tanh\", inner_activation=\"hard_sigmoid\", return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, input_shape=None, name=None)  Parameters:   outputDim : Hidden unit size. Dimension of internal projections and final output.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is 'tanh'.  innerActivation : String representation of the activation function for inner cells. See  here  for available activation strings. Default is 'hard_sigmoid'.  returnSequences : Whether to return the full sequence or only return the last output in the output sequence. Default is false.  goBackwards : Whether the input sequence will be processed backwards. Default is false.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  uRegularizer : An instance of  Regularizer , applied the recurrent weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, LSTM}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LSTM(8, inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n1.3485646   0.38385049  0.676986\n0.13189854  0.30926105  0.4539456\n\n(2,.,.) =\n-1.7166822  -0.71257055 -0.477679\n-0.36572325 -0.5534503  -0.018431915\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.20168768 -0.20359062 -0.11801678 -0.08987579 0.20480658  -0.05170132 -0.048530716    0.08447949\n-0.07134238 -0.11233686 0.073534355 0.047955263 0.13415548  0.12862797  -0.07839044     0.28296617\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(8, input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)  Input is:  [[[0.84004043 0.2081865  0.76093342]\n  [0.06878797 0.13804673 0.23251666]]\n\n [[0.24651173 0.5650254  0.41424478]\n  [0.49338729 0.40505622 0.01497762]]]  Output is:  [[ 0.01089199  0.02563154 -0.04335827  0.03037791  0.11265078 -0.17756112\n   0.14166507  0.01017009]\n [ 0.0144811   0.03360332  0.00676281 -0.01473055  0.09639315 -0.16620669\n   0.07391933  0.01746811]]",
            "title": "LSTM"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/recurrent/#gru",
            "text": "Gated Recurrent Unit architecture.  The input of this layer should be 3D, i.e. (batch, time steps, input dim).  Scala:  GRU(outputDim, activation = \"tanh\", innerActivation = \"hard_sigmoid\", returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)  Python:  GRU(output_dim, activation=\"tanh\", inner_activation=\"hard_sigmoid\", return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, name=None)  Parameters:   outputDim : Hidden unit size. Dimension of internal projections and final output.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is 'tanh'.  innerActivation : String representation of the activation function for inner cells. See  here  for available activation strings. Default is 'hard_sigmoid'.  returnSequences : Whether to return the full sequence or only return the last output in the output sequence. Default is false.  goBackwards : Whether the input sequence will be processed backwards. Default is false.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  uRegularizer : An instance of  Regularizer , applied the recurrent weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GRU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GRU(8, inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.010477358 -1.1201298  -0.86472356\n0.12688802   -0.6696582  0.08027417\n\n(2,.,.) =\n0.1724209    -0.52319324 -0.8808063\n0.17918338   -0.552886   -0.11891741\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.12018716  -0.31560755    0.2867627   0.6728765   0.13287778  0.2112865   0.13381396  -0.4267934\n-0.18521798  -0.30512968    0.14875418  0.63962734  0.1841841   0.25272882  0.016909363 -0.38463163\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GRU\n\nmodel = Sequential()\nmodel.add(GRU(8, input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)  Input is:  [[[0.25026651 0.35433442 0.01417391]\n  [0.77236921 0.97315472 0.66090386]]\n\n [[0.76037554 0.41029034 0.68725938]\n  [0.17888889 0.67670088 0.70580547]]]  Output is:  [[-0.03584666  0.07984452 -0.06159414 -0.13331707  0.34015405 -0.07107028  0.12444386 -0.06606203]\n [ 0.02881907  0.04856917 -0.15306929 -0.24991018  0.23814955  0.0303434   0.06634206 -0.15335503]]",
            "title": "GRU"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/recurrent/#highway",
            "text": "Densely connected highway network.  Highway layers are a natural extension of LSTMs to feedforward networks.  The input of this layer should be 2D, i.e. (batch, input dim).  Scala:  Highway(activation = null, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)  Python:  Highway(activation=None, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)  Parameters:   activation : String representation of the activation function to use. See  here  for available activation strings. Default is null.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  bias : Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Highway}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Highway(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.26041138 0.4286919   1.723103\n1.4516269   0.5557163   -0.1149741\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.006746907    -0.109112576    1.3375516\n0.6065166   0.41575465  -0.06849813\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Highway\n\nmodel = Sequential()\nmodel.add(Highway(input_shape = (3)))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.5762107  0.45679288 0.00370956]\n [0.24133312 0.38104653 0.05249192]]  Output is:  [[0.5762107  0.4567929  0.00370956]\n [0.24133313 0.38104653 0.05249191]]",
            "title": "Highway"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/recurrent/#convlstm2d",
            "text": "Convolutional LSTM.  Data format currently supported for this layer is 'CHANNEL_FIRST' (dimOrdering='th').  Border mode currently supported for this layer is 'same'.  The convolution kernel for this layer is a square kernel with equal strides 'subsample'.  The input of this layer should be 5D.  Scala:  ConvLSTM2D(nbFilter, nbKernel, activation = \"tanh\", innerActivation = \"hard_sigmoid\", dimOrdering = \"th\", subsample = 1, wRegularizer = null, uRegularizer = null, bRegularizer = null, returnSequences = false, goBackwards = false, inputShape = null)  Python:  ConvLSTM2D(nb_filter, nb_row, nb_col, activation=\"tanh\", inner_activation=\"hard_sigmoid\", dim_ordering=\"th\", border_mode=\"same\", subsample=(1, 1), W_regularizer=None, U_regularizer=None, b_regularizer=None, return_sequences=False, go_backwards=False, input_shape=None, name=None)  Parameters:   nbFilter : Number of convolution filters to use.  nbKernel : Number of rows/columns in the convolution kernel. Square kernel. In Python, require nb_row==nb_col.  activation : String representation of the activation function to use. See  here  for available activation strings. Default is 'tanh'.  innerActivation : String representation of the activation function to use for inner cells. See  here  for available activation strings. Default is 'hard_sigmoid'.  dimOrdering : Format of input data. Only 'th' (Channel First) is supported for now.  subsample : Factor by which to subsample output. Also called strides elsewhere.  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  uRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the recurrent weights matrices. Default is null.  bRegularizer : An instance of  Regularizer , applied to the bias. Default is null.  returnSequences : Whether to return the full sequence or the last output in the output sequence. Default is false.  goBackwards : Whether the input sequence will be processed backwards. Default is false.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, ConvLSTM2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ConvLSTM2D(2, 2, inputShape = Shape(1, 2, 2, 2)))\nval input = Tensor[Float](1, 1, 2, 2, 2).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n-0.3935159  -2.0734277\n0.16473202  -1.0574125\n\n(1,1,2,.,.) =\n1.2325795   0.510846\n-0.4246685  -0.109434046\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x2x2x2]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n-0.12613402     0.035963967\n0.046498444     0.03568305\n\n(1,2,.,.) =\n-0.1547083      -0.046905644\n-0.115438126    -0.08817647\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x2x2x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ConvLSTM2D\n\nmodel = Sequential()\nmodel.add(ConvLSTM2D(2, 2, 2, input_shape=(1, 2, 2, 2)))\ninput = np.random.random([1, 1, 2, 2, 2])\noutput = model.forward(input)  Input is:  [[[[[0.53293431 0.02606896]\n    [0.50916001 0.6927234 ]]\n\n   [[0.44282168 0.05963464]\n    [0.22863441 0.45312165]]]]]  Output is  [[[[ 0.09322705  0.09817358]\n   [ 0.12197719  0.11264911]]\n\n  [[ -0.03922357 -0.11715978]\n   [ -0.01915754 -0.03141996]]]]",
            "title": "ConvLSTM2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/normalization/",
            "text": "BatchNormalization\n\n\nBatch normalization layer.\n\n\nNormalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n\n\nIt is a feature-wise normalization, each feature map in the input will be normalized separately.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nBatchNormalization(epsilon = 0.001, momentum = 0.99, betaInit = \"zero\", gammaInit = \"one\", dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nBatchNormalization(epsilon=0.001, momentum=0.99, beta_init=\"zero\", gamma_init=\"one\", dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nepsilon\n: Fuzz parameter. Default is 0.001.\n\n\nmomentum\n: Momentum in the computation of the exponential average of the mean and standard deviation of the data, for feature-wise normalization. Default is 0.99.\n\n\nbetaInit\n: Name of initialization function for shift parameter. See \nhere\n for available initialization strings. Default is 'zero'.\n\n\ngammaInit\n: Name of initialization function for scale parameter. See \nhere\n for available initialization strings. Default is 'one'.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'. For 'th', axis along which to normalize is 1. For 'tf', axis is 3.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, BatchNormalization}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(BatchNormalization(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.77107763    0.2937704    0.5191167    1.7458088\n0.6895759     1.1034386    0.076277375  0.73515415\n0.8190946     0.63958114   0.5226141    -0.42864776\n\n(1,2,.,.) =\n-0.121818945  0.34588146   0.055290654  -0.07994603\n0.6463561     0.13930246   1.5822772    0.5089318\n-0.21778189   -1.4048384   0.47113693   0.7929269\n\n(2,1,.,.) =\n0.6308846     -0.3855579   1.1685323    1.5646453\n0.06638282    -1.7852567   2.5698936    0.54044205\n1.020025      0.9537036    -0.95600724  2.0834947\n\n(2,2,.,.) =\n-0.5315871    -1.5204562   -0.19082998  -1.5210537\n0.35849532    0.15615761   -0.55561566  1.1889576\n-0.16226959   -2.1243215   1.1446979    -1.1057223\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.16141568    -0.3597713   -0.1137085   1.2257558\n0.07242136    0.5243313    -0.5972589   0.12218969\n0.21384695    0.017830472  -0.10988956  -1.1486028\n\n(1,2,.,.) =\n-0.03555677   0.4775637    0.15875259   0.010382571\n0.80721855    0.25092307   1.8340303    0.6564485\n-0.14083901   -1.4431748   0.6149832    0.96802336\n\n(2,1,.,.) =\n0.008334424   -1.1015517   0.5954091    1.0279375\n-0.6080631    -2.6299274   2.1256003    -0.090422675\n0.4332493     0.36083078   -1.7244436   1.5944856\n\n(2,2,.,.) =\n-0.48511901   -1.5700207   -0.11126971  -1.5706762\n0.49140254    0.26941508   -0.51148105  1.402514\n-0.07993572   -2.2325294   1.3539561    -1.1150105\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import BatchNormalization\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.01633039 0.08359466 0.31828698 0.31132638]\n   [0.82236941 0.34455877 0.40301781 0.09545177]\n   [0.32995004 0.21716768 0.40654485 0.0607145 ]]\n  [[0.04502162 0.90428985 0.54087212 0.78525733]\n   [0.02355475 0.86309013 0.25354746 0.88168388]\n   [0.77375427 0.74295181 0.43970331 0.07890251]]]\n\n [[[0.87290131 0.15790927 0.25248005 0.56290773]\n   [0.47154244 0.98287739 0.59877866 0.3287331 ]\n   [0.0048165  0.47392756 0.32070177 0.51298559]]\n  [[0.89172586 0.68240756 0.86829594 0.79287212]\n   [0.13308157 0.04279427 0.59920687 0.26807939]\n   [0.42409288 0.54029318 0.65308363 0.90739643]]]]\n\n\n\n\nOutput is\n\n\n[[[[-1.3824786   -1.1216924   -0.21178117  -0.2387677 ]\n   [ 1.7425659   -0.10992443  0.11672354   -1.075722  ]\n   [-0.16656308  -0.6038247   0.13039804   -1.2103996 ]]\n  [[-1.6169451   1.149055     -0.02079336  0.7658872 ]\n   [-1.6860473   1.0164324    -0.9456966   1.076286  ]\n   [ 0.7288585   0.6297049    -0.3464575   -1.5078819 ]]]\n\n [[[ 1.93848     -0.8335718   -0.4669172   0.73662305]\n   [ 0.3823962   2.3648615    0.87569594   -0.17128123]\n   [-1.4271184   0.39164335   -0.20241894  0.5430728 ]]\n  [[ 1.1086112   0.43481186   1.0331899    0.7903993 ]\n   [-1.3334786   -1.6241149   0.16698733   -0.89891803]\n   [-0.39670774  -0.02265698  0.3404176    1.1590551 ]]]]",
            "title": "Normalization Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/normalization/#batchnormalization",
            "text": "Batch normalization layer.  Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.  It is a feature-wise normalization, each feature map in the input will be normalized separately.  The input of this layer should be 4D.  Scala:  BatchNormalization(epsilon = 0.001, momentum = 0.99, betaInit = \"zero\", gammaInit = \"one\", dimOrdering = \"th\", inputShape = null)  Python:  BatchNormalization(epsilon=0.001, momentum=0.99, beta_init=\"zero\", gamma_init=\"one\", dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   epsilon : Fuzz parameter. Default is 0.001.  momentum : Momentum in the computation of the exponential average of the mean and standard deviation of the data, for feature-wise normalization. Default is 0.99.  betaInit : Name of initialization function for shift parameter. See  here  for available initialization strings. Default is 'zero'.  gammaInit : Name of initialization function for scale parameter. See  here  for available initialization strings. Default is 'one'.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'. For 'th', axis along which to normalize is 1. For 'tf', axis is 3.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, BatchNormalization}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(BatchNormalization(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n0.77107763    0.2937704    0.5191167    1.7458088\n0.6895759     1.1034386    0.076277375  0.73515415\n0.8190946     0.63958114   0.5226141    -0.42864776\n\n(1,2,.,.) =\n-0.121818945  0.34588146   0.055290654  -0.07994603\n0.6463561     0.13930246   1.5822772    0.5089318\n-0.21778189   -1.4048384   0.47113693   0.7929269\n\n(2,1,.,.) =\n0.6308846     -0.3855579   1.1685323    1.5646453\n0.06638282    -1.7852567   2.5698936    0.54044205\n1.020025      0.9537036    -0.95600724  2.0834947\n\n(2,2,.,.) =\n-0.5315871    -1.5204562   -0.19082998  -1.5210537\n0.35849532    0.15615761   -0.55561566  1.1889576\n-0.16226959   -2.1243215   1.1446979    -1.1057223\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.16141568    -0.3597713   -0.1137085   1.2257558\n0.07242136    0.5243313    -0.5972589   0.12218969\n0.21384695    0.017830472  -0.10988956  -1.1486028\n\n(1,2,.,.) =\n-0.03555677   0.4775637    0.15875259   0.010382571\n0.80721855    0.25092307   1.8340303    0.6564485\n-0.14083901   -1.4431748   0.6149832    0.96802336\n\n(2,1,.,.) =\n0.008334424   -1.1015517   0.5954091    1.0279375\n-0.6080631    -2.6299274   2.1256003    -0.090422675\n0.4332493     0.36083078   -1.7244436   1.5944856\n\n(2,2,.,.) =\n-0.48511901   -1.5700207   -0.11126971  -1.5706762\n0.49140254    0.26941508   -0.51148105  1.402514\n-0.07993572   -2.2325294   1.3539561    -1.1150105\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import BatchNormalization\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.01633039 0.08359466 0.31828698 0.31132638]\n   [0.82236941 0.34455877 0.40301781 0.09545177]\n   [0.32995004 0.21716768 0.40654485 0.0607145 ]]\n  [[0.04502162 0.90428985 0.54087212 0.78525733]\n   [0.02355475 0.86309013 0.25354746 0.88168388]\n   [0.77375427 0.74295181 0.43970331 0.07890251]]]\n\n [[[0.87290131 0.15790927 0.25248005 0.56290773]\n   [0.47154244 0.98287739 0.59877866 0.3287331 ]\n   [0.0048165  0.47392756 0.32070177 0.51298559]]\n  [[0.89172586 0.68240756 0.86829594 0.79287212]\n   [0.13308157 0.04279427 0.59920687 0.26807939]\n   [0.42409288 0.54029318 0.65308363 0.90739643]]]]  Output is  [[[[-1.3824786   -1.1216924   -0.21178117  -0.2387677 ]\n   [ 1.7425659   -0.10992443  0.11672354   -1.075722  ]\n   [-0.16656308  -0.6038247   0.13039804   -1.2103996 ]]\n  [[-1.6169451   1.149055     -0.02079336  0.7658872 ]\n   [-1.6860473   1.0164324    -0.9456966   1.076286  ]\n   [ 0.7288585   0.6297049    -0.3464575   -1.5078819 ]]]\n\n [[[ 1.93848     -0.8335718   -0.4669172   0.73662305]\n   [ 0.3823962   2.3648615    0.87569594   -0.17128123]\n   [-1.4271184   0.39164335   -0.20241894  0.5430728 ]]\n  [[ 1.1086112   0.43481186   1.0331899    0.7903993 ]\n   [-1.3334786   -1.6241149   0.16698733   -0.89891803]\n   [-0.39670774  -0.02265698  0.3404176    1.1590551 ]]]]",
            "title": "BatchNormalization"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/embedding/",
            "text": "Embedding\n\n\nTurn positive integers (indexes) into dense vectors of fixed size.\n\n\nThe input of this layer should be 2D.\n\n\nScala:\n\n\nEmbedding(inputDim, outputDim, init = \"uniform\", wRegularizer = null, inputShape = null)\n\n\n\n\nPython:\n\n\nEmbedding(input_dim, output_dim, init=\"uniform\", W_regularizer=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ninputDim\n: Int > 0. Size of the vocabulary.\n\n\noutputDim\n: Int >= 0. Dimension of the dense embedding.\n\n\ninit\n: String representation of the initialization method for the weights of the layer. See \nhere\n for available initialization strings. Default is \"uniform\".\n\n\nwRegularizer\n: An instance of \nRegularizer\n, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Embedding}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Embedding(8, 2, inputShape = Shape(4)))\nval input = Tensor[Float](2, 4)\ninput(Array(1, 1)) = 1\ninput(Array(1, 2)) = 2\ninput(Array(1, 3)) = 4\ninput(Array(1, 4)) = 5\ninput(Array(2, 1)) = 4\ninput(Array(2, 2)) = 3\ninput(Array(2, 3)) = 2\ninput(Array(2, 4)) = 6\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 2.0 4.0 5.0\n4.0 3.0 2.0 6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.03256504      -0.043232664\n-0.044753443    0.026075097\n0.045668535     0.02456015\n0.021222712     -0.04373116\n\n(2,.,.) =\n0.045668535     0.02456015\n0.03761902      -0.0014174521\n-0.044753443    0.026075097\n-0.030343587    -0.0015718295\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x2]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Embedding\n\nmodel = Sequential()\nmodel.add(Embedding(8, 2, input_shape=(4,)))\ninput = np.random.randint(4, size=(2, 4))\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0 2 2 2]\n [2 1 1 0]]\n\n\n\n\nOutput is\n\n\n[[[ 0.0094721  -0.01927968]\n  [-0.00483634 -0.03992473]\n  [-0.00483634 -0.03992473]\n  [-0.00483634 -0.03992473]]\n\n [[-0.00483634 -0.03992473]\n  [-0.03603687 -0.03708585]\n  [-0.03603687 -0.03708585]\n  [ 0.0094721  -0.01927968]]]",
            "title": "Embedding Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/embedding/#embedding",
            "text": "Turn positive integers (indexes) into dense vectors of fixed size.  The input of this layer should be 2D.  Scala:  Embedding(inputDim, outputDim, init = \"uniform\", wRegularizer = null, inputShape = null)  Python:  Embedding(input_dim, output_dim, init=\"uniform\", W_regularizer=None, input_shape=None, name=None)  Parameters:   inputDim : Int > 0. Size of the vocabulary.  outputDim : Int >= 0. Dimension of the dense embedding.  init : String representation of the initialization method for the weights of the layer. See  here  for available initialization strings. Default is \"uniform\".  wRegularizer : An instance of  Regularizer , (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Embedding}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Embedding(8, 2, inputShape = Shape(4)))\nval input = Tensor[Float](2, 4)\ninput(Array(1, 1)) = 1\ninput(Array(1, 2)) = 2\ninput(Array(1, 3)) = 4\ninput(Array(1, 4)) = 5\ninput(Array(2, 1)) = 4\ninput(Array(2, 2)) = 3\ninput(Array(2, 3)) = 2\ninput(Array(2, 4)) = 6\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n1.0 2.0 4.0 5.0\n4.0 3.0 2.0 6.0\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.03256504      -0.043232664\n-0.044753443    0.026075097\n0.045668535     0.02456015\n0.021222712     -0.04373116\n\n(2,.,.) =\n0.045668535     0.02456015\n0.03761902      -0.0014174521\n-0.044753443    0.026075097\n-0.030343587    -0.0015718295\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x2]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Embedding\n\nmodel = Sequential()\nmodel.add(Embedding(8, 2, input_shape=(4,)))\ninput = np.random.randint(4, size=(2, 4))\noutput = model.forward(input)  Input is:  [[0 2 2 2]\n [2 1 1 0]]  Output is  [[[ 0.0094721  -0.01927968]\n  [-0.00483634 -0.03992473]\n  [-0.00483634 -0.03992473]\n  [-0.00483634 -0.03992473]]\n\n [[-0.00483634 -0.03992473]\n  [-0.03603687 -0.03708585]\n  [-0.03603687 -0.03708585]\n  [ 0.0094721  -0.01927968]]]",
            "title": "Embedding"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/",
            "text": "Dropout\n\n\nApplies Dropout to the input by randomly setting a fraction 'p' of input units to 0 at each update during training time in order to prevent overfitting.\n\n\nScala:\n\n\nDropout(p, inputShape = null)\n\n\n\n\nPython:\n\n\nDropout(p, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\np\n: Fraction of the input units to drop. Between 0 and 1.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Dropout}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Dropout(0.3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.1256621     2.5402398    -1.1346831   0.50337905\n-1.3835752     0.9513693    -0.24547328  -0.28897092\n-0.0302343     -0.4106753   0.46467322   -0.7328933\n\n(2,.,.) =\n1.2569109      0.16947697   -0.5000246   2.0856402\n-0.04246076    1.5827807    -1.0235463   1.7278075\n-0.0035352164  -1.2579697   0.206815     -0.053890422\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.0           0.0        -1.620976  0.0\n-1.976536     1.359099   0.0        -0.4128156\n-0.043191858  -0.586679  0.6638189  -1.0469904\n\n(2,.,.) =\n0.0            0.0         -0.7143209  2.979486\n-0.060658228   2.2611153   0.0         0.0\n-0.0050503095  -1.7970997  0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Dropout\n\nmodel = Sequential()\nmodel.add(Dropout(0.3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.61976372 0.36074095 0.59003926 0.75373888]\n  [0.2390103  0.93491731 0.89078166 0.93083315]\n  [0.62360382 0.73646417 0.32886041 0.25372008]]\n\n [[0.10235195 0.7782206  0.54940485 0.41757437]\n  [0.94804637 0.04642807 0.17194449 0.2675274 ]\n  [0.89322413 0.3301816  0.49910094 0.00819342]]]\n\n\n\n\nOutput is\n\n\n[[[0.88537675 0.51534426 0.0        0.0       ]\n  [0.0        1.3355962  1.2725452  1.3297616 ]\n  [0.89086264 1.0520917  0.4698006  0.0       ]]\n\n [[0.14621708 1.1117437  0.7848641  0.59653485]\n  [1.354352   0.06632582 0.24563499 0.382182  ]\n  [1.2760345  0.471688   0.7130013  0.01170488]]]\n\n\n\n\n\n\nGaussianDropout\n\n\nApply multiplicative 1-centered Gaussian noise.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\nScala:\n\n\nGaussianDropout(p, inputShape = null)\n\n\n\n\nPython:\n\n\nGaussianDropout(p, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\np\n: Drop probability (as with 'Dropout'). The multiplicative noise will have standard deviation 'sqrt(p/(1-p))'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GaussianDropout}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GaussianDropout(0.45, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.14522108 0.27993536  -0.38809696 0.26372102\n-0.5572615  0.091684595 0.27881327  -1.6235427\n-0.32884964 -0.46456075 1.6169231   0.31943536\n\n(2,.,.) =\n-1.813811   1.1577623   -0.8995344  -1.0607182\n-0.3952898  -2.3437335  -0.6608733  1.1752778\n-1.3373735  -1.7404749  0.82832927  0.3053458\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.27553135  0.15290284   -0.23144199  0.619676\n-0.6648747   0.053253293  -0.08241931  -0.47651786\n-0.46381548  -1.0048811   1.5911313    0.39929882\n\n(2,.,.) =\n-0.43828326  0.4397059    -0.7071283   -1.440457\n-0.27415445  -1.6525689   -0.14050363  0.8728552\n-2.0516112   -2.1537325   1.4714862    0.29218474\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GaussianDropout\n\nmodel = Sequential()\nmodel.add(GaussianDropout(0.45, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.87208899 0.1353189  0.325058   0.63174633]\n  [0.20479221 0.29774652 0.42038452 0.23819006]\n  [0.07608872 0.91696766 0.245824   0.84324374]]\n\n [[0.26268714 0.76275494 0.63620997 0.15049668]\n  [0.54144135 0.70412821 0.05555471 0.72317157]\n  [0.32796076 0.26804862 0.80775221 0.46948471]]]\n\n\n\n\nOutput is\n\n\n[[[ 2.1392     -0.16185573 -0.18517245 -0.36539674]\n  [ 0.15324984  0.17320508  0.82520926  0.21734479]\n  [ 0.17601383  0.24906069  0.15664667  0.12675671]]\n\n [[ 0.49689308  1.8231225   1.0023257   0.37604305]\n  [ 1.2827866  -0.08726044  0.01333602  0.8518126 ]\n  [ 0.20021693  0.31828243  1.0940336   0.00866747]]]\n\n\n\n\n\n\nGaussianNoise\n\n\nApply additive zero-centered Gaussian noise.\n\n\nThis is useful to mitigate overfitting (you could see it as a form of random data augmentation).\n\n\nGaussian Noise is a natural choice as corruption process for real valued inputs.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\nScala:\n\n\nGaussianNoise(sigma, inputShape = null)\n\n\n\n\nPython:\n\n\nGaussianNoise(sigma, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nsigma\n: Standard deviation of the noise distribution.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, GaussianNoise}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GaussianNoise(0.6, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.4226985   -0.010519333  -0.49748304   -0.3176052\n0.52444375  0.31100306    1.0308859     2.0337727\n0.21513703  -0.396619     -0.055275716  -0.40603992\n\n(2,.,.) =\n-1.2393064  -0.536477     -0.35633054   -0.09068655\n-1.7297741  -0.5812992    -1.2833812    -0.7185058\n0.13474904  0.06468039    -0.6630115    1.2471422\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.72299504   0.7733576    -0.13965577   0.72079915\n0.20137814    0.6300731    2.5559645     2.3056328\n-0.19732013   -0.482926    -0.22114205   -0.88772345\n\n(2,.,.) =\n-1.4293398    -1.0870209   -0.5509953    -0.31268832\n-2.244024     -0.23773572  -3.022697     -0.65151817\n-0.035656676  -0.7470889   -0.8566216    1.1347939\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GaussianNoise\n\nmodel = Sequential()\nmodel.add(GaussianNoise(0.6, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.61699657 0.9759922  0.62898391 0.57265605]\n  [0.88815108 0.9484446  0.0300381  0.54114527]\n  [0.94046216 0.05998474 0.24860526 0.82020617]]\n\n [[0.87308242 0.24780141 0.73385444 0.40836049]\n  [0.33166358 0.74540915 0.28333526 0.08263288]\n  [0.17527315 0.79798327 0.49351559 0.13895365]]]\n\n\n\n\nOutput is\n\n\n[[[ 1.5833025   1.1431103   0.14338043  1.634818  ]\n  [ 0.01713479  1.1608562   0.222246    0.40559798]\n  [ 0.9930201   0.1187391   -0.35643864 -0.7164774 ]]\n\n [[ 1.0105296   1.423961    0.90040827  1.3460591 ]\n  [ 0.943779    -0.48430538 0.20670155  -0.50143087]\n  [ -0.29849088 0.12774569  -0.16126743 -0.011041  ]]]\n\n\n\n\n\n\nSpatialDropout1D\n\n\nSpatial 1D version of Dropout.\n\n\nThis version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.\n\n\nThe input of this layer should be 3D.\n\n\nScala:\n\n\nSpatialDropout1D(p = 0.5, inputShape = null)\n\n\n\n\nPython:\n\n\nSpatialDropout1D(p=0.5, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\np\n: Fraction of the input units to drop. Between 0 and 1.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, SpatialDropout1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SpatialDropout1D(inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.41961443 -0.3900255  -0.11937201 1.2904007\n-1.7623849  -1.6778483  -0.30053464 0.33295104\n-0.29824665 -0.25474855 -2.1878588  1.2741995\n\n(2,.,.) =\n0.24517925  2.0451863   -0.4281332  -1.2022524\n-0.7767442  0.24794191  -0.5614063  0.14720131\n-1.4832486  0.59478635  -0.13351384 -0.8799204\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.41961443 -0.0   -0.11937201  1.2904007\n-1.7623849  -0.0   -0.30053464  0.33295104\n-0.29824665 -0.0   -2.1878588   1.2741995\n\n(2,.,.) =\n0.24517925  0.0    -0.4281332   -0.0\n-0.7767442  0.0    -0.5614063   0.0\n-1.4832486  0.0    -0.13351384  -0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SpatialDropout1D\n\nmodel = Sequential()\nmodel.add(SpatialDropout1D(input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.67162434 0.91104925 0.66869854 0.17295748]\n  [0.78326617 0.27447329 0.18051406 0.24230118]\n  [0.7098933  0.32496974 0.00517668 0.21293476]]\n\n [[0.26932307 0.33496273 0.71258256 0.15464896]\n  [0.75286915 0.210486   0.91826256 0.81379954]\n  [0.11960744 0.37420041 0.03886506 0.22882457]]]\n\n\n\n\nOutput is\n\n\n[[[0.0        0.0        0.0        0.0       ]\n  [0.0        0.0        0.0        0.0       ]\n  [0.0        0.0        0.0        0.0       ]]\n\n [[0.0        0.33496273 0.0        0.15464896]\n  [0.0        0.210486   0.0        0.81379956]\n  [0.0        0.3742004  0.0        0.22882457]]]\n\n\n\n\n\n\nSpatialDropout2D\n\n\nSpatial 2D version of Dropout.\n\n\nThis version performs the same function as Dropout, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.\n\n\nThe input of this layer should be 4D.\n\n\nScala:\n\n\nSpatialDropout2D(p = 0.5, dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nSpatialDropout2D(p=0.5, dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\np\n: Fraction of the input units to drop. Between 0 and 1.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, SpatialDropout2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SpatialDropout2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.1651757   1.0867785   -0.56122786 -0.542156\n-0.79321486 0.64733976  -0.7040698  -0.8619171\n-0.61122066 -1.9640825  -1.0078672  -0.12195914\n\n(1,2,.,.) =\n-0.24738677 -0.9351172  -0.11694977 0.8657273\n-0.4773825  -1.6853696  -1.4906564  -0.06981948\n-0.8184341  -1.3537912  1.2442955   -0.0071462104\n\n(2,1,.,.) =\n1.8801081   0.44946647  0.47776535  0.036228795\n-1.2122079  0.41413695  -0.691067   2.6273472\n1.4293005   -1.2627622  -1.8263477  0.015581204\n\n(2,2,.,.) =\n2.0050068   -0.32893315 0.19670151  0.8031714\n0.16645809  -0.68172836 0.5169275   -0.83938134\n0.1789333   2.1845143   1.3843338   -0.8283524\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.0         -0.0        -0.0\n-0.0        0.0         -0.0        -0.0\n-0.0        -0.0        -0.0        -0.0\n\n(1,2,.,.) =\n-0.0        -0.0        -0.0        0.0\n-0.0        -0.0        -0.0        -0.0\n-0.0        -0.0        0.0         -0.0\n\n(2,1,.,.) =\n1.8801081   0.44946647  0.47776535  0.036228795\n-1.2122079  0.41413695  -0.691067   2.6273472\n1.4293005   -1.2627622  -1.8263477  0.015581204\n\n(2,2,.,.) =\n2.0050068   -0.32893315 0.19670151  0.8031714\n0.16645809  -0.68172836 0.5169275   -0.83938134\n0.1789333   2.1845143   1.3843338   -0.8283524\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SpatialDropout2D\n\nmodel = Sequential()\nmodel.add(SpatialDropout2D(input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[0.21864846 0.43531162 0.23078088 0.81122115]\n   [0.19442596 0.11110444 0.533805   0.68291312]\n   [0.40738259 0.05448269 0.04647733 0.41683944]]\n  [[0.23354645 0.46005503 0.87695602 0.13318982]\n   [0.2596346  0.67654484 0.79389709 0.50408343]\n   [0.50043622 0.28028835 0.81897585 0.01629935]]]\n\n [[[0.32173241 0.38367311 0.10315543 0.22691558]\n   [0.41640003 0.45932496 0.70795718 0.67185326]\n   [0.11911477 0.90231481 0.49881045 0.74297438]]\n  [[0.48873758 0.53475116 0.06801025 0.50640297]\n   [0.95740488 0.14928652 0.10466387 0.29040436]\n   [0.44062539 0.36983024 0.35326756 0.60592402]]]]\n\n\n\n\nOutput is\n\n\n[[[[0.21864846 0.43531162 0.23078088 0.8112211 ]\n   [0.19442596 0.11110444 0.533805   0.6829131 ]\n   [0.4073826  0.05448269 0.04647733 0.41683942]]\n  [[0.23354645 0.46005502 0.87695605 0.13318983]\n   [0.2596346  0.67654485 0.7938971  0.50408345]\n   [0.50043625 0.28028834 0.81897587 0.01629935]]]\n\n [[[0.0        0.0        0.0        0.0       ]\n   [0.0        0.0        0.0        0.0       ]\n   [0.0        0.0        0.0        0.0       ]]\n  [[0.48873758 0.5347512  0.06801025 0.50640297]\n   [0.95740485 0.14928652 0.10466387 0.29040435]\n   [0.4406254  0.36983025 0.35326755 0.605924  ]]]]\n\n\n\n\n\n\nSpatialDropout3D\n\n\nSpatial 3D version of Dropout.\n\n\nThis version performs the same function as Dropout, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.\n\n\nThe input of this layer should be 5D.\n\n\nScala:\n\n\nSpatialDropout3D(p = 0.5, dimOrdering = \"th\", inputShape = null)\n\n\n\n\nPython:\n\n\nSpatialDropout3D(p=0.5, dim_ordering=\"th\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\np\n: Fraction of the input units to drop. Between 0 and 1.\n\n\ndimOrdering\n: Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, SpatialDropout3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SpatialDropout3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.28834015    -0.74598366  0.16951436\n0.17009573    0.3626017    -0.24652131\n\n(1,1,2,.,.) =\n1.3008109     0.37243804   0.073205866\n1.0715603     0.02033514   -1.7862324\n\n(1,2,1,.,.) =\n-0.5285066    -1.3859391   -1.0543352\n0.7904896     0.7473174    -0.5941196\n\n(1,2,2,.,.) =\n-0.060706574  -2.4405587    1.5963978\n-0.33285397   -0.48576602   0.8121179\n\n(2,1,1,.,.) =\n-0.7060156    0.31667668    -0.28765643\n-1.3115436    -1.7266335    1.0080509\n\n(2,1,2,.,.) =\n1.2365453     -0.13272893   -1.2130978\n0.26921487    -0.66259027   0.5537464\n\n(2,2,1,.,.) =\n1.6578121     -0.09890133   0.4794677\n1.5102282     0.067802615   0.76998603\n\n(2,2,2,.,.) =\n-0.47348467   0.19535838    0.62601316\n-2.4771519    -0.40744382   0.04029308\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.0             -0.0        0.0\n0.0             0.0         -0.0\n\n(1,1,2,.,.) =\n0.0             0.0         0.0\n0.0             0.0         -0.0\n\n(1,2,1,.,.) =\n-0.5285066      -1.3859391  -1.0543352\n0.7904896       0.7473174   -0.5941196\n\n(1,2,2,.,.) =\n-0.060706574    -2.4405587  1.5963978\n-0.33285397     -0.48576602 0.8121179\n\n(2,1,1,.,.) =\n-0.0            0.0         -0.0\n-0.0            -0.0        0.0\n\n(2,1,2,.,.) =\n0.0             -0.0        -0.0\n0.0             -0.0        0.0\n\n(2,2,1,.,.) =\n0.0             -0.0        0.0\n0.0             0.0         0.0\n\n(2,2,2,.,.) =\n-0.0            0.0         0.0\n-0.0            -0.0        0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SpatialDropout3D\n\nmodel = Sequential()\nmodel.add(SpatialDropout3D(input_shape=(2, 2, 2, 2)))\ninput = np.random.random([2, 2, 2, 2, 2])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[[[0.68128454 0.57379206]\n    [0.19533742 0.19906853]]\n   [[0.21527836 0.79586573]\n    [0.51065215 0.94422278]]]\n  [[[0.95178211 0.50359204]\n    [0.0306965  0.92563536]]\n   [[0.33744311 0.58750719]\n    [0.45437398 0.7081438 ]]]]\n\n [[[[0.00235233 0.8092749 ]\n    [0.65525661 0.01079958]]\n   [[0.29877429 0.42090468]\n    [0.28265598 0.81520172]]]\n  [[[0.91811333 0.3275563 ]\n    [0.66125455 0.15555596]]\n   [[0.53651033 0.66013486]\n    [0.45874838 0.7613676 ]]]]]\n\n\n\n\nOutput is\n\n\n[[[[[0.68128455 0.57379204]\n    [0.19533743 0.19906853]]\n   [[0.21527836 0.7958657 ]\n    [0.5106521  0.94422275]]]\n  [[[0.0        0.0       ]\n    [0.0        0.0       ]]\n   [[0.0        0.0       ]\n    [0.0        0.0       ]]]]\n\n [[[[0.0        0.0       ]\n    [0.0        0.0       ]]\n   [[0.0        0.0       ]\n    [0.0        0.0       ]]]\n  [[[0.91811335 0.3275563 ]\n    [0.6612545  0.15555596]]\n   [[0.53651035 0.66013485]\n    [0.45874837 0.7613676 ]]]]]",
            "title": "Dropout Layers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/#dropout",
            "text": "Applies Dropout to the input by randomly setting a fraction 'p' of input units to 0 at each update during training time in order to prevent overfitting.  Scala:  Dropout(p, inputShape = null)  Python:  Dropout(p, input_shape=None, name=None)  Parameters:   p : Fraction of the input units to drop. Between 0 and 1.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Dropout}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Dropout(0.3, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-1.1256621     2.5402398    -1.1346831   0.50337905\n-1.3835752     0.9513693    -0.24547328  -0.28897092\n-0.0302343     -0.4106753   0.46467322   -0.7328933\n\n(2,.,.) =\n1.2569109      0.16947697   -0.5000246   2.0856402\n-0.04246076    1.5827807    -1.0235463   1.7278075\n-0.0035352164  -1.2579697   0.206815     -0.053890422\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.0           0.0        -1.620976  0.0\n-1.976536     1.359099   0.0        -0.4128156\n-0.043191858  -0.586679  0.6638189  -1.0469904\n\n(2,.,.) =\n0.0            0.0         -0.7143209  2.979486\n-0.060658228   2.2611153   0.0         0.0\n-0.0050503095  -1.7970997  0.0         0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Dropout\n\nmodel = Sequential()\nmodel.add(Dropout(0.3, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.61976372 0.36074095 0.59003926 0.75373888]\n  [0.2390103  0.93491731 0.89078166 0.93083315]\n  [0.62360382 0.73646417 0.32886041 0.25372008]]\n\n [[0.10235195 0.7782206  0.54940485 0.41757437]\n  [0.94804637 0.04642807 0.17194449 0.2675274 ]\n  [0.89322413 0.3301816  0.49910094 0.00819342]]]  Output is  [[[0.88537675 0.51534426 0.0        0.0       ]\n  [0.0        1.3355962  1.2725452  1.3297616 ]\n  [0.89086264 1.0520917  0.4698006  0.0       ]]\n\n [[0.14621708 1.1117437  0.7848641  0.59653485]\n  [1.354352   0.06632582 0.24563499 0.382182  ]\n  [1.2760345  0.471688   0.7130013  0.01170488]]]",
            "title": "Dropout"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/#gaussiandropout",
            "text": "Apply multiplicative 1-centered Gaussian noise.  As it is a regularization layer, it is only active at training time.  Scala:  GaussianDropout(p, inputShape = null)  Python:  GaussianDropout(p, input_shape=None, name=None)  Parameters:   p : Drop probability (as with 'Dropout'). The multiplicative noise will have standard deviation 'sqrt(p/(1-p))'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GaussianDropout}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GaussianDropout(0.45, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.14522108 0.27993536  -0.38809696 0.26372102\n-0.5572615  0.091684595 0.27881327  -1.6235427\n-0.32884964 -0.46456075 1.6169231   0.31943536\n\n(2,.,.) =\n-1.813811   1.1577623   -0.8995344  -1.0607182\n-0.3952898  -2.3437335  -0.6608733  1.1752778\n-1.3373735  -1.7404749  0.82832927  0.3053458\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.27553135  0.15290284   -0.23144199  0.619676\n-0.6648747   0.053253293  -0.08241931  -0.47651786\n-0.46381548  -1.0048811   1.5911313    0.39929882\n\n(2,.,.) =\n-0.43828326  0.4397059    -0.7071283   -1.440457\n-0.27415445  -1.6525689   -0.14050363  0.8728552\n-2.0516112   -2.1537325   1.4714862    0.29218474\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GaussianDropout\n\nmodel = Sequential()\nmodel.add(GaussianDropout(0.45, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.87208899 0.1353189  0.325058   0.63174633]\n  [0.20479221 0.29774652 0.42038452 0.23819006]\n  [0.07608872 0.91696766 0.245824   0.84324374]]\n\n [[0.26268714 0.76275494 0.63620997 0.15049668]\n  [0.54144135 0.70412821 0.05555471 0.72317157]\n  [0.32796076 0.26804862 0.80775221 0.46948471]]]  Output is  [[[ 2.1392     -0.16185573 -0.18517245 -0.36539674]\n  [ 0.15324984  0.17320508  0.82520926  0.21734479]\n  [ 0.17601383  0.24906069  0.15664667  0.12675671]]\n\n [[ 0.49689308  1.8231225   1.0023257   0.37604305]\n  [ 1.2827866  -0.08726044  0.01333602  0.8518126 ]\n  [ 0.20021693  0.31828243  1.0940336   0.00866747]]]",
            "title": "GaussianDropout"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/#gaussiannoise",
            "text": "Apply additive zero-centered Gaussian noise.  This is useful to mitigate overfitting (you could see it as a form of random data augmentation).  Gaussian Noise is a natural choice as corruption process for real valued inputs.  As it is a regularization layer, it is only active at training time.  Scala:  GaussianNoise(sigma, inputShape = null)  Python:  GaussianNoise(sigma, input_shape=None, name=None)  Parameters:   sigma : Standard deviation of the noise distribution.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, GaussianNoise}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(GaussianNoise(0.6, inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.4226985   -0.010519333  -0.49748304   -0.3176052\n0.52444375  0.31100306    1.0308859     2.0337727\n0.21513703  -0.396619     -0.055275716  -0.40603992\n\n(2,.,.) =\n-1.2393064  -0.536477     -0.35633054   -0.09068655\n-1.7297741  -0.5812992    -1.2833812    -0.7185058\n0.13474904  0.06468039    -0.6630115    1.2471422\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.72299504   0.7733576    -0.13965577   0.72079915\n0.20137814    0.6300731    2.5559645     2.3056328\n-0.19732013   -0.482926    -0.22114205   -0.88772345\n\n(2,.,.) =\n-1.4293398    -1.0870209   -0.5509953    -0.31268832\n-2.244024     -0.23773572  -3.022697     -0.65151817\n-0.035656676  -0.7470889   -0.8566216    1.1347939\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import GaussianNoise\n\nmodel = Sequential()\nmodel.add(GaussianNoise(0.6, input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.61699657 0.9759922  0.62898391 0.57265605]\n  [0.88815108 0.9484446  0.0300381  0.54114527]\n  [0.94046216 0.05998474 0.24860526 0.82020617]]\n\n [[0.87308242 0.24780141 0.73385444 0.40836049]\n  [0.33166358 0.74540915 0.28333526 0.08263288]\n  [0.17527315 0.79798327 0.49351559 0.13895365]]]  Output is  [[[ 1.5833025   1.1431103   0.14338043  1.634818  ]\n  [ 0.01713479  1.1608562   0.222246    0.40559798]\n  [ 0.9930201   0.1187391   -0.35643864 -0.7164774 ]]\n\n [[ 1.0105296   1.423961    0.90040827  1.3460591 ]\n  [ 0.943779    -0.48430538 0.20670155  -0.50143087]\n  [ -0.29849088 0.12774569  -0.16126743 -0.011041  ]]]",
            "title": "GaussianNoise"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/#spatialdropout1d",
            "text": "Spatial 1D version of Dropout.  This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.  The input of this layer should be 3D.  Scala:  SpatialDropout1D(p = 0.5, inputShape = null)  Python:  SpatialDropout1D(p=0.5, input_shape=None, name=None)  Parameters:   p : Fraction of the input units to drop. Between 0 and 1.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, SpatialDropout1D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SpatialDropout1D(inputShape = Shape(3, 4)))\nval input = Tensor[Float](2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.41961443 -0.3900255  -0.11937201 1.2904007\n-1.7623849  -1.6778483  -0.30053464 0.33295104\n-0.29824665 -0.25474855 -2.1878588  1.2741995\n\n(2,.,.) =\n0.24517925  2.0451863   -0.4281332  -1.2022524\n-0.7767442  0.24794191  -0.5614063  0.14720131\n-1.4832486  0.59478635  -0.13351384 -0.8799204\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n-0.41961443 -0.0   -0.11937201  1.2904007\n-1.7623849  -0.0   -0.30053464  0.33295104\n-0.29824665 -0.0   -2.1878588   1.2741995\n\n(2,.,.) =\n0.24517925  0.0    -0.4281332   -0.0\n-0.7767442  0.0    -0.5614063   0.0\n-1.4832486  0.0    -0.13351384  -0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SpatialDropout1D\n\nmodel = Sequential()\nmodel.add(SpatialDropout1D(input_shape=(3, 4)))\ninput = np.random.random([2, 3, 4])\noutput = model.forward(input)  Input is:  [[[0.67162434 0.91104925 0.66869854 0.17295748]\n  [0.78326617 0.27447329 0.18051406 0.24230118]\n  [0.7098933  0.32496974 0.00517668 0.21293476]]\n\n [[0.26932307 0.33496273 0.71258256 0.15464896]\n  [0.75286915 0.210486   0.91826256 0.81379954]\n  [0.11960744 0.37420041 0.03886506 0.22882457]]]  Output is  [[[0.0        0.0        0.0        0.0       ]\n  [0.0        0.0        0.0        0.0       ]\n  [0.0        0.0        0.0        0.0       ]]\n\n [[0.0        0.33496273 0.0        0.15464896]\n  [0.0        0.210486   0.0        0.81379956]\n  [0.0        0.3742004  0.0        0.22882457]]]",
            "title": "SpatialDropout1D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/#spatialdropout2d",
            "text": "Spatial 2D version of Dropout.  This version performs the same function as Dropout, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.  The input of this layer should be 4D.  Scala:  SpatialDropout2D(p = 0.5, dimOrdering = \"th\", inputShape = null)  Python:  SpatialDropout2D(p=0.5, dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   p : Fraction of the input units to drop. Between 0 and 1.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, SpatialDropout2D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SpatialDropout2D(inputShape = Shape(2, 3, 4)))\nval input = Tensor[Float](2, 2, 3, 4).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,.,.) =\n1.1651757   1.0867785   -0.56122786 -0.542156\n-0.79321486 0.64733976  -0.7040698  -0.8619171\n-0.61122066 -1.9640825  -1.0078672  -0.12195914\n\n(1,2,.,.) =\n-0.24738677 -0.9351172  -0.11694977 0.8657273\n-0.4773825  -1.6853696  -1.4906564  -0.06981948\n-0.8184341  -1.3537912  1.2442955   -0.0071462104\n\n(2,1,.,.) =\n1.8801081   0.44946647  0.47776535  0.036228795\n-1.2122079  0.41413695  -0.691067   2.6273472\n1.4293005   -1.2627622  -1.8263477  0.015581204\n\n(2,2,.,.) =\n2.0050068   -0.32893315 0.19670151  0.8031714\n0.16645809  -0.68172836 0.5169275   -0.83938134\n0.1789333   2.1845143   1.3843338   -0.8283524\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,.,.) =\n0.0         0.0         -0.0        -0.0\n-0.0        0.0         -0.0        -0.0\n-0.0        -0.0        -0.0        -0.0\n\n(1,2,.,.) =\n-0.0        -0.0        -0.0        0.0\n-0.0        -0.0        -0.0        -0.0\n-0.0        -0.0        0.0         -0.0\n\n(2,1,.,.) =\n1.8801081   0.44946647  0.47776535  0.036228795\n-1.2122079  0.41413695  -0.691067   2.6273472\n1.4293005   -1.2627622  -1.8263477  0.015581204\n\n(2,2,.,.) =\n2.0050068   -0.32893315 0.19670151  0.8031714\n0.16645809  -0.68172836 0.5169275   -0.83938134\n0.1789333   2.1845143   1.3843338   -0.8283524\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3x4]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SpatialDropout2D\n\nmodel = Sequential()\nmodel.add(SpatialDropout2D(input_shape=(2, 3, 4)))\ninput = np.random.random([2, 2, 3, 4])\noutput = model.forward(input)  Input is:  [[[[0.21864846 0.43531162 0.23078088 0.81122115]\n   [0.19442596 0.11110444 0.533805   0.68291312]\n   [0.40738259 0.05448269 0.04647733 0.41683944]]\n  [[0.23354645 0.46005503 0.87695602 0.13318982]\n   [0.2596346  0.67654484 0.79389709 0.50408343]\n   [0.50043622 0.28028835 0.81897585 0.01629935]]]\n\n [[[0.32173241 0.38367311 0.10315543 0.22691558]\n   [0.41640003 0.45932496 0.70795718 0.67185326]\n   [0.11911477 0.90231481 0.49881045 0.74297438]]\n  [[0.48873758 0.53475116 0.06801025 0.50640297]\n   [0.95740488 0.14928652 0.10466387 0.29040436]\n   [0.44062539 0.36983024 0.35326756 0.60592402]]]]  Output is  [[[[0.21864846 0.43531162 0.23078088 0.8112211 ]\n   [0.19442596 0.11110444 0.533805   0.6829131 ]\n   [0.4073826  0.05448269 0.04647733 0.41683942]]\n  [[0.23354645 0.46005502 0.87695605 0.13318983]\n   [0.2596346  0.67654485 0.7938971  0.50408345]\n   [0.50043625 0.28028834 0.81897587 0.01629935]]]\n\n [[[0.0        0.0        0.0        0.0       ]\n   [0.0        0.0        0.0        0.0       ]\n   [0.0        0.0        0.0        0.0       ]]\n  [[0.48873758 0.5347512  0.06801025 0.50640297]\n   [0.95740485 0.14928652 0.10466387 0.29040435]\n   [0.4406254  0.36983025 0.35326755 0.605924  ]]]]",
            "title": "SpatialDropout2D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/dropout/#spatialdropout3d",
            "text": "Spatial 3D version of Dropout.  This version performs the same function as Dropout, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.  The input of this layer should be 5D.  Scala:  SpatialDropout3D(p = 0.5, dimOrdering = \"th\", inputShape = null)  Python:  SpatialDropout3D(p=0.5, dim_ordering=\"th\", input_shape=None, name=None)  Parameters:   p : Fraction of the input units to drop. Between 0 and 1.  dimOrdering : Format of input data. Either 'th' (Channel First) or 'tf' (Channel Last). Default is 'th'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, SpatialDropout3D}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SpatialDropout3D(inputShape = Shape(2, 2, 2, 3)))\nval input = Tensor[Float](2, 2, 2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,1,1,.,.) =\n0.28834015    -0.74598366  0.16951436\n0.17009573    0.3626017    -0.24652131\n\n(1,1,2,.,.) =\n1.3008109     0.37243804   0.073205866\n1.0715603     0.02033514   -1.7862324\n\n(1,2,1,.,.) =\n-0.5285066    -1.3859391   -1.0543352\n0.7904896     0.7473174    -0.5941196\n\n(1,2,2,.,.) =\n-0.060706574  -2.4405587    1.5963978\n-0.33285397   -0.48576602   0.8121179\n\n(2,1,1,.,.) =\n-0.7060156    0.31667668    -0.28765643\n-1.3115436    -1.7266335    1.0080509\n\n(2,1,2,.,.) =\n1.2365453     -0.13272893   -1.2130978\n0.26921487    -0.66259027   0.5537464\n\n(2,2,1,.,.) =\n1.6578121     -0.09890133   0.4794677\n1.5102282     0.067802615   0.76998603\n\n(2,2,2,.,.) =\n-0.47348467   0.19535838    0.62601316\n-2.4771519    -0.40744382   0.04029308\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,1,1,.,.) =\n0.0             -0.0        0.0\n0.0             0.0         -0.0\n\n(1,1,2,.,.) =\n0.0             0.0         0.0\n0.0             0.0         -0.0\n\n(1,2,1,.,.) =\n-0.5285066      -1.3859391  -1.0543352\n0.7904896       0.7473174   -0.5941196\n\n(1,2,2,.,.) =\n-0.060706574    -2.4405587  1.5963978\n-0.33285397     -0.48576602 0.8121179\n\n(2,1,1,.,.) =\n-0.0            0.0         -0.0\n-0.0            -0.0        0.0\n\n(2,1,2,.,.) =\n0.0             -0.0        -0.0\n0.0             -0.0        0.0\n\n(2,2,1,.,.) =\n0.0             -0.0        0.0\n0.0             0.0         0.0\n\n(2,2,2,.,.) =\n-0.0            0.0         0.0\n-0.0            -0.0        0.0\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x2x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SpatialDropout3D\n\nmodel = Sequential()\nmodel.add(SpatialDropout3D(input_shape=(2, 2, 2, 2)))\ninput = np.random.random([2, 2, 2, 2, 2])\noutput = model.forward(input)  Input is:  [[[[[0.68128454 0.57379206]\n    [0.19533742 0.19906853]]\n   [[0.21527836 0.79586573]\n    [0.51065215 0.94422278]]]\n  [[[0.95178211 0.50359204]\n    [0.0306965  0.92563536]]\n   [[0.33744311 0.58750719]\n    [0.45437398 0.7081438 ]]]]\n\n [[[[0.00235233 0.8092749 ]\n    [0.65525661 0.01079958]]\n   [[0.29877429 0.42090468]\n    [0.28265598 0.81520172]]]\n  [[[0.91811333 0.3275563 ]\n    [0.66125455 0.15555596]]\n   [[0.53651033 0.66013486]\n    [0.45874838 0.7613676 ]]]]]  Output is  [[[[[0.68128455 0.57379204]\n    [0.19533743 0.19906853]]\n   [[0.21527836 0.7958657 ]\n    [0.5106521  0.94422275]]]\n  [[[0.0        0.0       ]\n    [0.0        0.0       ]]\n   [[0.0        0.0       ]\n    [0.0        0.0       ]]]]\n\n [[[[0.0        0.0       ]\n    [0.0        0.0       ]]\n   [[0.0        0.0       ]\n    [0.0        0.0       ]]]\n  [[[0.91811335 0.3275563 ]\n    [0.6612545  0.15555596]]\n   [[0.53651035 0.66013485]\n    [0.45874837 0.7613676 ]]]]]",
            "title": "SpatialDropout3D"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/advanced-activation/",
            "text": "ELU\n\n\nExponential Linear Unit.\n\n\nIt follows: f(x) =  alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.\n\n\nScala:\n\n\nELU(alpha = 1.0, inputShape = null)\n\n\n\n\nPython:\n\n\nELU(alpha=1.0, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nalpha\n: Scale for the negative factor. Default is 1.0.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, ELU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ELU(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.13405465 0.05160992  -1.4711418\n1.5808829   -1.3145303  0.6709266\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.1254577  0.05160992  -0.77033687\n1.5808829   -0.73139954 0.6709266\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ELU\n\nmodel = Sequential()\nmodel.add(ELU(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.90404922 0.23530925 0.49711093]\n [0.43009161 0.22446032 0.90144771]]\n\n\n\n\nOutput is\n\n\n[[0.9040492  0.23530924 0.49711093]\n [0.43009162 0.22446032 0.9014477 ]]\n\n\n\n\n\n\nLeakyReLU\n\n\nLeaky version of a Rectified Linear Unit.\n\n\nIt allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0, f(x) = x for x >= 0.\n\n\nScala:\n\n\nLeakyReLU(alpha = 0.3, inputShape = null)\n\n\n\n\nPython:\n\n\nLeakyReLU(alpha=0.3, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nalpha\n: Negative slope coefficient. Default is 0.3.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, LeakyReLU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LeakyReLU(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.8846715    -0.5720033  -0.8220917\n-0.51755846  1.099684    2.6011446\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.8846715       -0.005720033   -0.008220917\n-0.0051755845   1.099684       2.6011446\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LeakyReLU\n\nmodel = Sequential()\nmodel.add(LeakyReLU(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.14422043 0.38066946 0.55092494]\n [0.60075682 0.53505094 0.78330962]]\n\n\n\n\nOutput is\n\n\n[[0.14422044 0.38066944 0.55092496]\n [0.6007568  0.5350509  0.78330964]]\n\n\n\n\n\n\nSReLU\n\n\nS-shaped Rectified Linear Unit.\n\n\nIt follows: f(x) = t^r + a^r(x - t^r) for x >= t^r, f(x) = x for t^r > x > t^l, f(x) = t^l + a^l(x - t^l) for x <= t^l.\n\n\nScala:\n\n\nSReLU(tLeftInit = \"zero\", aLeftInit = \"glorot_uniform\", tRightInit = \"glorot_uniform\", aRightInit = \"one\", sharedAxes = null, inputShape = null)\n\n\n\n\nPython:\n\n\nSReLU(t_left_init=\"zero\", a_left_init=\"glorot_uniform\", t_right_init=\"glorot_uniform\", a_right_init=\"one\", shared_axes=None, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ntLeftInit\n: String representation of the initialization method for the left part intercept. See \nhere\n for available initialization strings. Default is 'zero'.\n\n\naLeftInit\n: String representation of the initialization method for the left part slope. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\ntRightInit\n: String representation of ithe nitialization method for the right part intercept. See \nhere\n for available initialization strings. Default is 'glorot_uniform'.\n\n\naRightInit\n: String representation of the initialization method for the right part slope. See \nhere\n for available initialization strings. Default is 'one'.\n\n\nsharedAxes\n: The axes along which to share learnable parameters for the activation function. Default is null.\nFor example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set 'sharedAxes = Array(1,2)'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, SReLU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SReLU(inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.5599429   0.22811626  -0.027771426\n-0.56582874 1.9261217   1.2686813\n\n(2,.,.) =\n0.7538568   0.8725621   0.19803657\n0.49057     0.0537252   0.8684544\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.5599429   0.22811626  -0.009864618\n0.07011698  1.9261217   1.2686813\n\n(2,.,.) =\n0.7538568   0.87256205  0.19803657\n0.49057     0.0537252   0.8684544\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SReLU\n\nmodel = Sequential()\nmodel.add(SReLU(input_shape=(2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.42998132 0.47736492 0.9554154 ]\n  [0.93264942 0.56851545 0.39508313]]\n\n [[0.5164102  0.22304862 0.44380779]\n  [0.69137804 0.26413953 0.60638032]]]\n\n\n\n\nOutput is\n\n\n[[[0.42998132 0.47736493 0.9554154 ]\n  [0.93264943 0.5685154  0.39508313]]\n\n [[0.5164102  0.22304863 0.44380778]\n  [0.69137806 0.26413953 0.60638034]]]\n\n\n\n\n\n\nThresholdedReLU\n\n\nThresholded Rectified Linear Unit.\n\n\nIt follows: f(x) = x for x > theta, f(x) = 0 otherwise.\n\n\nScala:\n\n\nThresholdedReLU(theta = 1.0, inputShape = null)\n\n\n\n\nPython:\n\n\nThresholdedReLU(theta=1.0, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\ntheta\n: Threshold location of activation. Default is 1.0.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, ThresholdedReLU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ThresholdedReLU(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.220999    1.2022058   -1.0015608\n0.6532913   0.31831574  1.6747104\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n2.220999    1.2022058   0.0\n0.0         0.0         1.6747104\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ThresholdedReLU\n\nmodel = Sequential()\nmodel.add(ThresholdedReLU(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[0.91854565 0.58317415 0.33089385]\n [0.82472184 0.70572913 0.32803604]]\n\n\n\n\nOutput is\n\n\n[[0.0   0.0   0.0]\n [0.0   0.0   0.0]]",
            "title": "Advanced Activations"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/advanced-activation/#elu",
            "text": "Exponential Linear Unit.  It follows: f(x) =  alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.  Scala:  ELU(alpha = 1.0, inputShape = null)  Python:  ELU(alpha=1.0, input_shape=None, name=None)  Parameters:   alpha : Scale for the negative factor. Default is 1.0.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, ELU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ELU(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n-0.13405465 0.05160992  -1.4711418\n1.5808829   -1.3145303  0.6709266\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n-0.1254577  0.05160992  -0.77033687\n1.5808829   -0.73139954 0.6709266\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ELU\n\nmodel = Sequential()\nmodel.add(ELU(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.90404922 0.23530925 0.49711093]\n [0.43009161 0.22446032 0.90144771]]  Output is  [[0.9040492  0.23530924 0.49711093]\n [0.43009162 0.22446032 0.9014477 ]]",
            "title": "ELU"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/advanced-activation/#leakyrelu",
            "text": "Leaky version of a Rectified Linear Unit.  It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0, f(x) = x for x >= 0.  Scala:  LeakyReLU(alpha = 0.3, inputShape = null)  Python:  LeakyReLU(alpha=0.3, input_shape=None, name=None)  Parameters:   alpha : Negative slope coefficient. Default is 0.3.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, LeakyReLU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(LeakyReLU(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n0.8846715    -0.5720033  -0.8220917\n-0.51755846  1.099684    2.6011446\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n0.8846715       -0.005720033   -0.008220917\n-0.0051755845   1.099684       2.6011446\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import LeakyReLU\n\nmodel = Sequential()\nmodel.add(LeakyReLU(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.14422043 0.38066946 0.55092494]\n [0.60075682 0.53505094 0.78330962]]  Output is  [[0.14422044 0.38066944 0.55092496]\n [0.6007568  0.5350509  0.78330964]]",
            "title": "LeakyReLU"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/advanced-activation/#srelu",
            "text": "S-shaped Rectified Linear Unit.  It follows: f(x) = t^r + a^r(x - t^r) for x >= t^r, f(x) = x for t^r > x > t^l, f(x) = t^l + a^l(x - t^l) for x <= t^l.  Scala:  SReLU(tLeftInit = \"zero\", aLeftInit = \"glorot_uniform\", tRightInit = \"glorot_uniform\", aRightInit = \"one\", sharedAxes = null, inputShape = null)  Python:  SReLU(t_left_init=\"zero\", a_left_init=\"glorot_uniform\", t_right_init=\"glorot_uniform\", a_right_init=\"one\", shared_axes=None, input_shape=None, name=None)  Parameters:   tLeftInit : String representation of the initialization method for the left part intercept. See  here  for available initialization strings. Default is 'zero'.  aLeftInit : String representation of the initialization method for the left part slope. See  here  for available initialization strings. Default is 'glorot_uniform'.  tRightInit : String representation of ithe nitialization method for the right part intercept. See  here  for available initialization strings. Default is 'glorot_uniform'.  aRightInit : String representation of the initialization method for the right part slope. See  here  for available initialization strings. Default is 'one'.  sharedAxes : The axes along which to share learnable parameters for the activation function. Default is null.\nFor example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set 'sharedAxes = Array(1,2)'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, SReLU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(SReLU(inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.5599429   0.22811626  -0.027771426\n-0.56582874 1.9261217   1.2686813\n\n(2,.,.) =\n0.7538568   0.8725621   0.19803657\n0.49057     0.0537252   0.8684544\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.5599429   0.22811626  -0.009864618\n0.07011698  1.9261217   1.2686813\n\n(2,.,.) =\n0.7538568   0.87256205  0.19803657\n0.49057     0.0537252   0.8684544\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import SReLU\n\nmodel = Sequential()\nmodel.add(SReLU(input_shape=(2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)  Input is:  [[[0.42998132 0.47736492 0.9554154 ]\n  [0.93264942 0.56851545 0.39508313]]\n\n [[0.5164102  0.22304862 0.44380779]\n  [0.69137804 0.26413953 0.60638032]]]  Output is  [[[0.42998132 0.47736493 0.9554154 ]\n  [0.93264943 0.5685154  0.39508313]]\n\n [[0.5164102  0.22304863 0.44380778]\n  [0.69137806 0.26413953 0.60638034]]]",
            "title": "SReLU"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/advanced-activation/#thresholdedrelu",
            "text": "Thresholded Rectified Linear Unit.  It follows: f(x) = x for x > theta, f(x) = 0 otherwise.  Scala:  ThresholdedReLU(theta = 1.0, inputShape = null)  Python:  ThresholdedReLU(theta=1.0, input_shape=None, name=None)  Parameters:   theta : Threshold location of activation. Default is 1.0.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, ThresholdedReLU}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(ThresholdedReLU(inputShape = Shape(3)))\nval input = Tensor[Float](2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n2.220999    1.2022058   -1.0015608\n0.6532913   0.31831574  1.6747104\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n2.220999    1.2022058   0.0\n0.0         0.0         1.6747104\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import ThresholdedReLU\n\nmodel = Sequential()\nmodel.add(ThresholdedReLU(input_shape=(3, )))\ninput = np.random.random([2, 3])\noutput = model.forward(input)  Input is:  [[0.91854565 0.58317415 0.33089385]\n [0.82472184 0.70572913 0.32803604]]  Output is  [[0.0   0.0   0.0]\n [0.0   0.0   0.0]]",
            "title": "ThresholdedReLU"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/wrappers/",
            "text": "TimeDistributed\n\n\nTimeDistributed wrapper. Apply a layer to every temporal slice of an input.\n\n\nThe input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.\n\n\nScala:\n\n\nTimeDistributed(layer, inputShape = null)\n\n\n\n\nPython:\n\n\nTimeDistributed(layer, input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nlayer\n: A layer instance.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, TimeDistributed, Dense}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(TimeDistributed(Dense(8, activation = \"relu\"), inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.15650798 -0.60011286 -0.0883946\n-0.8020574  -2.0070791  0.58417106\n\n(2,.,.) =\n1.1210757   0.061217457 0.37585327\n0.11572507  0.045938224 -1.1890792\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.35345355  0.019948795 0.0         0.22901565   0.0  0.035260748  0.0          0.40403664\n1.4793522   0.803728    0.0         0.93547887   0.0  0.097175285  0.0          1.2386305\n\n(2,.,.) =\n0.06176605  0.0         0.051847294 0.76588714   0.0  0.67298067   0.10942559   0.0\n0.0         0.0         0.0         0.0          0.0  0.0          0.4285032    0.3072814\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import TimeDistributed, Dense\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(8, activation = \"relu\"), input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.37107995 0.16777911 0.07691505]\n  [0.42678424 0.53602176 0.01580607]]\n\n [[0.31664302 0.03947526 0.1556008 ]\n  [0.2834384  0.68845104 0.23020768]]]\n\n\n\n\nOutput is:\n\n\n[[[0.09678233 0.21351711 0.0   0.07420383 0.09885262 0.0 0.13514107 0.0 ]\n  [0.06882857 0.18277436 0.0   0.1371126  0.00853634 0.0 0.1224944  0.0 ]]\n\n [[0.11387025 0.20642482 0.0   0.04896355 0.11478973 0.0 0.12610494 0.0 ]\n  [0.08322716 0.08292685 0.0   0.14674747 0.0        0.0 0.05299555 0.0 ]]]\n\n\n\n\n\n\nBidirectional\n\n\nBidirectional wrapper for RNNs.\n\n\nBidirectional currently requires RNNs to return the full sequence, i.e. returnSequences = true.\n\n\nScala:\n\n\nBidirectional(layer, mergeMode = \"concat\", inputShape = null)\n\n\n\n\nPython:\n\n\nBidirectional(layer, merge_mode=\"concat\", input_shape=None, name=None)\n\n\n\n\nParameters:\n\n\n\n\nlayer\n: An instance of a recurrent layer.\n\n\nmergeMode\n: Mode by which outputs of the forward and backward RNNs will be combined. Must be one of: 'sum', 'mul', 'concat', 'ave'. Default is 'concat'.\n\n\ninputShape\n: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a \nShape\n object. For Python API, it should be a shape tuple. Batch dimension should be excluded.\n\n\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn.keras.{Sequential, Bidirectional, SimpleRNN}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Bidirectional(SimpleRNN(4, returnSequences = true), inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)\n\n\n\n\nInput is:\n\n\ninput: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.6564635   1.525706    -0.54619956\n0.67109746  -0.45657027 -0.5378798\n\n(2,.,.) =\n0.19413045  -0.08337678 -0.0016114949\n0.6112209   0.7706432   1.3831\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]\n\n\n\n\nOutput is:\n\n\noutput: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.7013748   0.4841168     0.10397806 0.3799655   0.6934304  0.27561978  0.44025457  0.44310626\n0.4784317   -0.040266205  0.6599038  -0.29032442 0.55478245 0.061714854 0.5239438   -0.2890968\n\n(2,.,.) =\n0.32227796  0.23023699  0.34051302  -0.18683606 0.38275728  0.49924713  0.3152017   -0.14768216\n0.1766845   0.39446256  -0.12303881 0.08089487  0.08701726  0.46380803  -0.3540904  -0.0030886582\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x8]\n\n\n\n\nPython example:\n\n\nimport numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Bidirectional, LSTM\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(4, return_sequences = True), merge_mode = \"sum\", input_shape = (3, 3)))\ninput = np.random.random([2, 3, 3])\noutput = model.forward(input)\n\n\n\n\nInput is:\n\n\n[[[0.95180543 0.87111702 0.08901385]\n  [0.77432517 0.27843224 0.83308397]\n  [0.9140173  0.28253884 0.01381966]]\n\n [[0.12674146 0.74173106 0.86059416]\n  [0.40666387 0.85293504 0.9403338 ]\n  [0.42748364 0.14310765 0.98098256]]]\n\n\n\n\nOutput is:\n\n\n[[[ 0.11651072  0.07040063  0.53200144 -0.37872505]\n  [ 0.03238479  0.15081021  0.55530167 -0.3390156 ]\n  [ 0.18388109  0.02891854  0.5591757  -0.28601688]]\n\n [[-0.17779878 -0.02685877  0.244566   -0.34734237]\n  [-0.17816684  0.077871    0.3195565  -0.40989208]\n  [-0.13442594  0.08941883  0.3418655  -0.29824993]]]",
            "title": "Layer Wrappers"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/wrappers/#timedistributed",
            "text": "TimeDistributed wrapper. Apply a layer to every temporal slice of an input.  The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.  Scala:  TimeDistributed(layer, inputShape = null)  Python:  TimeDistributed(layer, input_shape=None, name=None)  Parameters:   layer : A layer instance.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, TimeDistributed, Dense}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(TimeDistributed(Dense(8, activation = \"relu\"), inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n-0.15650798 -0.60011286 -0.0883946\n-0.8020574  -2.0070791  0.58417106\n\n(2,.,.) =\n1.1210757   0.061217457 0.37585327\n0.11572507  0.045938224 -1.1890792\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.35345355  0.019948795 0.0         0.22901565   0.0  0.035260748  0.0          0.40403664\n1.4793522   0.803728    0.0         0.93547887   0.0  0.097175285  0.0          1.2386305\n\n(2,.,.) =\n0.06176605  0.0         0.051847294 0.76588714   0.0  0.67298067   0.10942559   0.0\n0.0         0.0         0.0         0.0          0.0  0.0          0.4285032    0.3072814\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import TimeDistributed, Dense\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(8, activation = \"relu\"), input_shape = (2, 3)))\ninput = np.random.random([2, 2, 3])\noutput = model.forward(input)  Input is:  [[[0.37107995 0.16777911 0.07691505]\n  [0.42678424 0.53602176 0.01580607]]\n\n [[0.31664302 0.03947526 0.1556008 ]\n  [0.2834384  0.68845104 0.23020768]]]  Output is:  [[[0.09678233 0.21351711 0.0   0.07420383 0.09885262 0.0 0.13514107 0.0 ]\n  [0.06882857 0.18277436 0.0   0.1371126  0.00853634 0.0 0.1224944  0.0 ]]\n\n [[0.11387025 0.20642482 0.0   0.04896355 0.11478973 0.0 0.12610494 0.0 ]\n  [0.08322716 0.08292685 0.0   0.14674747 0.0        0.0 0.05299555 0.0 ]]]",
            "title": "TimeDistributed"
        },
        {
            "location": "/KerasStyleAPIGuide/Layers/wrappers/#bidirectional",
            "text": "Bidirectional wrapper for RNNs.  Bidirectional currently requires RNNs to return the full sequence, i.e. returnSequences = true.  Scala:  Bidirectional(layer, mergeMode = \"concat\", inputShape = null)  Python:  Bidirectional(layer, merge_mode=\"concat\", input_shape=None, name=None)  Parameters:   layer : An instance of a recurrent layer.  mergeMode : Mode by which outputs of the forward and backward RNNs will be combined. Must be one of: 'sum', 'mul', 'concat', 'ave'. Default is 'concat'.  inputShape : Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a  Shape  object. For Python API, it should be a shape tuple. Batch dimension should be excluded.   Scala example:  import com.intel.analytics.bigdl.nn.keras.{Sequential, Bidirectional, SimpleRNN}\nimport com.intel.analytics.bigdl.utils.Shape\nimport com.intel.analytics.bigdl.tensor.Tensor\n\nval model = Sequential[Float]()\nmodel.add(Bidirectional(SimpleRNN(4, returnSequences = true), inputShape = Shape(2, 3)))\nval input = Tensor[Float](2, 2, 3).randn()\nval output = model.forward(input)  Input is:  input: com.intel.analytics.bigdl.tensor.Tensor[Float] =\n(1,.,.) =\n0.6564635   1.525706    -0.54619956\n0.67109746  -0.45657027 -0.5378798\n\n(2,.,.) =\n0.19413045  -0.08337678 -0.0016114949\n0.6112209   0.7706432   1.3831\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]  Output is:  output: com.intel.analytics.bigdl.nn.abstractnn.Activity =\n(1,.,.) =\n0.7013748   0.4841168     0.10397806 0.3799655   0.6934304  0.27561978  0.44025457  0.44310626\n0.4784317   -0.040266205  0.6599038  -0.29032442 0.55478245 0.061714854 0.5239438   -0.2890968\n\n(2,.,.) =\n0.32227796  0.23023699  0.34051302  -0.18683606 0.38275728  0.49924713  0.3152017   -0.14768216\n0.1766845   0.39446256  -0.12303881 0.08089487  0.08701726  0.46380803  -0.3540904  -0.0030886582\n\n[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x8]  Python example:  import numpy as np\nfrom bigdl.nn.keras.topology import Sequential\nfrom bigdl.nn.keras.layer import Bidirectional, LSTM\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(4, return_sequences = True), merge_mode = \"sum\", input_shape = (3, 3)))\ninput = np.random.random([2, 3, 3])\noutput = model.forward(input)  Input is:  [[[0.95180543 0.87111702 0.08901385]\n  [0.77432517 0.27843224 0.83308397]\n  [0.9140173  0.28253884 0.01381966]]\n\n [[0.12674146 0.74173106 0.86059416]\n  [0.40666387 0.85293504 0.9403338 ]\n  [0.42748364 0.14310765 0.98098256]]]  Output is:  [[[ 0.11651072  0.07040063  0.53200144 -0.37872505]\n  [ 0.03238479  0.15081021  0.55530167 -0.3390156 ]\n  [ 0.18388109  0.02891854  0.5591757  -0.28601688]]\n\n [[-0.17779878 -0.02685877  0.244566   -0.34734237]\n  [-0.17816684  0.077871    0.3195565  -0.40989208]\n  [-0.13442594  0.08941883  0.3418655  -0.29824993]]]",
            "title": "Bidirectional"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/training/",
            "text": "This page shows how to train, evaluate or predict a model using the Keras-Style API.\n\n\nYou may refer to the \nUser Guide\n page to see how to define a model in \nPython\n or \nScala\n correspondingly.\n\n\nYou may refer to \nLayers\n section to find all the available layers.\n\n\nAfter defining a model with the Keras-Style API, you can call the following \nmethods\n on the model:\n\n\n\n\nCompile\n\n\nConfigure the learning process. Must be called before \nfit\n or \nevaluate\n.\n\n\nScala:\n\n\ncompile(optimizer, loss, metrics = null)\n\n\n\n\nPython\n\n\ncompile(optimizer, loss, metrics=None)\n\n\n\n\nParameters:\n\n\n\n\noptimizer\n: Optimization method to be used. Can either use the string representation of an optimization method (see \nhere\n) or an instance of \nOptimMethod\n. \n\n\nloss\n: Criterion to be used. Can either use the string representation of a criterion (see \nhere\n) or an instance of \nLoss\n.\n\n\nmetrics\n: One or more validation methods to be used. Default is null if no validation needs to be configured. Can either use the string representation \nArray(\"accuracy\")\n(Scala) \n[\"accuracy\"]\n(Python) or instances of \nValidationMethod\n.\n\n\n\n\n\n\nFit\n\n\nTrain a model for a fixed number of epochs on a dataset. Need to first \ncompile\n the model beforehand.\n\n\nScala:\n\n\nfit(x, nbEpoch = 10, validationData = null)\n\n\n\n\nPython\n\n\nfit(x, y=None, batch_size=32, nb_epoch=10, validation_data=None, distributed=True)\n\n\n\n\nParameters:\n\n\n\n\nx\n: Training dataset.\n\n\nbatchSize\n: Number of samples per gradient update.\n\n\nnbEpoch\n: Number of iterations to train.\n\n\nvalidationData\n: Dataset for validation. Default is null if validation is not configured.\n\n\n\n\nRemark\n\n\n\n\nFor \nScala\n, x can either be RDD of \nSample\n (specifying \nbatchSize\n) or an instance of \nDataSet\n.\n\n\nFor \nPython\n, you can use x (a Numpy array) as features with y (a Numpy array) as labels; or only x (RDD of \nSample\n) without specifying y.\n\n\nThe parameter \ndistributed\n is to choose whether to train the model using distributed mode or local mode in \nPython\n. Default is true. If in local mode, x and y must both be Numpy arrays.\n\n\n\n\n\n\nEvaluate\n\n\nEvaluate a model on a given dataset using the metrics specified when you \ncompile\n the model.\n\n\nScala:\n\n\nevaluate(x)\n\n\n\n\nPython\n\n\nevaluate(x, y=None, batch_size=32)\n\n\n\n\nParameters:\n\n\n\n\nx\n: Evaluation dataset.\n\n\nbatchSize\n: Number of samples per batch.\n\n\n\n\nRemark\n\n\n\n\nFor \nScala\n, x can either be RDD of \nSample\n (specifying \nbatchSize\n) or an instance of \nDataSet\n.\n\n\nFor \nPython\n, you can use x (a Numpy array) as features with y (a Numpy array) as labels; or only x (RDD of \nSample\n) without specifying y. Currently only evaluation in distributed mode is supported in Python.\n\n\n\n\n\n\nPredict\n\n\nUse a model to do prediction.\n\n\nScala:\n\n\npredict(x)\n\n\n\n\nPython\n\n\npredict(x, distributed=True)\n\n\n\n\nParameters:\n\n\n\n\nx\n: Prediction data.\n\n\n\n\nRemark\n\n\n\n\nFor \nScala\n, x can either be RDD of \nSample\n (specifying \nbatchSize\n) or an instance of \nLocalDataSet\n.\n\n\nFor \nPython\n, x can either be a Numpy array representing labels or RDD of \nSample\n.\n\n\nThe parameter \ndistributed\n is to choose whether to do prediction using distributed mode or local mode in \nPython\n. Default is true. If in local mode, x must be a Numpy array.",
            "title": "Train, evaluate or predict a model"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/training/#compile",
            "text": "Configure the learning process. Must be called before  fit  or  evaluate .  Scala:  compile(optimizer, loss, metrics = null)  Python  compile(optimizer, loss, metrics=None)  Parameters:   optimizer : Optimization method to be used. Can either use the string representation of an optimization method (see  here ) or an instance of  OptimMethod .   loss : Criterion to be used. Can either use the string representation of a criterion (see  here ) or an instance of  Loss .  metrics : One or more validation methods to be used. Default is null if no validation needs to be configured. Can either use the string representation  Array(\"accuracy\") (Scala)  [\"accuracy\"] (Python) or instances of  ValidationMethod .",
            "title": "Compile"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/training/#fit",
            "text": "Train a model for a fixed number of epochs on a dataset. Need to first  compile  the model beforehand.  Scala:  fit(x, nbEpoch = 10, validationData = null)  Python  fit(x, y=None, batch_size=32, nb_epoch=10, validation_data=None, distributed=True)  Parameters:   x : Training dataset.  batchSize : Number of samples per gradient update.  nbEpoch : Number of iterations to train.  validationData : Dataset for validation. Default is null if validation is not configured.   Remark   For  Scala , x can either be RDD of  Sample  (specifying  batchSize ) or an instance of  DataSet .  For  Python , you can use x (a Numpy array) as features with y (a Numpy array) as labels; or only x (RDD of  Sample ) without specifying y.  The parameter  distributed  is to choose whether to train the model using distributed mode or local mode in  Python . Default is true. If in local mode, x and y must both be Numpy arrays.",
            "title": "Fit"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/training/#evaluate",
            "text": "Evaluate a model on a given dataset using the metrics specified when you  compile  the model.  Scala:  evaluate(x)  Python  evaluate(x, y=None, batch_size=32)  Parameters:   x : Evaluation dataset.  batchSize : Number of samples per batch.   Remark   For  Scala , x can either be RDD of  Sample  (specifying  batchSize ) or an instance of  DataSet .  For  Python , you can use x (a Numpy array) as features with y (a Numpy array) as labels; or only x (RDD of  Sample ) without specifying y. Currently only evaluation in distributed mode is supported in Python.",
            "title": "Evaluate"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/training/#predict",
            "text": "Use a model to do prediction.  Scala:  predict(x)  Python  predict(x, distributed=True)  Parameters:   x : Prediction data.   Remark   For  Scala , x can either be RDD of  Sample  (specifying  batchSize ) or an instance of  LocalDataSet .  For  Python , x can either be a Numpy array representing labels or RDD of  Sample .  The parameter  distributed  is to choose whether to do prediction using distributed mode or local mode in  Python . Default is true. If in local mode, x must be a Numpy array.",
            "title": "Predict"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/optimizer/",
            "text": "An optimizer, specified when you \ncompile\n the model, is used to update model gradient parameters in the process of training.\n\n\nSee \nhere\n for available optimizer objects.\n\n\nFor the sake of convenience, you can also use the corresponding string representation of an optimizer. In this case, the default parameters of an optimizer will be applied.\n\n\n\n\nAvailable Optimizers\n\n\n\n\nsgd\n\n\nadam\n\n\nadamax\n\n\nadadelta\n\n\nadagrad\n\n\nrmsprop",
            "title": "Optimizers"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/optimizer/#available-optimizers",
            "text": "sgd  adam  adamax  adadelta  adagrad  rmsprop",
            "title": "Available Optimizers"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/loss/",
            "text": "A loss function (or objective function), specified when you \ncompile\n the model, is the function that the model intends to optimize in the process of training.\n\n\nSee \nhere\n for available loss objects.\n\n\nFor the sake of convenience, you can also use the corresponding string representation of a loss.\n\n\n\n\nAvailable Losses\n\n\n\n\nmean_squared_error\n or \nmse\n\n\nmean_absolute_error\n or \nmae\n\n\ncategorical_crossentropy\n\n\nsparse_categorical_crossentropy\n\n\nbinary_crossentropy\n\n\nmean_absolute_percentage_error\n or \nmape\n\n\nmean_squared_logarithmic_error\n or \nmsle\n\n\nkullback_leibler_divergence\n or \nkld\n\n\nhinge\n\n\nsquared_hinge\n\n\npoisson\n\n\ncosine_proximity",
            "title": "Losses"
        },
        {
            "location": "/KerasStyleAPIGuide/Optimization/loss/#available-losses",
            "text": "mean_squared_error  or  mse  mean_absolute_error  or  mae  categorical_crossentropy  sparse_categorical_crossentropy  binary_crossentropy  mean_absolute_percentage_error  or  mape  mean_squared_logarithmic_error  or  msle  kullback_leibler_divergence  or  kld  hinge  squared_hinge  poisson  cosine_proximity",
            "title": "Available Losses"
        },
        {
            "location": "/Notebooks/scala/",
            "text": "TBD",
            "title": "Scala"
        },
        {
            "location": "/Notebooks/scala/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/Notebooks/python/",
            "text": "TBD",
            "title": "Python"
        },
        {
            "location": "/Notebooks/python/#tbd",
            "text": "",
            "title": "TBD"
        },
        {
            "location": "/powered-by/",
            "text": "Powered By",
            "title": "Powered by"
        },
        {
            "location": "/powered-by/#powered-by",
            "text": "",
            "title": "Powered By"
        },
        {
            "location": "/known-issues/",
            "text": "Issue 1: pls add \n\n\n\n\n\n\nIssue 2: ......",
            "title": "FAQ and Known Issues"
        }
    ]
}