{
    "docs": [
        {
            "location": "/", 
            "text": "Analytics Zoo\n\n\nAnalytics + AI Platform for Apache Spark and \nBigDL\n\n\n\n\nAnalytics Zoo makes it easy to build deep learning application on Spark and BigDL, by providing an end-to-end analytics + AI Platform (including \nhigh level pipeline APIs\n, \nbuilt-in deep learning models\n, \nreference use cases\n, etc.).\n\n\n\n\nHigh level pipeline APIs\n\n\nnnframes\n: native deep learning support in \nSpark DataFrames and ML Pipelines\n\n\nautograd\n: build custom layer/loss using \nauto differentiation operations\n \n\n\n\n\nTransfer learning\n: customize pretained model for \nfeature extraction or fine-tuning\n\n\n\n\n\n\nBuilt-in deep learning models\n\n\n\n\nObject detection API\n: high-level API and pretrained models (e.g., SSD and Faster-RCNN) for \nobject detection\n\n\nImage classification API\n: high-level API and pretrained models (e.g., VGG, Inception, ResNet, MobileNet, etc.) for \nimage classification\n\n\nText classification API\n: high-level API and pre-defined models (using CNN, LSTM, etc.) for \ntext classification\n\n\n\n\nRecommedation API\n: high-level API and pre-defined models (e.g., Neural Collaborative Filtering, Wide and Deep Learning, etc.) for \nrecommendation\n\n\n\n\n\n\nReference use cases\n: a collection of end-to-end \nreference use cases\n (e.g., anomaly detection, sentiment analysis, fraud detection, image augmentation, object detection, variational autoencoder, etc.)\n\n\n\n\n\n\nHigh level pipeline APIs\n\n\nAnalytics Zoo provides a set of easy-to-use, high level pipeline APIs that natively support Spark DataFrames and ML Pipelines, autograd and custom layer/loss, trasnfer learning, etc.\n\n\nnnframes\n\n\nnnframes\n provides \nnative deep learning support in Spark DataFrames and ML Pipelines\n, so that you can easily build complex deep learning pipelines in just a few lines, as illustracted below. (See more details \nhere\n)\n\n\n\n\n\n\nLoad images into DataFrames using \nNNImageReader\n\n   \nfrom zoo.common.nncontext import *\n   from zoo.pipeline.nnframes import *\n   sc = get_nncontext()\n   imageDF = NNImageReader.readImages(image_path, sc)\n\n\n\n\n\n\nProcess loaded data using \nDataFrames transformations\n\n   \ngetName = udf(lambda row: ...)\n   getLabel = udf(lambda name: ...)\n   df = imageDF.withColumn(\"name\", getName(col(\"image\"))).withColumn(\"label\", getLabel(col('name')))\n\n\n\n\n\n\nProcessing image using built-in \nfeature engineering operations\n\n   \nfrom zoo.feature.image import *\n   transformer = RowToImageFeature() -\n ImageResize(64, 64) -\n ImageChannelNormalize(123.0, 117.0, 104.0) \\\n                 -\n ImageMatToTensor() -\n ImageFeatureToTensor())\n\n\n\n\n\n\nDefine model using \nKeras-style APIs\n\n   \nfrom zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   model = Sequential().add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1, 28, 28))) \\\n                   .add(MaxPooling2D(pool_size=(2, 2))).add(Flatten()).add(Dense(10, activation='softmax')))\n\n\n\n\n\n\nTrain model using \nSpark ML Pipelines\n\n   \nclassifier = NNClassifier(model, CrossEntropyCriterion(),transformer).setLearningRate(0.003) \\\n                   .setBatchSize(40).setMaxEpoch(1).setFeaturesCol(\"image\").setCachingSample(False)\n   nnModel = classifier.fit(df)\n\n\n\n\n\n\nautograd\n\n\nautograd\n provides automatic differentiation for math operations, so that you can easily build your own \ncustom loss and layer\n (in both Python and Scala), as illustracted below. (See more details \nhere\n)\n\n\n\n\nDefine custom functions using \nautograd\n\n   ```\n   from zoo.pipeline.api.autograd import *\n\n\n\n\ndef mean_absolute_error(y_true, y_pred):\n       return mean(abs(y_true - y_pred), axis=1)\n\n\ndef add_one_func(x):\n       return x + 1.0\n   ```\n\n\n\n\n\n\nDefine model using Keras-style API and \ncustom \nLambda\n layer\n\n   \nfrom zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   model = Sequential().add(Dense(1, input_shape=(2,))) \\\n                       .add(Lambda(function=add_one_func))\n\n\n\n\n\n\nTrain model with \ncustom loss function\n\n   \nmodel.compile(optimizer = SGD(), loss = mean_absolute_error)\n   model.fit(x = ..., y = ...)\n\n\n\n\n\n\nTransfer learning\n\n\nUsing the high level transfer learning APIs, you can easily customize pretrained models for \nfeature extraction or fine-tuning\n. (See more details \nhere\n)\n\n\n\n\n\n\nLoad an existing model (pretrained in Caffe)\n   \nfrom zoo.pipeline.api.net import *\n   full_model = Net.load_caffe(def_path, model_path)\n\n\n\n\n\n\nRemove last few layers\n   \n# create a new model by remove layers after pool5/drop_7x7_s1\n   model = full_model.new_graph([\"pool5/drop_7x7_s1\"])\n\n\n\n\n\n\nFreeze first few layers\n   \n# freeze layers from input to pool4/3x3_s2 inclusive\n   model.freeze_up_to([\"pool4/3x3_s2\"])\n\n\n\n\n\n\nAdd a few new layers\n   \nfrom zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   input = Input(name=\"input\", shape=(3, 224, 224))\n   inception = model.to_keras()(input)\n   flatten = Flatten()(inception)\n   logits = Dense(2)(flatten)\n   newModel = Model(inputNode, logits)\n\n\n\n\n\n\nBuilt-in deep learning models\n\n\nAnalytics Zoo provides several built-in deep learning models that you can use for a variety of problem types, such as \nobject detection\n, \nimage classification\n, \ntext classification\n, \nrecommendation\n, etc.\n\n\nObject detection API\n\n\nUsing \nAnalytics Zoo Object Detection API\n (including a set of pretrained detection models such as SSD and Faster-RCNN), you can easily build your object detection applications (e.g., localizing and identifying multiple objects in images and videos), as illustrated below. (See more details \nhere\n)\n\n\n\n\nDownload object detection models in Analytics Zoo\n\n\n\n\nYou can download a collection of detection models (pretrained on the PSCAL VOC dataset and COCO dataset) from \ndetection model zoo\n.\n\n\n\n\nUse \nObject Detection API\n for off-the-shell inference\n   \nfrom zoo.models.image.objectdetection import *\n   model = ObjectDetector.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)\n\n\n\n\nImage classification API\n\n\nUsing \nAnalytics Zoo Image Classification API\n (including a set of pretrained detection models such as VGG, Inception, ResNet, MobileNet,  etc.), you can easily build your image classification applications, as illustrated below. (See more details \nhere\n)\n\n\n\n\nDownload image classification models in Analytics Zoo\n\n\n\n\nYou can download a collection of image classification models (pretrained on the ImageNet dataset) from \nimage classification model zoo\n.\n\n\n\n\nUse \nImage classification API\n for off-the-shell inference\n   \nfrom zoo.models.image.imageclassification import *\n   model = ImageClassifier.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)\n\n\n\n\nText classification API\n\n\nAnalytics Zoo Text Classification API\n provides a set of pre-defined models (using CNN, LSTM, etc.) for text classifications. (See more details \nhere\n)\n\n\nRecommendation API\n\n\nAnalytics Zoo Recommendation API\n provides a set of pre-defined models (such as Neural Collaborative Filtering, Wide and Deep Learning, etc.) for receommendations. (See more details \nhere\n)\n\n\nReference use cases\n\n\nAnalytics Zoo provides a collection of end-to-end reference use cases, including \nanomaly detection (for time series data)\n, \nsentiment analysis\n, \nfraud detection\n, \nimage augmentation\n, \nobject detection\n, \nvariational autoencoder\n, etc. (See more details \nhere\n)", 
            "title": "Overview"
        }, 
        {
            "location": "/#analytics-zoo", 
            "text": "", 
            "title": "Analytics Zoo"
        }, 
        {
            "location": "/#analytics-ai-platform-for-apache-spark-and-bigdl", 
            "text": "Analytics Zoo makes it easy to build deep learning application on Spark and BigDL, by providing an end-to-end analytics + AI Platform (including  high level pipeline APIs ,  built-in deep learning models ,  reference use cases , etc.).   High level pipeline APIs  nnframes : native deep learning support in  Spark DataFrames and ML Pipelines  autograd : build custom layer/loss using  auto differentiation operations     Transfer learning : customize pretained model for  feature extraction or fine-tuning    Built-in deep learning models   Object detection API : high-level API and pretrained models (e.g., SSD and Faster-RCNN) for  object detection  Image classification API : high-level API and pretrained models (e.g., VGG, Inception, ResNet, MobileNet, etc.) for  image classification  Text classification API : high-level API and pre-defined models (using CNN, LSTM, etc.) for  text classification   Recommedation API : high-level API and pre-defined models (e.g., Neural Collaborative Filtering, Wide and Deep Learning, etc.) for  recommendation    Reference use cases : a collection of end-to-end  reference use cases  (e.g., anomaly detection, sentiment analysis, fraud detection, image augmentation, object detection, variational autoencoder, etc.)", 
            "title": "Analytics + AI Platform for Apache Spark and BigDL"
        }, 
        {
            "location": "/#high-level-pipeline-apis", 
            "text": "Analytics Zoo provides a set of easy-to-use, high level pipeline APIs that natively support Spark DataFrames and ML Pipelines, autograd and custom layer/loss, trasnfer learning, etc.", 
            "title": "High level pipeline APIs"
        }, 
        {
            "location": "/#nnframes", 
            "text": "nnframes  provides  native deep learning support in Spark DataFrames and ML Pipelines , so that you can easily build complex deep learning pipelines in just a few lines, as illustracted below. (See more details  here )    Load images into DataFrames using  NNImageReader \n    from zoo.common.nncontext import *\n   from zoo.pipeline.nnframes import *\n   sc = get_nncontext()\n   imageDF = NNImageReader.readImages(image_path, sc)    Process loaded data using  DataFrames transformations \n    getName = udf(lambda row: ...)\n   getLabel = udf(lambda name: ...)\n   df = imageDF.withColumn(\"name\", getName(col(\"image\"))).withColumn(\"label\", getLabel(col('name')))    Processing image using built-in  feature engineering operations \n    from zoo.feature.image import *\n   transformer = RowToImageFeature() -  ImageResize(64, 64) -  ImageChannelNormalize(123.0, 117.0, 104.0) \\\n                 -  ImageMatToTensor() -  ImageFeatureToTensor())    Define model using  Keras-style APIs \n    from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   model = Sequential().add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1, 28, 28))) \\\n                   .add(MaxPooling2D(pool_size=(2, 2))).add(Flatten()).add(Dense(10, activation='softmax')))    Train model using  Spark ML Pipelines \n    classifier = NNClassifier(model, CrossEntropyCriterion(),transformer).setLearningRate(0.003) \\\n                   .setBatchSize(40).setMaxEpoch(1).setFeaturesCol(\"image\").setCachingSample(False)\n   nnModel = classifier.fit(df)", 
            "title": "nnframes"
        }, 
        {
            "location": "/#autograd", 
            "text": "autograd  provides automatic differentiation for math operations, so that you can easily build your own  custom loss and layer  (in both Python and Scala), as illustracted below. (See more details  here )   Define custom functions using  autograd \n   ```\n   from zoo.pipeline.api.autograd import *   def mean_absolute_error(y_true, y_pred):\n       return mean(abs(y_true - y_pred), axis=1)  def add_one_func(x):\n       return x + 1.0\n   ```    Define model using Keras-style API and  custom  Lambda  layer \n    from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   model = Sequential().add(Dense(1, input_shape=(2,))) \\\n                       .add(Lambda(function=add_one_func))    Train model with  custom loss function \n    model.compile(optimizer = SGD(), loss = mean_absolute_error)\n   model.fit(x = ..., y = ...)", 
            "title": "autograd"
        }, 
        {
            "location": "/#transfer-learning", 
            "text": "Using the high level transfer learning APIs, you can easily customize pretrained models for  feature extraction or fine-tuning . (See more details  here )    Load an existing model (pretrained in Caffe)\n    from zoo.pipeline.api.net import *\n   full_model = Net.load_caffe(def_path, model_path)    Remove last few layers\n    # create a new model by remove layers after pool5/drop_7x7_s1\n   model = full_model.new_graph([\"pool5/drop_7x7_s1\"])    Freeze first few layers\n    # freeze layers from input to pool4/3x3_s2 inclusive\n   model.freeze_up_to([\"pool4/3x3_s2\"])    Add a few new layers\n    from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   input = Input(name=\"input\", shape=(3, 224, 224))\n   inception = model.to_keras()(input)\n   flatten = Flatten()(inception)\n   logits = Dense(2)(flatten)\n   newModel = Model(inputNode, logits)", 
            "title": "Transfer learning"
        }, 
        {
            "location": "/#built-in-deep-learning-models", 
            "text": "Analytics Zoo provides several built-in deep learning models that you can use for a variety of problem types, such as  object detection ,  image classification ,  text classification ,  recommendation , etc.", 
            "title": "Built-in deep learning models"
        }, 
        {
            "location": "/#object-detection-api", 
            "text": "Using  Analytics Zoo Object Detection API  (including a set of pretrained detection models such as SSD and Faster-RCNN), you can easily build your object detection applications (e.g., localizing and identifying multiple objects in images and videos), as illustrated below. (See more details  here )   Download object detection models in Analytics Zoo   You can download a collection of detection models (pretrained on the PSCAL VOC dataset and COCO dataset) from  detection model zoo .   Use  Object Detection API  for off-the-shell inference\n    from zoo.models.image.objectdetection import *\n   model = ObjectDetector.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)", 
            "title": "Object detection API"
        }, 
        {
            "location": "/#image-classification-api", 
            "text": "Using  Analytics Zoo Image Classification API  (including a set of pretrained detection models such as VGG, Inception, ResNet, MobileNet,  etc.), you can easily build your image classification applications, as illustrated below. (See more details  here )   Download image classification models in Analytics Zoo   You can download a collection of image classification models (pretrained on the ImageNet dataset) from  image classification model zoo .   Use  Image classification API  for off-the-shell inference\n    from zoo.models.image.imageclassification import *\n   model = ImageClassifier.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)", 
            "title": "Image classification API"
        }, 
        {
            "location": "/#text-classification-api", 
            "text": "Analytics Zoo Text Classification API  provides a set of pre-defined models (using CNN, LSTM, etc.) for text classifications. (See more details  here )", 
            "title": "Text classification API"
        }, 
        {
            "location": "/#recommendation-api", 
            "text": "Analytics Zoo Recommendation API  provides a set of pre-defined models (such as Neural Collaborative Filtering, Wide and Deep Learning, etc.) for receommendations. (See more details  here )", 
            "title": "Recommendation API"
        }, 
        {
            "location": "/#reference-use-cases", 
            "text": "Analytics Zoo provides a collection of end-to-end reference use cases, including  anomaly detection (for time series data) ,  sentiment analysis ,  fraud detection ,  image augmentation ,  object detection ,  variational autoencoder , etc. (See more details  here )", 
            "title": "Reference use cases"
        }, 
        {
            "location": "/release-download/", 
            "text": "Release 0.1.0\n\n\n\n\n\n\n\n\n\n\nLinux x64\n\n\nMac\n\n\n\n\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.0.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\ndownload", 
            "title": "Download"
        }, 
        {
            "location": "/release-download/#release-010", 
            "text": "Linux x64  Mac      Spark 1.6.0  download  download    Spark 2.0.0  download  download    Spark 2.1.0  download  download", 
            "title": "Release 0.1.0"
        }, 
        {
            "location": "/release-docs/", 
            "text": "Release 0.1.1\n\n\nAnalytics-Zoo 0.1 Docs", 
            "title": "Documentation"
        }, 
        {
            "location": "/release-docs/#release-011", 
            "text": "Analytics-Zoo 0.1 Docs", 
            "title": "Release 0.1.1"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/", 
            "text": "Overview\n\n\nNNFrames is a package in Analytics Zoo aiming to provide DataFrame-based high level API to\nfacilitate Spark users and speed-up development. It supports native integration with Spark ML\nPipeline, which allows user to combine the power of Analytics Zoo, BigDL and Apache Spark MLlib.\nNNFrames provides both Python and Scala interfaces, and is compatible with both Spark 1.6 and\nSpark 2.x.\n\n\nHighlights\n\n1. Easy-to-use DataFrame(DataSet)-based API for training, prediction and evaluation with deep learning models.\n2. Effortless integration with Spark ML pipeline and compatibility with other feature transformers and algorithms in Spark ML.\n3. In a few lines, run large scale inference or transfer learning from pre-trained models of Caffe, Keras, Tensorflow or BigDL.\n4. Training of customized model or BigDL built-in neural models (e.g. Inception, ResNet, Wide And Deep).\n5. Rich toolset for feature extraction and processing, including image, audio and texts.\n\n\nExamples:\n\n\nThe examples are included in the Analytics Zoo source code.\n\n\n\n\nimage classification: model inference using pre-trained Inception v1 model.\n    \nScala version\n\n    \nPython version\n\n\nimage classification: transfer learning from pre-trained Inception v1 model.\n    \nScala version\n\n    \nPython version\n\n\n\n\nPrimary APIs\n\n\nNNEstimator and NNModel\n\n\nAnalytics Zoo provides \nNNEstimator\n for model training with Spark DataFrame, which\nprovides high level API for training a BigDL Model with the Apache Spark\n\nEstimator\n/\n\nTransfomer\n\npattern, thus users can conveniently fit Analytics Zoo into a ML pipeline. The fit result of\n\nNNEstimator\n is a NNModel, which is a Spark ML Transformer.\n\n\nplease check our\n\nNNEstimator API\n for detailed usage.\n\n\nNNClassifier and NNClassifierModel\n\n\nNNClassifier\n and \nNNClassifierModel\nextends \nNNEstimator\n and \nNNModel\n and focus on \nclassification tasks, where both label column and prediction column are of Double type.\n\n\nNNImageReader\n\nNNImageReader loads image into Spark DataFrame.\n\n\nplease check our\n\nImageProcessing\n for detailed usage.", 
            "title": "DataFrame and ML Pipeline"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/#overview", 
            "text": "NNFrames is a package in Analytics Zoo aiming to provide DataFrame-based high level API to\nfacilitate Spark users and speed-up development. It supports native integration with Spark ML\nPipeline, which allows user to combine the power of Analytics Zoo, BigDL and Apache Spark MLlib.\nNNFrames provides both Python and Scala interfaces, and is compatible with both Spark 1.6 and\nSpark 2.x.  Highlights \n1. Easy-to-use DataFrame(DataSet)-based API for training, prediction and evaluation with deep learning models.\n2. Effortless integration with Spark ML pipeline and compatibility with other feature transformers and algorithms in Spark ML.\n3. In a few lines, run large scale inference or transfer learning from pre-trained models of Caffe, Keras, Tensorflow or BigDL.\n4. Training of customized model or BigDL built-in neural models (e.g. Inception, ResNet, Wide And Deep).\n5. Rich toolset for feature extraction and processing, including image, audio and texts.", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/#examples", 
            "text": "The examples are included in the Analytics Zoo source code.   image classification: model inference using pre-trained Inception v1 model.\n     Scala version \n     Python version  image classification: transfer learning from pre-trained Inception v1 model.\n     Scala version \n     Python version", 
            "title": "Examples:"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/#primary-apis", 
            "text": "NNEstimator and NNModel  Analytics Zoo provides  NNEstimator  for model training with Spark DataFrame, which\nprovides high level API for training a BigDL Model with the Apache Spark Estimator / Transfomer \npattern, thus users can conveniently fit Analytics Zoo into a ML pipeline. The fit result of NNEstimator  is a NNModel, which is a Spark ML Transformer.  please check our NNEstimator API  for detailed usage.  NNClassifier and NNClassifierModel  NNClassifier  and  NNClassifierModel extends  NNEstimator  and  NNModel  and focus on \nclassification tasks, where both label column and prediction column are of Double type.  NNImageReader \nNNImageReader loads image into Spark DataFrame.  please check our ImageProcessing  for detailed usage.", 
            "title": "Primary APIs"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/", 
            "text": "", 
            "title": "Autograd"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/", 
            "text": "", 
            "title": "Transfer Learning"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/", 
            "text": "", 
            "title": "Working with Images"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/", 
            "text": "Analytics Zoo Object Detection API\n\n\nAnalytics Zoo provides a collection of pre-trained models for Object Detection. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios, users can embed the models locally, distributedly in Spark such as Apache Storm and Apache Flink.\n\n\nObject Detection models\n\n\nAnalytics Zoo provides two typical kind of pre-trained Object Detection models : \nSSD\n and \nFaster-RCNN\n on dataset \nPASCAL\n and \nCOCO\n. For the usage of these models, please check below examples.\n\n\nScala example\n\n\nIt's very easy to apply the model for inference with below code piece.\n\n\nval model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval output = model.predictImageSet(data)\n\n\n\n\nFor preprocessors for Object Detection models, please check \nObject Detection Config\n\n\nUsers can also do the inference directly using Analytics zoo.\nSample code for SSD VGG on PASCAL as below:\n\n\nval model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval preprocessor = Resize(300, 300) -\n\n                         ChannelNormalize(123f, 117f, 104f, 1f, 1f, 1f) -\n\n                         MatToTensor() -\n ImageFrameToSample()\nval output = model.predictImageset(data)\n\n\n\n\nDownload link\n\n\nObject Detection\n\n\n\n\nPASCAL VOC models\n\n\nSSD 300x300 MobileNet\n\n\nSSD 300x300 VGG\n\n\nSSD 512x512 VGG\n\n\nFaster-RCNN VGG\n\n\nFaster-RCNN VGG Compress\n\n\nFaster-RCNN PvaNet\n\n\n\n\nFaster-RCNN PvaNet Compress\n\n\n\n\n\n\nCOCO models\n\n\n\n\n\n\nSSD 300x300 VGG\n\n\n\n\nSSD 512x512 VGG", 
            "title": "Object Detection"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/#analytics-zoo-object-detection-api", 
            "text": "Analytics Zoo provides a collection of pre-trained models for Object Detection. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios, users can embed the models locally, distributedly in Spark such as Apache Storm and Apache Flink.  Object Detection models  Analytics Zoo provides two typical kind of pre-trained Object Detection models :  SSD  and  Faster-RCNN  on dataset  PASCAL  and  COCO . For the usage of these models, please check below examples.  Scala example  It's very easy to apply the model for inference with below code piece.  val model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval output = model.predictImageSet(data)  For preprocessors for Object Detection models, please check  Object Detection Config  Users can also do the inference directly using Analytics zoo.\nSample code for SSD VGG on PASCAL as below:  val model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval preprocessor = Resize(300, 300) - \n                         ChannelNormalize(123f, 117f, 104f, 1f, 1f, 1f) - \n                         MatToTensor() -  ImageFrameToSample()\nval output = model.predictImageset(data)", 
            "title": "Analytics Zoo Object Detection API"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/#download-link", 
            "text": "", 
            "title": "Download link"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/#object-detection", 
            "text": "PASCAL VOC models  SSD 300x300 MobileNet  SSD 300x300 VGG  SSD 512x512 VGG  Faster-RCNN VGG  Faster-RCNN VGG Compress  Faster-RCNN PvaNet   Faster-RCNN PvaNet Compress    COCO models    SSD 300x300 VGG   SSD 512x512 VGG", 
            "title": "Object Detection"
        }, 
        {
            "location": "/ProgrammingGuide/image-classification/", 
            "text": "", 
            "title": "Image Classification"
        }, 
        {
            "location": "/ProgrammingGuide/text-classification/", 
            "text": "", 
            "title": "Text Classification"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/", 
            "text": "Analytics Zoo Recommender\n\n\nAnalytics Zoo provides two Recommender models, including Wide and Deep(WND) learning model and Neural network-based Collaborative Filtering (NCF) model. \n\n\nHighlights\n\n1. Easy-to-use models, could be fed into NNFrames and BigDL Optimizer for training..\n2. Recommenders can handle either explict or implicit feedback, given corresponding features.\n3. It provides three user-friendly APIs to predict user item pairs, and recommend items (users) for users (items). \n\n\nThe examples/notebooks are included in the Analytics Zoo source code.\n\n\n\n\nWide and Deep Learning Model.\n    \nScala example\n\n    \nPython notebook\n\n\nNCF.\n    \nScala example\n\n    \nPython notebook\n\n\n\n\nWide and Deep Recommender model\n\n\nScala\n\n\nBuild a WND model for recommendation. \n\n\nval wideAndDeep = WideAndDeep(modelType = \nwide_n_deep\n, numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))\n\n\n\n\nTrain a WND model using BigDL Optimizer.\n\n\nval optimizer = Optimizer(\n      model = wideAndDeep,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()\n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nval userItemPairPrediction = wideAndDeep.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = wideAndDeep.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = wideAndDeep.recommendForItem(validationpairFeatureRdds, 3)\n\n\n\n\nSee more details in our\nRecommender API\n and \nScala example\n\n\nPython\n\n\nBuild a WND model for recommendation. \n\n\nwide_n_deep = WideAndDeep(class_num, column_info, model_type=\nwide_n_deep\n, hidden_layers=(40, 20, 10))\n\n\n\n\nTrain a WND model using BigDL Optimizer \n\n\noptimizer = Optimizer(\n    model=wide_n_deep,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize() \n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nuserItemPairPrediction = wide_n_deep.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = wide_n_deep.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = wide_n_deep.recommend_for_item(valPairFeatureRdds, 3)\n\n\n\n\nSee more details in our\nRecommender API\n and \nPython notebook\n\n\nNeural network-based Collaborative Filtering\n\n\nScala\n\n\nBuild a NCF model for recommendation. \n\n\nval ncf = NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)\n\n\n\n\nTrain a NCF model using BigDL Optimizer \n\n\nval optimizer = Optimizer(\n      model = ncf,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()\n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nval userItemPairPrediction = ncf.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = ncf.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = ncf.recommendForItem(validationpairFeatureRdds, 3)\n\n\n\n\nSee more details in our\nRecommender API\n and \nScala example\n\n\nPython\n\n\nBuild a NCF model for recommendation. \n\n\nncf=NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)\n\n\n\n\nTrain a NCF model using BigDL Optimizer \n\n\noptimizer = Optimizer(\n    model=ncf,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize() \n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nuserItemPairPrediction = ncf.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = ncf.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = ncf.recommend_for_item(valPairFeatureRdds, 3)\n\n\n\n\nSee more details in our\nRecommender API\n and \nPython notebook", 
            "title": "Recommendation"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/#analytics-zoo-recommender", 
            "text": "Analytics Zoo provides two Recommender models, including Wide and Deep(WND) learning model and Neural network-based Collaborative Filtering (NCF) model.   Highlights \n1. Easy-to-use models, could be fed into NNFrames and BigDL Optimizer for training..\n2. Recommenders can handle either explict or implicit feedback, given corresponding features.\n3. It provides three user-friendly APIs to predict user item pairs, and recommend items (users) for users (items).   The examples/notebooks are included in the Analytics Zoo source code.   Wide and Deep Learning Model.\n     Scala example \n     Python notebook  NCF.\n     Scala example \n     Python notebook", 
            "title": "Analytics Zoo Recommender"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/#wide-and-deep-recommender-model", 
            "text": "Scala  Build a WND model for recommendation.   val wideAndDeep = WideAndDeep(modelType =  wide_n_deep , numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))  Train a WND model using BigDL Optimizer.  val optimizer = Optimizer(\n      model = wideAndDeep,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()  Predict and recommend items(users) for users(items) with given features.  val userItemPairPrediction = wideAndDeep.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = wideAndDeep.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = wideAndDeep.recommendForItem(validationpairFeatureRdds, 3)  See more details in our Recommender API  and  Scala example  Python  Build a WND model for recommendation.   wide_n_deep = WideAndDeep(class_num, column_info, model_type= wide_n_deep , hidden_layers=(40, 20, 10))  Train a WND model using BigDL Optimizer   optimizer = Optimizer(\n    model=wide_n_deep,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize()   Predict and recommend items(users) for users(items) with given features.  userItemPairPrediction = wide_n_deep.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = wide_n_deep.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = wide_n_deep.recommend_for_item(valPairFeatureRdds, 3)  See more details in our Recommender API  and  Python notebook", 
            "title": "Wide and Deep Recommender model"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/#neural-network-based-collaborative-filtering", 
            "text": "Scala  Build a NCF model for recommendation.   val ncf = NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)  Train a NCF model using BigDL Optimizer   val optimizer = Optimizer(\n      model = ncf,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()  Predict and recommend items(users) for users(items) with given features.  val userItemPairPrediction = ncf.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = ncf.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = ncf.recommendForItem(validationpairFeatureRdds, 3)  See more details in our Recommender API  and  Scala example  Python  Build a NCF model for recommendation.   ncf=NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)  Train a NCF model using BigDL Optimizer   optimizer = Optimizer(\n    model=ncf,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize()   Predict and recommend items(users) for users(items) with given features.  userItemPairPrediction = ncf.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = ncf.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = ncf.recommend_for_item(valPairFeatureRdds, 3)  See more details in our Recommender API  and  Python notebook", 
            "title": "Neural network-based Collaborative Filtering"
        }, 
        {
            "location": "/ProgrammingGuide/usercases-overview/", 
            "text": "", 
            "title": "overview"
        }, 
        {
            "location": "/ProgrammingGuide/anomaly-detection/", 
            "text": "Analytics Zoo Anomaly detection\n\n\nAnalytics Zoo shows how to detect anomalies in time series data based on RNN network. Currently, a \npython notebook\n is provided. \nIn the example, a RNN network using Analytics Zoo Keras-Style API is built, and \nNYC taxi passengers dataset\n is used to train and test the model.\n\n\nBuild a RNN model for anomaly detection. There are three LSTM layers followed by one Dense layer at the end.\n\n\nmodel = Sequential()\nmodel.add(LSTM(input_shape=(input_dim1, input_dim2, output_dim=8, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(15,return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(output_dim=1))\n\n\n\n\nTrain the model using Analytics Zoo APIs.\n\n\nmodel.compile(loss='mse', optimizer='rmsprop')\nmodel.fit(x_train,y_train,batch_size=3028,nb_epoch=30)\n\n\n\n\nPredict with given features.\n\n\npredictions = model.predict(x_test)\n\n\n\n\nAnomalies could be defined by comparing the predictions and actual values. The current example defines data points as anomalies if the difference of predictions and actual values are larger than a certain value.\nSee more details in the exmaple \nPython notebook", 
            "title": "Anomaly Detection"
        }, 
        {
            "location": "/ProgrammingGuide/anomaly-detection/#analytics-zoo-anomaly-detection", 
            "text": "Analytics Zoo shows how to detect anomalies in time series data based on RNN network. Currently, a  python notebook  is provided. \nIn the example, a RNN network using Analytics Zoo Keras-Style API is built, and  NYC taxi passengers dataset  is used to train and test the model.  Build a RNN model for anomaly detection. There are three LSTM layers followed by one Dense layer at the end.  model = Sequential()\nmodel.add(LSTM(input_shape=(input_dim1, input_dim2, output_dim=8, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(15,return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(output_dim=1))  Train the model using Analytics Zoo APIs.  model.compile(loss='mse', optimizer='rmsprop')\nmodel.fit(x_train,y_train,batch_size=3028,nb_epoch=30)  Predict with given features.  predictions = model.predict(x_test)  Anomalies could be defined by comparing the predictions and actual values. The current example defines data points as anomalies if the difference of predictions and actual values are larger than a certain value.\nSee more details in the exmaple  Python notebook", 
            "title": "Analytics Zoo Anomaly detection"
        }, 
        {
            "location": "/PythonUserGuide/install/", 
            "text": "Install From Pip\n\n\n\n\nNOTES\n\n\n\n\nPip install supports \nMac\n and \nLinux\n platforms.\n\n\nPip install only supports \nlocal\n mode. Cluster mode might be supported in the future. For those who want to use Analytics Zoo in cluster mode, please try to \ninstall without pip\n.\n\n\nIf you use pip install, it is \nnot\n necessary to set \nSPARK_HOME\n.\n\n\nYou need to install Java \n= JDK8\n before running Analytics Zoo, which is required by \npyspark\n.\n\n\nWe've tested this package with \nPython 2.7\n, \nPython 3.5\n and \nPython 3.6\n. Only these three Python versions are supported for now.\n\n\n\n\n\n\nInstall analytics-zoo-0.1.0.dev0\n\n\npip install --upgrade pip\npip install analytics-zoo==0.1.0.dev0     # for Python 2.7\npip3 install analytics-zoo==0.1.0.dev0    # for Python 3.5 and Python 3.6\n\n\n\n\nRemark:\n\n\n\n\n\n\nWe tested this on pip 9.0.1.\n\n\n\n\n\n\nYou might need to add \nsudo\n if you don't have the permission for installation.\n\n\n\n\n\n\nbigdl==0.5.0\n and its dependencies (including \npyspark\n, \nnumpy\n and \nsix\n) will be automatically installed first before installing analytics-zoo if they haven't been detected in the current Python environment.", 
            "title": "Install"
        }, 
        {
            "location": "/PythonUserGuide/install/#install-from-pip", 
            "text": "", 
            "title": "Install From Pip"
        }, 
        {
            "location": "/PythonUserGuide/install/#notes", 
            "text": "Pip install supports  Mac  and  Linux  platforms.  Pip install only supports  local  mode. Cluster mode might be supported in the future. For those who want to use Analytics Zoo in cluster mode, please try to  install without pip .  If you use pip install, it is  not  necessary to set  SPARK_HOME .  You need to install Java  = JDK8  before running Analytics Zoo, which is required by  pyspark .  We've tested this package with  Python 2.7 ,  Python 3.5  and  Python 3.6 . Only these three Python versions are supported for now.", 
            "title": "NOTES"
        }, 
        {
            "location": "/PythonUserGuide/install/#install-analytics-zoo-010dev0", 
            "text": "pip install --upgrade pip\npip install analytics-zoo==0.1.0.dev0     # for Python 2.7\npip3 install analytics-zoo==0.1.0.dev0    # for Python 3.5 and Python 3.6  Remark:    We tested this on pip 9.0.1.    You might need to add  sudo  if you don't have the permission for installation.    bigdl==0.5.0  and its dependencies (including  pyspark ,  numpy  and  six ) will be automatically installed first before installing analytics-zoo if they haven't been detected in the current Python environment.", 
            "title": "Install analytics-zoo-0.1.0.dev0"
        }, 
        {
            "location": "/PythonUserGuide/run/", 
            "text": "Run after pip install\n\n\nPrecondition\n\n\n\n\nInstall analytics-zoo from pip\n\n\nOnly \nPython 2.7\n, \nPython 3.5\n and \nPython 3.6\n are supported for now.\n\n\n\n\n\n\nUse an Interactive Shell\n\n\n\n\nType \npython\n in the command line to start a REPL.\n\n\nTry to run the \nexample code\n to verify the installation.\n\n\n\n\n\n\nUse Jupyter Notebook\n\n\n\n\nJust start jupyter notebook as you normally do, e.g.\n\n\n\n\njupyter notebook --notebook-dir=./ --ip=* --no-browser\n\n\n\n\n\n\nTry to run the \nexample code\n to verify the installation.\n\n\n\n\n\n\nExample code\n\n\nTo verify if Analytics Zoo can run successfully, run the following simple code:\n\n\nimport zoo.version\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.keras.layers import *\n\n# Get the current Analytics Zoo version\nzoo.version.__version__\n# Create or get a SparkContext. This will also init the BigDL engine.\nsc = get_nncontext()\n# Create a Sequential model containing a Dense layer.\nmodel = Sequential()\nmodel.add(Dense(8, input_shape=(10, )))\n\n\n\n\nConfigurations\n\n\n\n\nIncrease memory\n\n\n\n\nexport SPARK_DRIVER_MEMORY=20g\n\n\n\n\n\n\nAdd extra jars or python packages\n\n\n\n\n Set the environment variables \nBIGDL_JARS\n and \nBIGDL_PACKAGES\n \nBEFORE\n creating \nSparkContext\n:\n\n\nexport BIGDL_JARS=...\nexport BIGDL_PACKAGES=...", 
            "title": "Run"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-after-pip-install", 
            "text": "", 
            "title": "Run after pip install"
        }, 
        {
            "location": "/PythonUserGuide/run/#precondition", 
            "text": "Install analytics-zoo from pip  Only  Python 2.7 ,  Python 3.5  and  Python 3.6  are supported for now.", 
            "title": "Precondition"
        }, 
        {
            "location": "/PythonUserGuide/run/#use-an-interactive-shell", 
            "text": "Type  python  in the command line to start a REPL.  Try to run the  example code  to verify the installation.", 
            "title": "Use an Interactive Shell"
        }, 
        {
            "location": "/PythonUserGuide/run/#use-jupyter-notebook", 
            "text": "Just start jupyter notebook as you normally do, e.g.   jupyter notebook --notebook-dir=./ --ip=* --no-browser   Try to run the  example code  to verify the installation.", 
            "title": "Use Jupyter Notebook"
        }, 
        {
            "location": "/PythonUserGuide/run/#example-code", 
            "text": "To verify if Analytics Zoo can run successfully, run the following simple code:  import zoo.version\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.keras.layers import *\n\n# Get the current Analytics Zoo version\nzoo.version.__version__\n# Create or get a SparkContext. This will also init the BigDL engine.\nsc = get_nncontext()\n# Create a Sequential model containing a Dense layer.\nmodel = Sequential()\nmodel.add(Dense(8, input_shape=(10, )))", 
            "title": "Example code"
        }, 
        {
            "location": "/PythonUserGuide/run/#configurations", 
            "text": "Increase memory   export SPARK_DRIVER_MEMORY=20g   Add extra jars or python packages    Set the environment variables  BIGDL_JARS  and  BIGDL_PACKAGES   BEFORE  creating  SparkContext :  export BIGDL_JARS=...\nexport BIGDL_PACKAGES=...", 
            "title": "Configurations"
        }, 
        {
            "location": "/PythonUserGuide/examples/", 
            "text": "TBD", 
            "title": "Examples"
        }, 
        {
            "location": "/PythonUserGuide/examples/#tbd", 
            "text": "", 
            "title": "TBD"
        }, 
        {
            "location": "/ScalaUserGuide/install/", 
            "text": "TBD", 
            "title": "Install"
        }, 
        {
            "location": "/ScalaUserGuide/install/#tbd", 
            "text": "", 
            "title": "TBD"
        }, 
        {
            "location": "/ScalaUserGuide/run/", 
            "text": "", 
            "title": "Run"
        }, 
        {
            "location": "/ScalaUserGuide/examples/", 
            "text": "TBD", 
            "title": "Examples"
        }, 
        {
            "location": "/ScalaUserGuide/examples/#tbd", 
            "text": "", 
            "title": "TBD"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/", 
            "text": "NNEstimator\n\n\nScala:\n\n\nval estimator = NNEstimator(model, criterion)\n\n\n\n\nPython:\n\n\nestimator = NNEstimator(model, criterion)\n\n\n\n\nNNEstimator\n extends \norg.apache.spark.ml.Estimator\n and supports training a BigDL\nmodel with Spark DataFrame data. It can be integrated into a standard Spark ML Pipeline\nto allow users to combine the components of BigDL and Spark MLlib.\n\n\nNNEstimator\n supports different feature and label data types through \nPreprocessing\n.\nDuring fit (training), NNEstimator will extract feature and label data from input DataFrame and use\nthe \nPreprocessing\n to convert data for the model, typically converts the feature and label\nto Tensors or converts the (feature, option[Label]) tuple to a BigDL \nSample\n. Each\n\nPreprocessing\n conducts a data conversion step in the preprocessing phase, multiple\n\nPreprocessing\n can be combined into a \nChainedPreprocessing\n. Some pre-defined \n\nPreprocessing\n for popular data types like Image, Array or Vector are provided in package\n\ncom.intel.analytics.zoo.feature\n, while user can also develop customized \nPreprocessing\n.\nBy default, \nSeqToTensor\n is used to convert an array or Vector to a 1-dimension Tensor.\nUsing the \nPreprocessing\n allows \nNNEstimator\n to cache only the raw data and decrease the \nmemory consumption during feature conversion and training, it also enables the model to digest\nextra data types that DataFrame does not support currently.\nMore concrete examples are available in package \ncom.intel.analytics.zoo.examples.nnframes\n\n\nNNEstimator\n can be created with various parameters for different scenarios.\n\n\n\n\nNNEstimator(model, criterion)\n\n\n\n\nTakes only model and criterion and use \nSeqToTensor\n as feature and label\n   \nPreprocessing\n. \nNNEstimator\n will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL \nSample\n and send to model for\n   training.\n2. \nNNEstimator(model, criterion, featureSize: Array[Int], labelSize: Array[Int])\n\n\nTakes model, criterion, featureSize(Array of Int) and labelSize(Array of Int). \nNNEstimator\n\n   will extract the data from feature and label columns (only Scalar, Array[_] or Vector data\n   type are supported) and convert each feature/label to Tensor according to the specified Tensor\n   size.\n3. \nNNEstimator(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]],\nlabelPreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model, criterion, featurePreprocessing and labelPreprocessing.  \nNNEstimator\n\n   will extract the data from feature and label columns and convert each feature/label to Tensor\n   with the featurePreprocessing and labelPreprocessing. This constructor provides more flexibility\n   in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNEstimator\n supports:\n\nsetSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])\n to directly compose\nSample according to user-specified Preprocessing.\n\n\nScala Example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNEstimator\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNEstimator(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0)),\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0))))\nval df = sqlContext.createDataFrame(data).toDF(\nfeatures\n, \nlabel\n)\nval nnModel = estimator.fit(df)\nnnModel.transform(df).show(false)\n\n\n\n\nPython Example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom zoo.pipeline.nnframes.nn_classifier import *\nfrom zoo.feature.common import *\n\ndata = self.sc.parallelize([\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0)),\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0))])\n\nschema = StructType([\n    StructField(\nfeatures\n, ArrayType(DoubleType(), False), False),\n    StructField(\nlabel\n, ArrayType(DoubleType(), False), False)])\ndf = self.sqlContext.createDataFrame(data, schema)\nmodel = Sequential().add(Linear(2, 2))\ncriterion = MSECriterion()\nestimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2]))\\\n    .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(40)\nnnModel = estimator.fit(df)\nres = nnModel.transform(df)\n\n\n\n\n\n\nNNModel\n\n\nScala:\n\n\nval nnModel = NNModel(bigDLModel)\n\n\n\n\nPython:\n\n\nnn_model = NNModel(bigDLModel)\n\n\n\n\nNNModel\n extends Spark's ML\n\nTransformer\n. User can invoke\n\nfit\n in \nNNEstimator\n to get a \nNNModel\n, or directly compose a \nNNModel\n from BigDLModel.\nIt enables users to wrap a pre-trained BigDL Model into a NNModel,\nand use it as a transformer in your Spark ML pipeline to predict the results for \nDataFrame\n(DataSet)\n. \n\n\nNNModel\n can be created with various parameters for different scenarios.\n\n\n\n\nNNModel(model)\n\n\n\n\nTakes only model and use \nSeqToTensor\n as feature Preprocessing. \nNNModel\n will extract the\n   data from feature column (only Scalar, Array[_] or Vector data type are supported) and\n   convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.\n2. \nNNModel(model, featureSize: Array[Int])\n\n\nTakes model and featureSize(Array of Int). \nNNModel\n will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.\n3. \nNNModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model and featurePreprocessing. \nNNModel\n will extract the data from feature column\n   and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNModel\n supports:\n\nsetSamplePreprocessing(value: Preprocessing[Any, Sample[T]])\nto directly compose\nSample according to user-specified Preprocessing.\n\n\n\n\nNNClassifier\n\n\nScala:\n\n\nval classifer =  NNClassifer(model, criterion)\n\n\n\n\nPython:\n\n\nclassifier = NNClassifer(model, criterion)\n\n\n\n\nNNClassifier\n is a specialized \nNNEstimator\n that simplifies the data format for\nclassification tasks where the label space is discrete. It only supports label column of\nDoubleType, and the fitted \nNNClassifierModel\n will have the prediction column of \nDoubleType.\n\n\n\n\nmodel\n BigDL module to be optimized in the fit() method\n\n\ncriterion\n the criterion used to compute the loss and the gradient\n\n\n\n\nNNClassifier\n can be created with various parameters for different scenarios.\n\n\n\n\nNNClassifier(model, criterion)\n\n\n\n\nTakes only model and criterion and use \nSeqToTensor\n as feature and label\n   Preprocessing. \nNNClassifier\n will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL samples and send to model for\n   training.\n2. \nNNClassifier(model, criterion, featureSize: Array[Int])\n\n\nTakes model, criterion, featureSize(Array of Int). \nNNClassifier\n\n   will extract the data from feature and label columns and convert each feature to Tensor\n   according to the specified Tensor size. \nScalarToTensor\n is used to convert the label column.\n3. \nNNClassifier(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model, criterion and featurePreprocessing.  \nNNClassifier\n\n   will extract the data from feature and label columns and convert each feature to Tensor\n   with the featurePreprocessing. This constructor provides more flexibility\n   in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNClassifier\n supports:\n\nsetSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])\n to directly compose\nSample with user-specified Preprocessing.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNClassifier\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNClassifier(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0),\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0)))\nval df = sqlContext.createDataFrame(data).toDF(\nfeatures\n, \nlabel\n)\nval dlModel = estimator.fit(df)\ndlModel.transform(df).show(false)\n\n\n\n\nPython Example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom bigdl.dlframes.dl_classifier import *\nfrom pyspark.sql.types import *\n\n#Logistic Regression with BigDL layers and Analytics zoo NNClassifier\nmodel = Sequential().add(Linear(2, 2)).add(LogSoftMax())\ncriterion = ClassNLLCriterion()\nestimator = NNClassifier(model, criterion, [2]).setBatchSize(4).setMaxEpoch(10)\ndata = sc.parallelize([\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0]),\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0])])\n\nschema = StructType([\n    StructField(\nfeatures\n, ArrayType(DoubleType(), False), False),\n    StructField(\nlabel\n, ArrayType(DoubleType(), False), False)])\ndf = sqlContext.createDataFrame(data, schema)\ndlModel = estimator.fit(df)\ndlModel.transform(df).show(False)\n\n\n\n\nNNClassifierModel\n\n\nScala:\n\n\nval nnClassifierModel = NNClassifierModel(model, featureSize)\n\n\n\n\nPython:\n\n\nnn_classifier_model = NNClassifierModel(model)\n\n\n\n\nNNClassifierModel is a specialized \nNNModel\n for classification tasks.\nBoth label and prediction column will have the datatype of Double.\n\n\nNNClassifierModel\n can be created with various parameters for different scenarios.\n\n\n\n\nNNClassifierModel(model)\n\n\n\n\nTakes only model and use \nSeqToTensor\n as feature Preprocessing. \nNNClassifierModel\n will\n   extract the data from feature column (only Scalar, Array[_] or Vector data type are supported)\n   and convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.\n2. \nNNClassifierModel(model, featureSize: Array[Int])\n\n\nTakes model and featureSize(Array of Int). \nNNClassifierModel\n will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.\n3. \nNNClassifierModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model and featurePreprocessing. \nNNClassifierModel\n will extract the data from feature\n   column and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNClassifierModel\n\nsupports: \nsetSamplePreprocessing(value: Preprocessing[Any, Sample[T]])\nto directly compose\nSample according to user-specified Preprocessing.\n\n\n\n\nHyperparameter setting\n\n\nPrior to the commencement of the training process, you can modify the optimization algorithm, batch \nsize, the epoch number of your training, and learning rate to meet your goal or\n\nNNEstimator\n/\nNNClassifier\n will use the default value.\n\n\nContinue the codes above, NNEstimator and NNClassifier can be set in the same way.\n\n\nScala:\n\n\n//for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())\n//for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())\n\n\n\n\nPython:\n\n\n# for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())\n# for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())\n\n\n\n\n\nPrepare the data and start the training process\n\n\nNNEstimator/NNCLassifer supports training with Spark's\n\nDataFrame/DataSet\n\n\nSuppose \ndf\n is the training data, simple call \nfit\n method and let Analytics Zoo train the model\nfor you.\n\n\nScala:\n\n\n//get a NNClassifierModel\nval nnClassifierModel = classifier.fit(df)\n\n\n\n\nPython:\n\n\n# get a NNClassifierModel\nnnClassifierModel = classifier.fit(df)\n\n\n\n\nUser may also set validation DataFrame and validation frequency through \nsetValidation\n method.\nTrain summay and validation summary can also be configured to log the training process for\nvisualization in Tensorboard.\n\n\nMake prediction on chosen data\n\n\nSince \nNNModel\n/\nNNClassifierModel\n inherits from Spark's \nTransformer\n abstract class, simply call \n\ntransform\n method on \nNNModel\n/\nNNClassifierModel\n to make prediction.\n\n\nScala:\n\n\nnnModel.transform(df).show(false)\n\n\n\n\nPython:\n\n\nnnModel.transform(df).show(false)\n\n\n\n\nFor the complete examples of NNFrames, please refer to:\n\nScala examples\n\n\nPython examples\n\n\nNNImageReader\n\n\nNNImageReader\n is the primary DataFrame-based image loading interface, defining API to read images\ninto DataFrame.\n\n\nScala:\n\n\n    val imageDF = NNImageReader.readImages(imageDirectory, sc)\n\n\n\n\nPython:\n\n\n    image_frame = NNImageReader.readImages(image_path, self.sc)\n\n\n\n\nThe output DataFrame contains a sinlge column named \"image\". The schema of \"image\" column can be\naccessed from \ncom.intel.analytics.zoo.pipeline.nnframes.DLImageSchema.byteSchema\n.\nEach record in \"image\" column represents one image record, in the format of\nRow(origin, height, width, num of channels, mode, data), where origin contains the URI for the image file,\nand \ndata\n holds the original file bytes for the image file. \nmode\n represents the OpenCV-compatible\ntype: CV_8UC3, CV_8UC1 in most cases.\n\n\n  val byteSchema = StructType(\n    StructField(\norigin\n, StringType, true) ::\n      StructField(\nheight\n, IntegerType, false) ::\n      StructField(\nwidth\n, IntegerType, false) ::\n      StructField(\nnChannels\n, IntegerType, false) ::\n      // OpenCV-compatible type: CV_8UC3, CV_32FC3 in most cases\n      StructField(\nmode\n, IntegerType, false) ::\n      // Bytes in OpenCV-compatible order: row-wise BGR in most cases\n      StructField(\ndata\n, BinaryType, false) :: Nil)\n\n\n\n\nAfter loading the image, user can compose the preprocess steps with the \nPreprocessing\n defined\nin \ncom.intel.analytics.zoo.feature.image\n.", 
            "title": "NNFrames"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnestimator", 
            "text": "Scala:  val estimator = NNEstimator(model, criterion)  Python:  estimator = NNEstimator(model, criterion)  NNEstimator  extends  org.apache.spark.ml.Estimator  and supports training a BigDL\nmodel with Spark DataFrame data. It can be integrated into a standard Spark ML Pipeline\nto allow users to combine the components of BigDL and Spark MLlib.  NNEstimator  supports different feature and label data types through  Preprocessing .\nDuring fit (training), NNEstimator will extract feature and label data from input DataFrame and use\nthe  Preprocessing  to convert data for the model, typically converts the feature and label\nto Tensors or converts the (feature, option[Label]) tuple to a BigDL  Sample . Each Preprocessing  conducts a data conversion step in the preprocessing phase, multiple Preprocessing  can be combined into a  ChainedPreprocessing . Some pre-defined  Preprocessing  for popular data types like Image, Array or Vector are provided in package com.intel.analytics.zoo.feature , while user can also develop customized  Preprocessing .\nBy default,  SeqToTensor  is used to convert an array or Vector to a 1-dimension Tensor.\nUsing the  Preprocessing  allows  NNEstimator  to cache only the raw data and decrease the \nmemory consumption during feature conversion and training, it also enables the model to digest\nextra data types that DataFrame does not support currently.\nMore concrete examples are available in package  com.intel.analytics.zoo.examples.nnframes  NNEstimator  can be created with various parameters for different scenarios.   NNEstimator(model, criterion)   Takes only model and criterion and use  SeqToTensor  as feature and label\n    Preprocessing .  NNEstimator  will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL  Sample  and send to model for\n   training.\n2.  NNEstimator(model, criterion, featureSize: Array[Int], labelSize: Array[Int])  Takes model, criterion, featureSize(Array of Int) and labelSize(Array of Int).  NNEstimator \n   will extract the data from feature and label columns (only Scalar, Array[_] or Vector data\n   type are supported) and convert each feature/label to Tensor according to the specified Tensor\n   size.\n3.  NNEstimator(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]],\nlabelPreprocessing: Preprocessing[F, Tensor[T]])  Takes model, criterion, featurePreprocessing and labelPreprocessing.   NNEstimator \n   will extract the data from feature and label columns and convert each feature/label to Tensor\n   with the featurePreprocessing and labelPreprocessing. This constructor provides more flexibility\n   in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNEstimator  supports: setSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])  to directly compose\nSample according to user-specified Preprocessing.  Scala Example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNEstimator\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNEstimator(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0)),\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0))))\nval df = sqlContext.createDataFrame(data).toDF( features ,  label )\nval nnModel = estimator.fit(df)\nnnModel.transform(df).show(false)  Python Example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom zoo.pipeline.nnframes.nn_classifier import *\nfrom zoo.feature.common import *\n\ndata = self.sc.parallelize([\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0)),\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0))])\n\nschema = StructType([\n    StructField( features , ArrayType(DoubleType(), False), False),\n    StructField( label , ArrayType(DoubleType(), False), False)])\ndf = self.sqlContext.createDataFrame(data, schema)\nmodel = Sequential().add(Linear(2, 2))\ncriterion = MSECriterion()\nestimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2]))\\\n    .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(40)\nnnModel = estimator.fit(df)\nres = nnModel.transform(df)", 
            "title": "NNEstimator"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnmodel", 
            "text": "Scala:  val nnModel = NNModel(bigDLModel)  Python:  nn_model = NNModel(bigDLModel)  NNModel  extends Spark's ML Transformer . User can invoke fit  in  NNEstimator  to get a  NNModel , or directly compose a  NNModel  from BigDLModel.\nIt enables users to wrap a pre-trained BigDL Model into a NNModel,\nand use it as a transformer in your Spark ML pipeline to predict the results for  DataFrame\n(DataSet) .   NNModel  can be created with various parameters for different scenarios.   NNModel(model)   Takes only model and use  SeqToTensor  as feature Preprocessing.  NNModel  will extract the\n   data from feature column (only Scalar, Array[_] or Vector data type are supported) and\n   convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.\n2.  NNModel(model, featureSize: Array[Int])  Takes model and featureSize(Array of Int).  NNModel  will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.\n3.  NNModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])  Takes model and featurePreprocessing.  NNModel  will extract the data from feature column\n   and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNModel  supports: setSamplePreprocessing(value: Preprocessing[Any, Sample[T]]) to directly compose\nSample according to user-specified Preprocessing.", 
            "title": "NNModel"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnclassifier", 
            "text": "Scala:  val classifer =  NNClassifer(model, criterion)  Python:  classifier = NNClassifer(model, criterion)  NNClassifier  is a specialized  NNEstimator  that simplifies the data format for\nclassification tasks where the label space is discrete. It only supports label column of\nDoubleType, and the fitted  NNClassifierModel  will have the prediction column of \nDoubleType.   model  BigDL module to be optimized in the fit() method  criterion  the criterion used to compute the loss and the gradient   NNClassifier  can be created with various parameters for different scenarios.   NNClassifier(model, criterion)   Takes only model and criterion and use  SeqToTensor  as feature and label\n   Preprocessing.  NNClassifier  will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL samples and send to model for\n   training.\n2.  NNClassifier(model, criterion, featureSize: Array[Int])  Takes model, criterion, featureSize(Array of Int).  NNClassifier \n   will extract the data from feature and label columns and convert each feature to Tensor\n   according to the specified Tensor size.  ScalarToTensor  is used to convert the label column.\n3.  NNClassifier(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]])  Takes model, criterion and featurePreprocessing.   NNClassifier \n   will extract the data from feature and label columns and convert each feature to Tensor\n   with the featurePreprocessing. This constructor provides more flexibility\n   in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNClassifier  supports: setSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])  to directly compose\nSample with user-specified Preprocessing.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNClassifier\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNClassifier(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0),\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0)))\nval df = sqlContext.createDataFrame(data).toDF( features ,  label )\nval dlModel = estimator.fit(df)\ndlModel.transform(df).show(false)  Python Example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom bigdl.dlframes.dl_classifier import *\nfrom pyspark.sql.types import *\n\n#Logistic Regression with BigDL layers and Analytics zoo NNClassifier\nmodel = Sequential().add(Linear(2, 2)).add(LogSoftMax())\ncriterion = ClassNLLCriterion()\nestimator = NNClassifier(model, criterion, [2]).setBatchSize(4).setMaxEpoch(10)\ndata = sc.parallelize([\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0]),\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0])])\n\nschema = StructType([\n    StructField( features , ArrayType(DoubleType(), False), False),\n    StructField( label , ArrayType(DoubleType(), False), False)])\ndf = sqlContext.createDataFrame(data, schema)\ndlModel = estimator.fit(df)\ndlModel.transform(df).show(False)", 
            "title": "NNClassifier"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnclassifiermodel", 
            "text": "Scala:  val nnClassifierModel = NNClassifierModel(model, featureSize)  Python:  nn_classifier_model = NNClassifierModel(model)  NNClassifierModel is a specialized  NNModel  for classification tasks.\nBoth label and prediction column will have the datatype of Double.  NNClassifierModel  can be created with various parameters for different scenarios.   NNClassifierModel(model)   Takes only model and use  SeqToTensor  as feature Preprocessing.  NNClassifierModel  will\n   extract the data from feature column (only Scalar, Array[_] or Vector data type are supported)\n   and convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.\n2.  NNClassifierModel(model, featureSize: Array[Int])  Takes model and featureSize(Array of Int).  NNClassifierModel  will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.\n3.  NNClassifierModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])  Takes model and featurePreprocessing.  NNClassifierModel  will extract the data from feature\n   column and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNClassifierModel \nsupports:  setSamplePreprocessing(value: Preprocessing[Any, Sample[T]]) to directly compose\nSample according to user-specified Preprocessing.", 
            "title": "NNClassifierModel"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#hyperparameter-setting", 
            "text": "Prior to the commencement of the training process, you can modify the optimization algorithm, batch \nsize, the epoch number of your training, and learning rate to meet your goal or NNEstimator / NNClassifier  will use the default value.  Continue the codes above, NNEstimator and NNClassifier can be set in the same way.  Scala:  //for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())\n//for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())  Python:  # for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())\n# for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())", 
            "title": "Hyperparameter setting"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#prepare-the-data-and-start-the-training-process", 
            "text": "NNEstimator/NNCLassifer supports training with Spark's DataFrame/DataSet  Suppose  df  is the training data, simple call  fit  method and let Analytics Zoo train the model\nfor you.  Scala:  //get a NNClassifierModel\nval nnClassifierModel = classifier.fit(df)  Python:  # get a NNClassifierModel\nnnClassifierModel = classifier.fit(df)  User may also set validation DataFrame and validation frequency through  setValidation  method.\nTrain summay and validation summary can also be configured to log the training process for\nvisualization in Tensorboard.", 
            "title": "Prepare the data and start the training process"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#make-prediction-on-chosen-data", 
            "text": "Since  NNModel / NNClassifierModel  inherits from Spark's  Transformer  abstract class, simply call  transform  method on  NNModel / NNClassifierModel  to make prediction.  Scala:  nnModel.transform(df).show(false)  Python:  nnModel.transform(df).show(false)  For the complete examples of NNFrames, please refer to: Scala examples  Python examples", 
            "title": "Make prediction on chosen data"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnimagereader", 
            "text": "NNImageReader  is the primary DataFrame-based image loading interface, defining API to read images\ninto DataFrame.  Scala:      val imageDF = NNImageReader.readImages(imageDirectory, sc)  Python:      image_frame = NNImageReader.readImages(image_path, self.sc)  The output DataFrame contains a sinlge column named \"image\". The schema of \"image\" column can be\naccessed from  com.intel.analytics.zoo.pipeline.nnframes.DLImageSchema.byteSchema .\nEach record in \"image\" column represents one image record, in the format of\nRow(origin, height, width, num of channels, mode, data), where origin contains the URI for the image file,\nand  data  holds the original file bytes for the image file.  mode  represents the OpenCV-compatible\ntype: CV_8UC3, CV_8UC1 in most cases.    val byteSchema = StructType(\n    StructField( origin , StringType, true) ::\n      StructField( height , IntegerType, false) ::\n      StructField( width , IntegerType, false) ::\n      StructField( nChannels , IntegerType, false) ::\n      // OpenCV-compatible type: CV_8UC3, CV_32FC3 in most cases\n      StructField( mode , IntegerType, false) ::\n      // Bytes in OpenCV-compatible order: row-wise BGR in most cases\n      StructField( data , BinaryType, false) :: Nil)  After loading the image, user can compose the preprocess steps with the  Preprocessing  defined\nin  com.intel.analytics.zoo.feature.image .", 
            "title": "NNImageReader"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/autograd/", 
            "text": "", 
            "title": "Autograd"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/", 
            "text": "", 
            "title": "Net"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/", 
            "text": "", 
            "title": "Image"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/preprocessing/", 
            "text": "", 
            "title": "Preprocessing"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/", 
            "text": "Analytics Zoo Object Detection API\n\n\nAnalytics Zoo provides a collection of pre-trained models for Object Detection. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios. User can run the inference as local program without Spark Context, or in a distributed environment such like Apache Spark, Apache Storm or Apache Flink.\n\n\nModel Load\n\n\nUse \nObjectDetector.loadModel\n(in Scala) or \nObjectDetector.load_model\n (in Python) to load an pre-trained Analytics Zoo model or third-party(BigDL) model. We just need to specify the model path and optionally weight path if exists where we previously saved the model.\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.objectdetection._\n\nval model = ObjectDetector.loadModel[Float](\n/tmp/zoo.model\n) //load from local fs\nval model = ObjectDetector.loadModel(\nhdfs://...\n) //load from hdfs\nval model = ObjectDetector.loadModel(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nfrom zoo.models.image.objectdetection import *\n\nmodel = ObjectDetector.load_model(\n/tmp/zoo.model\n) //load from local fs\nmodel = ObjectDetector.load_model(\nhdfs://...\n) //load from hdfs\nmodel = ObjectDetector.load_model(\ns3://...\n) //load from s3\n\n\n\n\nCreat image configuration\n\n\nIf the loaded model is a published Analytics Zoo model, when you call \nObjectDetector.loadModel\n(in Scala) or \nObjectDetector.load_model\n (in Python), it would create the default Image Configuration for model inference. If the loaded model is not a published Analytics Zoo model or you want to customize the configuration for model inference, you need to create your own Image Configuration.\n\n\nScala API\n\n\nImageConfigure[T: ClassTag](\n  preProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  postProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  batchPerPartition: Int = 4,\n  labelMap: Map[Int, String] = null,\n  featurePaddingParam: Option[PaddingParam[T]] = None)\n\n\n\n\n\n\npreProcessor: preprocessor of ImageSet before model inference\n\n\npostProcessor: postprocessor of ImageSet after model inference\n\n\nbatchPerPartition: batch size per partition\n\n\nlabelMap: label mapping\n\n\nfeaturePaddingParam: featurePaddingParam if the inputs have variant size\n\n\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.common._\nimport com.intel.analytics.zoo.feature.image._\n\nval preprocessing = ImageResize(256, 256)-\n ImageCenterCrop(224, 224) -\n\n                     ImageChannelNormalize(123, 117, 104) -\n\n                     ImageMatToTensor[Float]() -\n\n                     ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preProcessor=preprocessing)\n\n\n\n\nPython API\n\n\nclass ImageConfigure()\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type=\nfloat\n)\n\n\n\n\n\n\npre_processor:  preprocessor of ImageSet before model inference\n\n\npost_processor:  postprocessor of ImageSet after model inference\n\n\nbatch_per_partition:  batch size per partition\n\n\nlabel_map mapping:  from prediction result indexes to real dataset labels\n\n\nfeature_padding_param:  featurePaddingParam if the inputs have variant size\n\n\n\n\nPython example\n\n\nfrom zoo.models.image.common.image_config import *\nfrom zoo.feature.image.imagePreprocessing import *\n\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(pre_processor=preprocessing)\n\n\n\n\nPredict with loaded object detection model\n\n\nScala API\n\n\npredictImageSet(image: ImageSet, configure: ImageConfigure[T] = null)\n\n\n\n\n\n\nimage:  Analytics Zoo ImageSet to be predicted\n\n\nconfigure: Image Configure for this prediction\n\n\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.objectdetection._\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.feature.image._\n\nval imagePath=\n/tmp/image\n\nval sc = NNContext.getNNContext()\nval model = ObjectDetector.loadModel(\n/tmp/analytics-zoo_ssd-mobilenet-300x300_PASCAL_0.1.0.model\n)\nval data = ImageSet.read(image_path, sc)\nval output = model.predictImageSet(data)\n\n\n\n\nPython API\n\n\npredict_image_set(image, configure=None)\n\n\n\n\n\n\nimage:  Analytics Zoo ImageSet to be predicted\n\n\nconfigure: Image Configure for this  prediction\n\n\n\n\nPython example\n\n\nfrom zoo.common.nncontext import get_nncontext\nfrom zoo.models.image.objectdetection import *\n\nimc = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = imc.predict_image_set(image_set)", 
            "title": "Object Detection"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#analytics-zoo-object-detection-api", 
            "text": "Analytics Zoo provides a collection of pre-trained models for Object Detection. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios. User can run the inference as local program without Spark Context, or in a distributed environment such like Apache Spark, Apache Storm or Apache Flink.", 
            "title": "Analytics Zoo Object Detection API"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#model-load", 
            "text": "Use  ObjectDetector.loadModel (in Scala) or  ObjectDetector.load_model  (in Python) to load an pre-trained Analytics Zoo model or third-party(BigDL) model. We just need to specify the model path and optionally weight path if exists where we previously saved the model.  Scala example  import com.intel.analytics.zoo.models.image.objectdetection._\n\nval model = ObjectDetector.loadModel[Float]( /tmp/zoo.model ) //load from local fs\nval model = ObjectDetector.loadModel( hdfs://... ) //load from hdfs\nval model = ObjectDetector.loadModel( s3://... ) //load from s3  Python example  from zoo.models.image.objectdetection import *\n\nmodel = ObjectDetector.load_model( /tmp/zoo.model ) //load from local fs\nmodel = ObjectDetector.load_model( hdfs://... ) //load from hdfs\nmodel = ObjectDetector.load_model( s3://... ) //load from s3", 
            "title": "Model Load"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#creat-image-configuration", 
            "text": "If the loaded model is a published Analytics Zoo model, when you call  ObjectDetector.loadModel (in Scala) or  ObjectDetector.load_model  (in Python), it would create the default Image Configuration for model inference. If the loaded model is not a published Analytics Zoo model or you want to customize the configuration for model inference, you need to create your own Image Configuration.  Scala API  ImageConfigure[T: ClassTag](\n  preProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  postProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  batchPerPartition: Int = 4,\n  labelMap: Map[Int, String] = null,\n  featurePaddingParam: Option[PaddingParam[T]] = None)   preProcessor: preprocessor of ImageSet before model inference  postProcessor: postprocessor of ImageSet after model inference  batchPerPartition: batch size per partition  labelMap: label mapping  featurePaddingParam: featurePaddingParam if the inputs have variant size   Scala example  import com.intel.analytics.zoo.models.image.common._\nimport com.intel.analytics.zoo.feature.image._\n\nval preprocessing = ImageResize(256, 256)-  ImageCenterCrop(224, 224) - \n                     ImageChannelNormalize(123, 117, 104) - \n                     ImageMatToTensor[Float]() - \n                     ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preProcessor=preprocessing)  Python API  class ImageConfigure()\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type= float )   pre_processor:  preprocessor of ImageSet before model inference  post_processor:  postprocessor of ImageSet after model inference  batch_per_partition:  batch size per partition  label_map mapping:  from prediction result indexes to real dataset labels  feature_padding_param:  featurePaddingParam if the inputs have variant size   Python example  from zoo.models.image.common.image_config import *\nfrom zoo.feature.image.imagePreprocessing import *\n\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(pre_processor=preprocessing)", 
            "title": "Creat image configuration"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#predict-with-loaded-object-detection-model", 
            "text": "Scala API  predictImageSet(image: ImageSet, configure: ImageConfigure[T] = null)   image:  Analytics Zoo ImageSet to be predicted  configure: Image Configure for this prediction   Scala example  import com.intel.analytics.zoo.models.image.objectdetection._\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.feature.image._\n\nval imagePath= /tmp/image \nval sc = NNContext.getNNContext()\nval model = ObjectDetector.loadModel( /tmp/analytics-zoo_ssd-mobilenet-300x300_PASCAL_0.1.0.model )\nval data = ImageSet.read(image_path, sc)\nval output = model.predictImageSet(data)  Python API  predict_image_set(image, configure=None)   image:  Analytics Zoo ImageSet to be predicted  configure: Image Configure for this  prediction   Python example  from zoo.common.nncontext import get_nncontext\nfrom zoo.models.image.objectdetection import *\n\nimc = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = imc.predict_image_set(image_set)", 
            "title": "Predict with loaded object detection model"
        }, 
        {
            "location": "/APIGuide/Models/image-classification/", 
            "text": "", 
            "title": "Image Classification"
        }, 
        {
            "location": "/APIGuide/Models/text-classification/", 
            "text": "Analytics Zoo Text Classification API\n\n\nAnalytics Zoo provides a set of pre-defined models that can be used for classifying texts with different encoders. This model could be fed into NNFrames and BigDL Optimizer directly for training.\n\n\nScala\n\n\nTextClassifier(classNum, tokenLength, sequenceLength = 500, encoder = \ncnn\n, encoderOutputDim = 256)\n\n\n\n\n\n\nclassNum\n: The number of text categories to be classified. Positive integer.\n\n\ntokenLength\n: The size of each word vector. Positive integer.\n\n\nsequenceLength\n: The length of a sequence. Positive integer. Default is 500.\n\n\nencoder\n: The encoder for input sequences. String. \"cnn\" or \"lstm\" or \"gru\" are supported. Default is \"cnn\".\n\n\nencoderOutputDim\n: The output dimension for the encoder. Positive integer. Default is 256.\n\n\n\n\nSee \nhere\n for the Scala example that trains the \nTextClassifier\n model on 20 Newsgroup dataset and uses the model to do prediction.\n\n\nPython\n\n\nTextClassifier(class_num, token_length, sequence_length=500, encoder=\ncnn\n, encoder_output_dim=256)\n\n\n\n\n\n\nclass_num\n: The number of text categories to be classified. Positive int.\n\n\ntoken_length\n: The size of each word vector. Positive int.\n\n\nsequence_length\n: The length of a sequence. Positive int. Default is 500.\n\n\nencoder\n: The encoder for input sequences. String. 'cnn' or 'lstm' or 'gru' are supported. Default is 'cnn'.\n\n\nencoder_output_dim\n: The output dimension for the encoder. Positive int. Default is 256.\n\n\n\n\nSee \nhere\n for the Python example that trains the \nTextClassifier\n model on 20 Newsgroup dataset and uses the model to do prediction.", 
            "title": "Text Classification"
        }, 
        {
            "location": "/APIGuide/Models/text-classification/#analytics-zoo-text-classification-api", 
            "text": "Analytics Zoo provides a set of pre-defined models that can be used for classifying texts with different encoders. This model could be fed into NNFrames and BigDL Optimizer directly for training.  Scala  TextClassifier(classNum, tokenLength, sequenceLength = 500, encoder =  cnn , encoderOutputDim = 256)   classNum : The number of text categories to be classified. Positive integer.  tokenLength : The size of each word vector. Positive integer.  sequenceLength : The length of a sequence. Positive integer. Default is 500.  encoder : The encoder for input sequences. String. \"cnn\" or \"lstm\" or \"gru\" are supported. Default is \"cnn\".  encoderOutputDim : The output dimension for the encoder. Positive integer. Default is 256.   See  here  for the Scala example that trains the  TextClassifier  model on 20 Newsgroup dataset and uses the model to do prediction.  Python  TextClassifier(class_num, token_length, sequence_length=500, encoder= cnn , encoder_output_dim=256)   class_num : The number of text categories to be classified. Positive int.  token_length : The size of each word vector. Positive int.  sequence_length : The length of a sequence. Positive int. Default is 500.  encoder : The encoder for input sequences. String. 'cnn' or 'lstm' or 'gru' are supported. Default is 'cnn'.  encoder_output_dim : The output dimension for the encoder. Positive int. Default is 256.   See  here  for the Python example that trains the  TextClassifier  model on 20 Newsgroup dataset and uses the model to do prediction.", 
            "title": "Analytics Zoo Text Classification API"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/", 
            "text": "Analytics Zoo Recommender API\n\n\nAnalytics Zoo provides two Recommenders, including Wide and Deep (WND) model and Neural network-based Collaborative Filtering (NCF) model. Each model could be fed into NNFrames and BigDL Optimizer directly for training.\n\n\nRecommenders can handle models with either explict or implicit feedback, given corresponding features.\n\n\nWe also provide three user-friendly APIs to predict user item pairs, and recommend items (users) for users (items). See \nhere\n for more details.\n\n\n\n\nWide and Deep\n\n\nWide and Deep Learning Model, proposed by \nGoogle, 2016\n, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.\n\n\nAfter training the model, users can use the model to \ndo prediction and recommendation\n.\n\n\nScala\n\n\nWideAndDeep(modelType = \nwide_n_deep\n, numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))\n\n\n\n\n\n\nmodelType\n: String. \"wide\", \"deep\", \"wide_n_deep\" are supported. Default is \"wide_n_deep\".\n\n\nnumClasses\n: The number of classes. Positive integer.\n\n\ncolumnInfo\n An instance of \nColumnFeatureInfo\n.\n\n\nhiddenLayers\n: Units of hidden layers for the deep model. Array of positive integers. Default is Array(40, 20, 10).\n\n\n\n\nSee \nhere\n for the Scala example that trains the \nWideAndDeep\n model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\nPython\n\n\nWideAndDeep(class_num, column_info, model_type=\nwide_n_deep\n, hidden_layers=(40, 20, 10))\n\n\n\n\n\n\nclass_num\n: The number of classes. Positive int.\n\n\ncolumn_info\n: An instance of \nColumnFeatureInfo\n.\n\n\nmodel_type\n: String. 'wide', 'deep' and 'wide_n_deep' are supported. Default is 'wide_n_deep'.\n\n\nhidden_layers\n: Units of hidden layers for the deep model. Tuple of positive int. Default is (40, 20, 10).\n\n\n\n\nSee \nhere\n for the Python notebook that trains the \nWideAndDeep\n model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\nColumnFeatureInfo\n\n\nAn instance of \nColumnFeatureInfo\n contains the same data information shared by the \nWideAndDeep\n model and its feature generation part.\n\n\nYou can choose to include the following information for feature engineering and the \nWideAndDeep\n model:\n\n\n\n\nwideBaseCols\n: Data of \nwideBaseCols\n together with \nwideCrossCols\n will be fed into the wide model.\n\n\nwideBaseDims\n: Dimensions of \nwideBaseCols\n. The dimensions of the data in \nwideBaseCols\n should be within the range of \nwideBaseDims\n.\n\n\nwideCrossCols\n: Data of \nwideCrossCols\n will be fed into the wide model.\n\n\nwideCrossDims\n: Dimensions of \nwideCrossCols\n. The dimensions of the data in \nwideCrossCols\n should be within the range of \nwideCrossDims\n.\n\n\nindicatorCols\n: Data of \nindicatorCols\n will be fed into the deep model as multi-hot vectors. \n\n\nindicatorDims\n: Dimensions of \nindicatorCols\n. The dimensions of the data in \nindicatorCols\n should be within the range of \nindicatorDims\n.\n\n\nembedCols\n: Data of \nembedCols\n will be fed into the deep model as embeddings.\n\n\nembedInDims\n: Input dimension of the data in \nembedCols\n. The dimensions of the data in \nembedCols\n should be within the range of \nembedInDims\n.\n\n\nembedOutDims\n: The dimensions of embeddings for \nembedCols\n.\n\n\ncontinuousCols\n: Data of \ncontinuousCols\n will be treated as continuous values for the deep model.\n\n\nlabel\n: The name of the 'label' column. String. Default is \"label\".\n\n\n\n\nRemark:\n\n\nFields that involve \nCols\n should be an array of String (Scala) or a list of String (Python) indicating the name of the columns in the data.\n\n\nFields that involve \nDims\n should be an array of integers (Scala) or a list of integers (Python) indicating the dimensions of the corresponding columns.\n\n\nIf any field is not specified, it will by default to be an empty array (Scala) or an empty list (Python).\n\n\nScala\n\n\nColumnFeatureInfo(\n    wideBaseCols = Array[String](),\n    wideBaseDims = Array[Int](),\n    wideCrossCols = Array[String](),\n    wideCrossDims = Array[Int](),\n    indicatorCols = Array[String](),\n    indicatorDims = Array[Int](),\n    embedCols = Array[String](),\n    embedInDims = Array[Int](),\n    embedOutDims = Array[Int](),\n    continuousCols = Array[String](),\n    label = \nlabel\n)\n\n\n\n\nPython\n\n\nColumnFeatureInfo(\n    wide_base_cols=None,\n    wide_base_dims=None,\n    wide_cross_cols=None,\n    wide_cross_dims=None,\n    indicator_cols=None,\n    indicator_dims=None,\n    embed_cols=None,\n    embed_in_dims=None,\n    embed_out_dims=None,\n    continuous_cols=None,\n    label=\nlabel\n)\n\n\n\n\n\n\nNeural network-based Collaborative Filtering\n\n\nNCF (\nHe, 2015\n) leverages a multi-layer perceptrons to learn the user\u2013item interaction function. At the mean time, NCF can express and generalize matrix factorization under its framework. \nincludeMF\n(Boolean) is provided for users to build a \nNeuralCF\n model with or without matrix factorization. \n\n\nAfter training the model, users can use the model to \ndo prediction and recommendation\n.\n\n\nScala\n\n\nNeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)\n\n\n\n\n\n\nuserCount\n: The number of users. Positive integer.\n\n\nitemCount\n: The number of items. Positive integer.\n\n\nnumClasses\n: The number of classes. Positive integer.\n\n\nuserEmbed\n: Units of user embedding. Positive integer. Default is 20.\n\n\nitemEmbed\n: Units of item embedding. Positive integer. Default is 20.\n\n\nhiddenLayers\n: Units hiddenLayers for MLP. Array of positive integers. Default is Array(40, 20, 10).\n\n\nincludeMF\n: Whether to include Matrix Factorization. Boolean. Default is true.\n\n\nmfEmbed\n: Units of matrix factorization embedding. Positive integer. Default is 20.\n\n\n\n\nSee \nhere\n for the Scala example that trains the \nNeuralCF\n model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\nPython\n\n\nNeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)\n\n\n\n\n\n\nuser_count\n: The number of users. Positive int.\n\n\nitem_count\n: The number of classes. Positive int.\n\n\nclass_num:\n The number of classes. Positive int.\n\n\nuser_embed\n: Units of user embedding. Positive int. Default is 20.\n\n\nitem_embed\n: itemEmbed Units of item embedding. Positive int. Default is 20.\n\n\nhidden_layers\n: Units of hidden layers for MLP. Tuple of positive int. Default is (40, 20, 10).\n\n\ninclude_mf\n: Whether to include Matrix Factorization. Boolean. Default is True.\n\n\nmf_embed\n: Units of matrix factorization embedding. Positive int. Default is 20.\n\n\n\n\nSee \nhere\n for the Python notebook that trains the \nNeuralCF\n model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\n\n\nPrediction and Recommendation\n\n\nPredict for user-item pairs\n\n\nGive prediction for each pair of user and item. Return RDD of \nUserItemPrediction\n.\n\n\nScala\n\n\npredictUserItemPair(featureRdd)\n\n\n\n\nPython\n\n\npredict_user_item_pair(feature_rdd)\n\n\n\n\nParameters:\n\n\n\n\nfeatureRdd\n: RDD of \nUserItemFeature\n.\n\n\n\n\nRecommend for users\n\n\nRecommend a number of items for each user. Return RDD of \nUserItemPrediction\n.\n\n\nScala\n\n\nrecommendForUser(featureRdd, maxItems)\n\n\n\n\nPython\n\n\nrecommend_for_user(feature_rdd, max_items)\n\n\n\n\nParameters:\n\n\n\n\nfeatureRdd\n: RDD of \nUserItemFeature\n.\n\n\nmaxItems\n: The number of items to be recommended to each user. Positive integer.\n\n\n\n\nRecommend for items\n\n\nRecommend a number of users for each item. Return RDD of \nUserItemPrediction\n.\n\n\nScala\n\n\nrecommendForItem(featureRdd, maxUsers)\n\n\n\n\nPython\n\n\nrecommend_for_item(feature_rdd, max_users)\n\n\n\n\nParameters:\n\n\n\n\nfeatureRdd\n: RDD of \nUserItemFeature\n.\n\n\nmaxUsers\n: The number of users to be recommended to each item. Positive integer.\n\n\n\n\nUserItemFeature\n\n\nRepresent records of user-item with features.\n\n\nEach record should contain the following fields:\n\n\n\n\nuserId\n: Positive integer.\n\n\nitem_id\n: Positive integer.\n\n\nsample\n: \nSample\n which consists of feature(s) and label(s).\n\n\n\n\nScala\n\n\nUserItemFeature(userId, itemId, sample)\n\n\n\n\nPython\n\n\nUserItemFeature(user_id, item_id, sample)\n\n\n\n\nUserItemPrediction\n\n\nRepresent the prediction results of user-item pairs.\n\n\nEach prediction record will contain the following information:\n\n\n\n\nuserId\n: Positive integer.\n\n\nitemId\n: Positive integer.\n\n\nprediction\n: The prediction (rating) for the user on the item.\n\n\nprobability\n: The probability for the prediction.\n\n\n\n\nScala\n\n\nUserItemPrediction(userId, itemId, prediction, probability)\n\n\n\n\nPython\n\n\nUserItemPrediction(user_id, item_id, prediction, probability)", 
            "title": "Recommendation"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#analytics-zoo-recommender-api", 
            "text": "Analytics Zoo provides two Recommenders, including Wide and Deep (WND) model and Neural network-based Collaborative Filtering (NCF) model. Each model could be fed into NNFrames and BigDL Optimizer directly for training.  Recommenders can handle models with either explict or implicit feedback, given corresponding features.  We also provide three user-friendly APIs to predict user item pairs, and recommend items (users) for users (items). See  here  for more details.", 
            "title": "Analytics Zoo Recommender API"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#wide-and-deep", 
            "text": "Wide and Deep Learning Model, proposed by  Google, 2016 , is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.  After training the model, users can use the model to  do prediction and recommendation .  Scala  WideAndDeep(modelType =  wide_n_deep , numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))   modelType : String. \"wide\", \"deep\", \"wide_n_deep\" are supported. Default is \"wide_n_deep\".  numClasses : The number of classes. Positive integer.  columnInfo  An instance of  ColumnFeatureInfo .  hiddenLayers : Units of hidden layers for the deep model. Array of positive integers. Default is Array(40, 20, 10).   See  here  for the Scala example that trains the  WideAndDeep  model on MovieLens 1M dataset and uses the model to do prediction and recommendation.  Python  WideAndDeep(class_num, column_info, model_type= wide_n_deep , hidden_layers=(40, 20, 10))   class_num : The number of classes. Positive int.  column_info : An instance of  ColumnFeatureInfo .  model_type : String. 'wide', 'deep' and 'wide_n_deep' are supported. Default is 'wide_n_deep'.  hidden_layers : Units of hidden layers for the deep model. Tuple of positive int. Default is (40, 20, 10).   See  here  for the Python notebook that trains the  WideAndDeep  model on MovieLens 1M dataset and uses the model to do prediction and recommendation.", 
            "title": "Wide and Deep"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#columnfeatureinfo", 
            "text": "An instance of  ColumnFeatureInfo  contains the same data information shared by the  WideAndDeep  model and its feature generation part.  You can choose to include the following information for feature engineering and the  WideAndDeep  model:   wideBaseCols : Data of  wideBaseCols  together with  wideCrossCols  will be fed into the wide model.  wideBaseDims : Dimensions of  wideBaseCols . The dimensions of the data in  wideBaseCols  should be within the range of  wideBaseDims .  wideCrossCols : Data of  wideCrossCols  will be fed into the wide model.  wideCrossDims : Dimensions of  wideCrossCols . The dimensions of the data in  wideCrossCols  should be within the range of  wideCrossDims .  indicatorCols : Data of  indicatorCols  will be fed into the deep model as multi-hot vectors.   indicatorDims : Dimensions of  indicatorCols . The dimensions of the data in  indicatorCols  should be within the range of  indicatorDims .  embedCols : Data of  embedCols  will be fed into the deep model as embeddings.  embedInDims : Input dimension of the data in  embedCols . The dimensions of the data in  embedCols  should be within the range of  embedInDims .  embedOutDims : The dimensions of embeddings for  embedCols .  continuousCols : Data of  continuousCols  will be treated as continuous values for the deep model.  label : The name of the 'label' column. String. Default is \"label\".   Remark:  Fields that involve  Cols  should be an array of String (Scala) or a list of String (Python) indicating the name of the columns in the data.  Fields that involve  Dims  should be an array of integers (Scala) or a list of integers (Python) indicating the dimensions of the corresponding columns.  If any field is not specified, it will by default to be an empty array (Scala) or an empty list (Python).  Scala  ColumnFeatureInfo(\n    wideBaseCols = Array[String](),\n    wideBaseDims = Array[Int](),\n    wideCrossCols = Array[String](),\n    wideCrossDims = Array[Int](),\n    indicatorCols = Array[String](),\n    indicatorDims = Array[Int](),\n    embedCols = Array[String](),\n    embedInDims = Array[Int](),\n    embedOutDims = Array[Int](),\n    continuousCols = Array[String](),\n    label =  label )  Python  ColumnFeatureInfo(\n    wide_base_cols=None,\n    wide_base_dims=None,\n    wide_cross_cols=None,\n    wide_cross_dims=None,\n    indicator_cols=None,\n    indicator_dims=None,\n    embed_cols=None,\n    embed_in_dims=None,\n    embed_out_dims=None,\n    continuous_cols=None,\n    label= label )", 
            "title": "ColumnFeatureInfo"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#neural-network-based-collaborative-filtering", 
            "text": "NCF ( He, 2015 ) leverages a multi-layer perceptrons to learn the user\u2013item interaction function. At the mean time, NCF can express and generalize matrix factorization under its framework.  includeMF (Boolean) is provided for users to build a  NeuralCF  model with or without matrix factorization.   After training the model, users can use the model to  do prediction and recommendation .  Scala  NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)   userCount : The number of users. Positive integer.  itemCount : The number of items. Positive integer.  numClasses : The number of classes. Positive integer.  userEmbed : Units of user embedding. Positive integer. Default is 20.  itemEmbed : Units of item embedding. Positive integer. Default is 20.  hiddenLayers : Units hiddenLayers for MLP. Array of positive integers. Default is Array(40, 20, 10).  includeMF : Whether to include Matrix Factorization. Boolean. Default is true.  mfEmbed : Units of matrix factorization embedding. Positive integer. Default is 20.   See  here  for the Scala example that trains the  NeuralCF  model on MovieLens 1M dataset and uses the model to do prediction and recommendation.  Python  NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)   user_count : The number of users. Positive int.  item_count : The number of classes. Positive int.  class_num:  The number of classes. Positive int.  user_embed : Units of user embedding. Positive int. Default is 20.  item_embed : itemEmbed Units of item embedding. Positive int. Default is 20.  hidden_layers : Units of hidden layers for MLP. Tuple of positive int. Default is (40, 20, 10).  include_mf : Whether to include Matrix Factorization. Boolean. Default is True.  mf_embed : Units of matrix factorization embedding. Positive int. Default is 20.   See  here  for the Python notebook that trains the  NeuralCF  model on MovieLens 1M dataset and uses the model to do prediction and recommendation.", 
            "title": "Neural network-based Collaborative Filtering"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#prediction-and-recommendation", 
            "text": "", 
            "title": "Prediction and Recommendation"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#predict-for-user-item-pairs", 
            "text": "Give prediction for each pair of user and item. Return RDD of  UserItemPrediction .  Scala  predictUserItemPair(featureRdd)  Python  predict_user_item_pair(feature_rdd)  Parameters:   featureRdd : RDD of  UserItemFeature .", 
            "title": "Predict for user-item pairs"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#recommend-for-users", 
            "text": "Recommend a number of items for each user. Return RDD of  UserItemPrediction .  Scala  recommendForUser(featureRdd, maxItems)  Python  recommend_for_user(feature_rdd, max_items)  Parameters:   featureRdd : RDD of  UserItemFeature .  maxItems : The number of items to be recommended to each user. Positive integer.", 
            "title": "Recommend for users"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#recommend-for-items", 
            "text": "Recommend a number of users for each item. Return RDD of  UserItemPrediction .  Scala  recommendForItem(featureRdd, maxUsers)  Python  recommend_for_item(feature_rdd, max_users)  Parameters:   featureRdd : RDD of  UserItemFeature .  maxUsers : The number of users to be recommended to each item. Positive integer.", 
            "title": "Recommend for items"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#useritemfeature", 
            "text": "Represent records of user-item with features.  Each record should contain the following fields:   userId : Positive integer.  item_id : Positive integer.  sample :  Sample  which consists of feature(s) and label(s).   Scala  UserItemFeature(userId, itemId, sample)  Python  UserItemFeature(user_id, item_id, sample)", 
            "title": "UserItemFeature"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#useritemprediction", 
            "text": "Represent the prediction results of user-item pairs.  Each prediction record will contain the following information:   userId : Positive integer.  itemId : Positive integer.  prediction : The prediction (rating) for the user on the item.  probability : The probability for the prediction.   Scala  UserItemPrediction(userId, itemId, prediction, probability)  Python  UserItemPrediction(user_id, item_id, prediction, probability)", 
            "title": "UserItemPrediction"
        }, 
        {
            "location": "/powered-by/", 
            "text": "Powered By", 
            "title": "Powered by"
        }, 
        {
            "location": "/powered-by/#powered-by", 
            "text": "", 
            "title": "Powered By"
        }, 
        {
            "location": "/known-issues/", 
            "text": "If you encounter the following exception when calling the Python API of Analytics Zoo:\n\n\n\n\nPy4JJavaError: An error occurred while calling z:org.apache.spark.bigdl.api.python.BigDLSerDe.loads.\n: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\n\n\n\nyou may need to check whether your input argument involves Numpy types (such as \nnumpy.int64\n). See \nhere\n for the related issue.\n\n\nFor example, invoking \nnp.min\n, \nnp.max\n, \nnp.unique\n, etc. will return type \nnumpy.int64\n. One way to solve this is to use \nint()\n to convert a number of type \nnumpy.int64\n to a Python int.", 
            "title": "FAQ and Known Issues"
        }
    ]
}