{
    "docs": [
        {
            "location": "/", 
            "text": "Analytics Zoo\n\n\nA unified analytics + AI platform for \ndistributed TensorFlow, Keras and BigDL on Apache Spark\n\n\n\n\nWhat is Analytics Zoo?\n\n\nAnalytics Zoo\n provides a unified analytics + AI platform that seamlessly unites \nSpark, TensorFlow, Keras and BigDL\n programs into an integrated pipeline; the entire pipeline can then transparently scale out to a large Hadoop/Spark cluster for distributed training or inference. \n\n\n\n\nData wrangling and analysis using PySpark\n\n\nDeep learning model development using TensorFlow or Keras\n\n\nDistributed training/inference on Spark and BigDL\n\n\nAll within a single unified pipeline and in a user-transparent fashion!\n\n\n\n\nIn addition, Analytics Zoo also provides a rich set of analytics and AI support for the end-to-end pipeline, including:\n\n\n\n\nEasy-to-use abstractions and APIs\n (e.g., transfer learning support, autograd operations, Spark DataFrame and ML pipeline support, online model serving API, etc.) \n\n\nCommon feature engineering operations\n (for image, text, 3D image, etc.)\n\n\nBuilt-in deep learning models\n (e.g., object detection, image classification, text classification, recommendation, etc.) \n\n\nReference use cases\n (e.g., anomaly detection, sentiment analysis, fraud detection, image similarity, etc.)\n\n\n\n\nHow to use Analytics Zoo?\n\n\n\n\n\n\nTo get started, please refer to the \nPython install guide\n or \nScala install guide\n.\n\n\n\n\n\n\nFor running distributed TensorFlow on Spark and BigDL, please refer to the quick start \nhere\n and the details \nhere\n.\n\n\n\n\n\n\nFor more information, You may refer to the \nAnalytics Zoo document website\n.\n\n\n\n\n\n\nFor additional questions and discussions, you can join the \nGoogle User Group\n (or subscribe to the \nMail List\n).\n\n\n\n\n\n\n\n\nOverview\n\n\n\n\n\n\nDistributed Tensorflow and Keras on Spark/BigDL\n\n\n\n\nData wrangling and analysis using PySpark\n\n\nDeep learning model development using TensorFlow or Keras\n\n\nDistributed training/inference on Spark and BigDL\n\n\nAll within a single unified pipeline and in a user-transparent fashion!\n\n\n\n\n\n\n\n\nHigh level abstractions and APIs\n\n\n\n\nTransfer learning\n: customize pretained model for \nfeature extraction or fine-tuning\n\n\nautograd\n: build custom layer/loss using \nauto differentiation operations\n \n\n\nnnframes\n: native deep learning support in \nSpark DataFrames and ML Pipelines\n\n\nModel serving\n: productionize \nmodel serving and inference\n using \nPOJO\n APIs\n\n\n\n\n\n\n\n\nBuilt-in deep learning models\n\n\n\n\nObject detection API\n: high-level API and pretrained models (e.g., SSD and Faster-RCNN) for \nobject detection\n\n\nImage classification API\n: high-level API and pretrained models (e.g., VGG, Inception, ResNet, MobileNet, etc.) for \nimage classification\n\n\nText classification API\n: high-level API and pre-defined models (using CNN, LSTM, etc.) for \ntext classification\n\n\nRecommedation API\n: high-level API and pre-defined models (e.g., Neural Collaborative Filtering, Wide and Deep Learning, etc.) for \nrecommendation\n\n\n\n\n\n\n\n\nReference use cases\n: a collection of end-to-end \nreference use cases\n (e.g., anomaly detection, sentiment analysis, fraud detection, image augmentation, object detection, variational autoencoder, etc.)\n\n\n\n\n\n\nDistributed Tensorflow and Keras on Spark/BigDL\n\n\nTo make it easy to build and productionize the deep learning applications for Big Data, Analytics Zoo provides a unified analytics + AI platform that seamlessly unites Spark, TensorFlow, Keras and BigDL programs into an integrated pipeline (as illustrated below), which can then transparently run on a large-scale Hadoop/Spark clusters for distributed training and inference. (Please see more details \nhere\n).\n\n\n1.Data wrangling and analysis using PySpark\n\n\n   from zoo import init_nncontext\n   from zoo.pipeline.api.net import TFDataset\n\n   sc = init_nncontext()\n\n   #Each record in the train_rdd consists of a list of NumPy ndrrays\n   train_rdd = sc.parallelize(file_list)\n     .map(lambda x: read_image_and_label(x))\n     .map(lambda image_label: decode_to_ndarrays(image_label))\n\n   #TFDataset represents a distributed set of elements,\n   #in which each element contains one or more Tensorflow Tensor objects. \n   dataset = TFDataset.from_rdd(train_rdd,\n                                names=[\nfeatures\n, \nlabels\n],\n                                shapes=[[28, 28, 1], [1]],\n                                types=[tf.float32, tf.int32],\n                                batch_size=BATCH_SIZE)\n\n\n\n\n2.Deep learning model development using TensorFlow\n\n\n   import tensorflow as tf\n\n   slim = tf.contrib.slim\n\n   images, labels = dataset.tensors\n   labels = tf.squeeze(labels)\n   with slim.arg_scope(lenet.lenet_arg_scope()):\n        logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)\n\n   loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n\n\n\n\n3.Distributed training on Spark and BigDL\n\n\n   from zoo.pipeline.api.net import TFOptimizer\n   from bigdl.optim.optimizer import MaxIteration, Adam, MaxEpoch, TrainSummary\n\n   optimizer = TFOptimizer(loss, Adam(1e-3))\n   optimizer.set_train_summary(TrainSummary(\n/tmp/az_lenet\n, \nlenet\n))\n   optimizer.optimize(end_trigger=MaxEpoch(5))\n\n\n\n\n4.Alternatively, using Keras APIs for model development and distributed training\n\n\n   from zoo.pipeline.api.keras.models import *\n   from zoo.pipeline.api.keras.layers import *\n\n   model = Sequential()\n   model.add(Reshape((1, 28, 28), input_shape=(28, 28, 1)))\n   model.add(Convolution2D(6, 5, 5, activation=\ntanh\n, name=\nconv1_5x5\n))\n   model.add(MaxPooling2D())\n   model.add(Convolution2D(12, 5, 5, activation=\ntanh\n, name=\nconv2_5x5\n))\n   model.add(MaxPooling2D())\n   model.add(Flatten())\n   model.add(Dense(100, activation=\ntanh\n, name=\nfc1\n))\n   model.add(Dense(class_num, activation=\nsoftmax\n, name=\nfc2\n))\n\n   model.compile(loss='sparse_categorical_crossentropy',\n                 optimizer='adam')\n   model.fit(train_rdd, batch_size=BATCH_SIZE, nb_epoch=5)\n\n\n\n\nHigh level abstractions and APIs\n\n\nAnalytics Zoo provides a set of easy-to-use, high level abstractions and APIs that natively transfer learning, autograd and custom layer/loss, Spark DataFrames and ML Pipelines, online model serving, etc. etc.\n\n\nTransfer learning\n\n\nUsing the high level transfer learning APIs, you can easily customize pretrained models for \nfeature extraction or fine-tuning\n. (See more details \nhere\n)\n\n\n1.Load an existing model (pretrained in Caffe)\n\n\n   from zoo.pipeline.api.net import *\n   full_model = Net.load_caffe(def_path, model_path)\n\n\n\n\n2.Remove the last few layers\n\n\n   # create a new model by removing layers after pool5/drop_7x7_s1\n   model = full_model.new_graph([\npool5/drop_7x7_s1\n])\n\n\n\n\n3.Freeze the first few layers\n\n\n   # freeze layers from input to pool4/3x3_s2 inclusive\n   model.freeze_up_to([\npool4/3x3_s2\n])\n\n\n\n\n4.Add a few new layers\n\n\n   from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   inputs = Input(name=\ninput\n, shape=(3, 224, 224))\n   inception = model.to_keras()(inputs)\n   flatten = Flatten()(inception)\n   logits = Dense(2)(flatten)\n   newModel = Model(inputs, logits)\n\n\n\n\nautograd\n\n\nautograd\n provides automatic differentiation for math operations, so that you can easily build your own \ncustom loss and layer\n (in both Python and Scala), as illustracted below. (See more details \nhere\n)\n\n\n1.Define model using Keras-style API and \nautograd\n \n\n\n   import zoo.pipeline.api.autograd as A\n   from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n\n   input = Input(shape=[2, 20])\n   features = TimeDistributed(layer=Dense(30))(input)\n   f1 = features.index_select(1, 0)\n   f2 = features.index_select(1, 1)\n   diff = A.abs(f1 - f2)\n   model = Model(input, diff)\n\n\n\n\n2.Optionally define custom loss function using \nautograd\n\n\n   def mean_absolute_error(y_true, y_pred):\n       return mean(abs(y_true - y_pred), axis=1)\n\n\n\n\n3.Train model with \ncustom loss function\n\n\n   model.compile(optimizer=SGD(), loss=mean_absolute_error)\n   model.fit(x=..., y=...)\n\n\n\n\nnnframes\n\n\nnnframes\n provides \nnative deep learning support in Spark DataFrames and ML Pipelines\n, so that you can easily build complex deep learning pipelines in just a few lines, as illustrated below. (See more details \nhere\n)\n\n\n1.Initialize \nNNContext\n and load images into \nDataFrames\n using \nNNImageReader\n\n\n   from zoo.common.nncontext import *\n   from zoo.pipeline.nnframes import *\n   from zoo.feature.image import *\n   sc = init_nncontext()\n   imageDF = NNImageReader.readImages(image_path, sc)\n\n\n\n\n2.Process loaded data using \nDataFrames transformations\n\n\n   getName = udf(lambda row: ...)\n   getLabel = udf(lambda name: ...)\n   df = imageDF.withColumn(\nname\n, getName(col(\nimage\n))).withColumn(\nlabel\n, getLabel(col('name')))\n\n\n\n\n3.Processing image using built-in \nfeature engineering operations\n\n\n   transformer = RowToImageFeature() -\n ImageResize(64, 64) -\n ImageChannelNormalize(123.0, 117.0, 104.0) \\\n                 -\n ImageMatToTensor() -\n ImageFeatureToTensor())\n\n\n\n\n4.Define model using \nKeras-style APIs\n\n\n   from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   model = Sequential().add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1, 28, 28))) \\\n                   .add(MaxPooling2D(pool_size=(2, 2))).add(Flatten()).add(Dense(10, activation='softmax')))\n\n\n\n\n5.Train model using \nSpark ML Pipelines\n\n\n   classifier = NNClassifier(model, CrossEntropyCriterion(),transformer).setLearningRate(0.003) \\\n                   .setBatchSize(40).setMaxEpoch(1).setFeaturesCol(\nimage\n).setCachingSample(False)\n   nnModel = classifier.fit(df)\n\n\n\n\nModel Serving\n\n\nUsing the \nPOJO\n model serving API, you can productionize model serving and infernece in any Java based frameworks (e.g., \nSpring Framework\n, Apache \nStorm\n, \nKafka\n or \nFlink\n, etc.), as illustrated below:\n\n\nimport com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;\nimport com.intel.analytics.zoo.pipeline.inference.JTensor;\n\npublic class TextClassificationModel extends AbstractInferenceModel {\n    public TextClassificationModel() {\n        super();\n    }\n}\n\nTextClassificationModel model = new TextClassificationModel();\nmodel.load(modelPath, weightPath);\n\nList\nJTensor\n inputs = preprocess(...);\nList\nList\nJTensor\n result = model.predict(inputs);\n...\n\n\n\n\nBuilt-in deep learning models\n\n\nAnalytics Zoo provides several built-in deep learning models that you can use for a variety of problem types, such as \nobject detection\n, \nimage classification\n, \ntext classification\n, \nrecommendation\n, etc.\n\n\nObject detection API\n\n\nUsing \nAnalytics Zoo Object Detection API\n (including a set of pretrained detection models such as SSD and Faster-RCNN), you can easily build your object detection applications (e.g., localizing and identifying multiple objects in images and videos), as illustrated below. (See more details \nhere\n)\n\n\n1.Download object detection models in Analytics Zoo\n\n\nYou can download a collection of detection models (pretrained on the PSCAL VOC dataset and COCO dataset) from \ndetection model zoo\n.\n\n\n2.Use \nObject Detection API\n for off-the-shell inference\n\n\n   from zoo.models.image.objectdetection import *\n   model = ObjectDetector.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)\n\n\n\n\nImage classification API\n\n\nUsing \nAnalytics Zoo Image Classification API\n (including a set of pretrained detection models such as VGG, Inception, ResNet, MobileNet,  etc.), you can easily build your image classification applications, as illustrated below. (See more details \nhere\n)\n\n\n1.Download image classification models in Analytics Zoo\n\n\nYou can download a collection of image classification models (pretrained on the ImageNet dataset) from \nimage classification model zoo\n.\n\n\n2.Use \nImage classification API\n for off-the-shell inference\n\n\n   from zoo.models.image.imageclassification import *\n   model = ImageClassifier.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)\n\n\n\n\nText classification API\n\n\nAnalytics Zoo Text Classification API\n provides a set of pre-defined models (using CNN, LSTM, etc.) for text classifications. (See more details \nhere\n)\n\n\nRecommendation API\n\n\nAnalytics Zoo Recommendation API\n provides a set of pre-defined models (such as Neural Collaborative Filtering, Wide and Deep Learning, etc.) for recommendations. (See more details \nhere\n)\n\n\nReference use cases\n\n\nAnalytics Zoo provides a collection of end-to-end reference use cases, including \ntime series anomaly detection\n, \nsentiment analysis\n, \nfraud detection\n, \nimage similarity\n, etc. (See more details \nhere\n)", 
            "title": "Overview"
        }, 
        {
            "location": "/#analytics-zoo", 
            "text": "A unified analytics + AI platform for  distributed TensorFlow, Keras and BigDL on Apache Spark", 
            "title": "Analytics Zoo"
        }, 
        {
            "location": "/#what-is-analytics-zoo", 
            "text": "Analytics Zoo  provides a unified analytics + AI platform that seamlessly unites  Spark, TensorFlow, Keras and BigDL  programs into an integrated pipeline; the entire pipeline can then transparently scale out to a large Hadoop/Spark cluster for distributed training or inference.    Data wrangling and analysis using PySpark  Deep learning model development using TensorFlow or Keras  Distributed training/inference on Spark and BigDL  All within a single unified pipeline and in a user-transparent fashion!   In addition, Analytics Zoo also provides a rich set of analytics and AI support for the end-to-end pipeline, including:   Easy-to-use abstractions and APIs  (e.g., transfer learning support, autograd operations, Spark DataFrame and ML pipeline support, online model serving API, etc.)   Common feature engineering operations  (for image, text, 3D image, etc.)  Built-in deep learning models  (e.g., object detection, image classification, text classification, recommendation, etc.)   Reference use cases  (e.g., anomaly detection, sentiment analysis, fraud detection, image similarity, etc.)", 
            "title": "What is Analytics Zoo?"
        }, 
        {
            "location": "/#how-to-use-analytics-zoo", 
            "text": "To get started, please refer to the  Python install guide  or  Scala install guide .    For running distributed TensorFlow on Spark and BigDL, please refer to the quick start  here  and the details  here .    For more information, You may refer to the  Analytics Zoo document website .    For additional questions and discussions, you can join the  Google User Group  (or subscribe to the  Mail List ).", 
            "title": "How to use Analytics Zoo?"
        }, 
        {
            "location": "/#overview", 
            "text": "Distributed Tensorflow and Keras on Spark/BigDL   Data wrangling and analysis using PySpark  Deep learning model development using TensorFlow or Keras  Distributed training/inference on Spark and BigDL  All within a single unified pipeline and in a user-transparent fashion!     High level abstractions and APIs   Transfer learning : customize pretained model for  feature extraction or fine-tuning  autograd : build custom layer/loss using  auto differentiation operations    nnframes : native deep learning support in  Spark DataFrames and ML Pipelines  Model serving : productionize  model serving and inference  using  POJO  APIs     Built-in deep learning models   Object detection API : high-level API and pretrained models (e.g., SSD and Faster-RCNN) for  object detection  Image classification API : high-level API and pretrained models (e.g., VGG, Inception, ResNet, MobileNet, etc.) for  image classification  Text classification API : high-level API and pre-defined models (using CNN, LSTM, etc.) for  text classification  Recommedation API : high-level API and pre-defined models (e.g., Neural Collaborative Filtering, Wide and Deep Learning, etc.) for  recommendation     Reference use cases : a collection of end-to-end  reference use cases  (e.g., anomaly detection, sentiment analysis, fraud detection, image augmentation, object detection, variational autoencoder, etc.)", 
            "title": "Overview"
        }, 
        {
            "location": "/#distributed-tensorflow-and-keras-on-sparkbigdl", 
            "text": "To make it easy to build and productionize the deep learning applications for Big Data, Analytics Zoo provides a unified analytics + AI platform that seamlessly unites Spark, TensorFlow, Keras and BigDL programs into an integrated pipeline (as illustrated below), which can then transparently run on a large-scale Hadoop/Spark clusters for distributed training and inference. (Please see more details  here ).  1.Data wrangling and analysis using PySpark     from zoo import init_nncontext\n   from zoo.pipeline.api.net import TFDataset\n\n   sc = init_nncontext()\n\n   #Each record in the train_rdd consists of a list of NumPy ndrrays\n   train_rdd = sc.parallelize(file_list)\n     .map(lambda x: read_image_and_label(x))\n     .map(lambda image_label: decode_to_ndarrays(image_label))\n\n   #TFDataset represents a distributed set of elements,\n   #in which each element contains one or more Tensorflow Tensor objects. \n   dataset = TFDataset.from_rdd(train_rdd,\n                                names=[ features ,  labels ],\n                                shapes=[[28, 28, 1], [1]],\n                                types=[tf.float32, tf.int32],\n                                batch_size=BATCH_SIZE)  2.Deep learning model development using TensorFlow     import tensorflow as tf\n\n   slim = tf.contrib.slim\n\n   images, labels = dataset.tensors\n   labels = tf.squeeze(labels)\n   with slim.arg_scope(lenet.lenet_arg_scope()):\n        logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)\n\n   loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))  3.Distributed training on Spark and BigDL     from zoo.pipeline.api.net import TFOptimizer\n   from bigdl.optim.optimizer import MaxIteration, Adam, MaxEpoch, TrainSummary\n\n   optimizer = TFOptimizer(loss, Adam(1e-3))\n   optimizer.set_train_summary(TrainSummary( /tmp/az_lenet ,  lenet ))\n   optimizer.optimize(end_trigger=MaxEpoch(5))  4.Alternatively, using Keras APIs for model development and distributed training     from zoo.pipeline.api.keras.models import *\n   from zoo.pipeline.api.keras.layers import *\n\n   model = Sequential()\n   model.add(Reshape((1, 28, 28), input_shape=(28, 28, 1)))\n   model.add(Convolution2D(6, 5, 5, activation= tanh , name= conv1_5x5 ))\n   model.add(MaxPooling2D())\n   model.add(Convolution2D(12, 5, 5, activation= tanh , name= conv2_5x5 ))\n   model.add(MaxPooling2D())\n   model.add(Flatten())\n   model.add(Dense(100, activation= tanh , name= fc1 ))\n   model.add(Dense(class_num, activation= softmax , name= fc2 ))\n\n   model.compile(loss='sparse_categorical_crossentropy',\n                 optimizer='adam')\n   model.fit(train_rdd, batch_size=BATCH_SIZE, nb_epoch=5)", 
            "title": "Distributed Tensorflow and Keras on Spark/BigDL"
        }, 
        {
            "location": "/#high-level-abstractions-and-apis", 
            "text": "Analytics Zoo provides a set of easy-to-use, high level abstractions and APIs that natively transfer learning, autograd and custom layer/loss, Spark DataFrames and ML Pipelines, online model serving, etc. etc.", 
            "title": "High level abstractions and APIs"
        }, 
        {
            "location": "/#transfer-learning", 
            "text": "Using the high level transfer learning APIs, you can easily customize pretrained models for  feature extraction or fine-tuning . (See more details  here )  1.Load an existing model (pretrained in Caffe)     from zoo.pipeline.api.net import *\n   full_model = Net.load_caffe(def_path, model_path)  2.Remove the last few layers     # create a new model by removing layers after pool5/drop_7x7_s1\n   model = full_model.new_graph([ pool5/drop_7x7_s1 ])  3.Freeze the first few layers     # freeze layers from input to pool4/3x3_s2 inclusive\n   model.freeze_up_to([ pool4/3x3_s2 ])  4.Add a few new layers     from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   inputs = Input(name= input , shape=(3, 224, 224))\n   inception = model.to_keras()(inputs)\n   flatten = Flatten()(inception)\n   logits = Dense(2)(flatten)\n   newModel = Model(inputs, logits)", 
            "title": "Transfer learning"
        }, 
        {
            "location": "/#autograd", 
            "text": "autograd  provides automatic differentiation for math operations, so that you can easily build your own  custom loss and layer  (in both Python and Scala), as illustracted below. (See more details  here )  1.Define model using Keras-style API and  autograd       import zoo.pipeline.api.autograd as A\n   from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n\n   input = Input(shape=[2, 20])\n   features = TimeDistributed(layer=Dense(30))(input)\n   f1 = features.index_select(1, 0)\n   f2 = features.index_select(1, 1)\n   diff = A.abs(f1 - f2)\n   model = Model(input, diff)  2.Optionally define custom loss function using  autograd     def mean_absolute_error(y_true, y_pred):\n       return mean(abs(y_true - y_pred), axis=1)  3.Train model with  custom loss function     model.compile(optimizer=SGD(), loss=mean_absolute_error)\n   model.fit(x=..., y=...)", 
            "title": "autograd"
        }, 
        {
            "location": "/#nnframes", 
            "text": "nnframes  provides  native deep learning support in Spark DataFrames and ML Pipelines , so that you can easily build complex deep learning pipelines in just a few lines, as illustrated below. (See more details  here )  1.Initialize  NNContext  and load images into  DataFrames  using  NNImageReader     from zoo.common.nncontext import *\n   from zoo.pipeline.nnframes import *\n   from zoo.feature.image import *\n   sc = init_nncontext()\n   imageDF = NNImageReader.readImages(image_path, sc)  2.Process loaded data using  DataFrames transformations     getName = udf(lambda row: ...)\n   getLabel = udf(lambda name: ...)\n   df = imageDF.withColumn( name , getName(col( image ))).withColumn( label , getLabel(col('name')))  3.Processing image using built-in  feature engineering operations     transformer = RowToImageFeature() -  ImageResize(64, 64) -  ImageChannelNormalize(123.0, 117.0, 104.0) \\\n                 -  ImageMatToTensor() -  ImageFeatureToTensor())  4.Define model using  Keras-style APIs     from zoo.pipeline.api.keras.layers import *\n   from zoo.pipeline.api.keras.models import *\n   model = Sequential().add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1, 28, 28))) \\\n                   .add(MaxPooling2D(pool_size=(2, 2))).add(Flatten()).add(Dense(10, activation='softmax')))  5.Train model using  Spark ML Pipelines     classifier = NNClassifier(model, CrossEntropyCriterion(),transformer).setLearningRate(0.003) \\\n                   .setBatchSize(40).setMaxEpoch(1).setFeaturesCol( image ).setCachingSample(False)\n   nnModel = classifier.fit(df)", 
            "title": "nnframes"
        }, 
        {
            "location": "/#model-serving", 
            "text": "Using the  POJO  model serving API, you can productionize model serving and infernece in any Java based frameworks (e.g.,  Spring Framework , Apache  Storm ,  Kafka  or  Flink , etc.), as illustrated below:  import com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;\nimport com.intel.analytics.zoo.pipeline.inference.JTensor;\n\npublic class TextClassificationModel extends AbstractInferenceModel {\n    public TextClassificationModel() {\n        super();\n    }\n}\n\nTextClassificationModel model = new TextClassificationModel();\nmodel.load(modelPath, weightPath);\n\nList JTensor  inputs = preprocess(...);\nList List JTensor  result = model.predict(inputs);\n...", 
            "title": "Model Serving"
        }, 
        {
            "location": "/#built-in-deep-learning-models", 
            "text": "Analytics Zoo provides several built-in deep learning models that you can use for a variety of problem types, such as  object detection ,  image classification ,  text classification ,  recommendation , etc.", 
            "title": "Built-in deep learning models"
        }, 
        {
            "location": "/#object-detection-api", 
            "text": "Using  Analytics Zoo Object Detection API  (including a set of pretrained detection models such as SSD and Faster-RCNN), you can easily build your object detection applications (e.g., localizing and identifying multiple objects in images and videos), as illustrated below. (See more details  here )  1.Download object detection models in Analytics Zoo  You can download a collection of detection models (pretrained on the PSCAL VOC dataset and COCO dataset) from  detection model zoo .  2.Use  Object Detection API  for off-the-shell inference     from zoo.models.image.objectdetection import *\n   model = ObjectDetector.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)", 
            "title": "Object detection API"
        }, 
        {
            "location": "/#image-classification-api", 
            "text": "Using  Analytics Zoo Image Classification API  (including a set of pretrained detection models such as VGG, Inception, ResNet, MobileNet,  etc.), you can easily build your image classification applications, as illustrated below. (See more details  here )  1.Download image classification models in Analytics Zoo  You can download a collection of image classification models (pretrained on the ImageNet dataset) from  image classification model zoo .  2.Use  Image classification API  for off-the-shell inference     from zoo.models.image.imageclassification import *\n   model = ImageClassifier.load_model(model_path)\n   image_set = ImageSet.read(img_path, sc)\n   output = model.predict_image_set(image_set)", 
            "title": "Image classification API"
        }, 
        {
            "location": "/#text-classification-api", 
            "text": "Analytics Zoo Text Classification API  provides a set of pre-defined models (using CNN, LSTM, etc.) for text classifications. (See more details  here )", 
            "title": "Text classification API"
        }, 
        {
            "location": "/#recommendation-api", 
            "text": "Analytics Zoo Recommendation API  provides a set of pre-defined models (such as Neural Collaborative Filtering, Wide and Deep Learning, etc.) for recommendations. (See more details  here )", 
            "title": "Recommendation API"
        }, 
        {
            "location": "/#reference-use-cases", 
            "text": "Analytics Zoo provides a collection of end-to-end reference use cases, including  time series anomaly detection ,  sentiment analysis ,  fraud detection ,  image similarity , etc. (See more details  here )", 
            "title": "Reference use cases"
        }, 
        {
            "location": "/release-download/", 
            "text": "Release 0.3.0 nightly build\n\n\n\n\n\n\n\n\n\n\nBigDL 0.6.0\n\n\nBigDL 0.5.0\n\n\n\n\n\n\n\n\n\n\nSpark 1.6.2\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.2.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\n\n\nRelease 0.2.0\n\n\n\n\n\n\n\n\n\n\nBigDL 0.6.0\n\n\nBigDL 0.5.0\n\n\n\n\n\n\n\n\n\n\nSpark 1.6.2\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.1\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\nSpark 2.2.0\n\n\ndownload\n\n\ndownload\n\n\n\n\n\n\n\n\n\n\nRelease 0.1.0\n\n\n\n\n\n\n\n\n\n\nDownload Links\n\n\n\n\n\n\n\n\n\n\nSpark 1.6.0\n\n\ndownload\n\n\n\n\n\n\nSpark 2.1.0\n\n\ndownload\n\n\n\n\n\n\nSpark 2.2.0\n\n\ndownload", 
            "title": "Download"
        }, 
        {
            "location": "/release-download/#release-030-nightly-build", 
            "text": "BigDL 0.6.0  BigDL 0.5.0      Spark 1.6.2  download  download    Spark 2.1.1  download  download    Spark 2.2.0  download  download", 
            "title": "Release 0.3.0 nightly build"
        }, 
        {
            "location": "/release-download/#release-020", 
            "text": "BigDL 0.6.0  BigDL 0.5.0      Spark 1.6.2  download  download    Spark 2.1.1  download  download    Spark 2.2.0  download  download", 
            "title": "Release 0.2.0"
        }, 
        {
            "location": "/release-download/#release-010", 
            "text": "Download Links      Spark 1.6.0  download    Spark 2.1.0  download    Spark 2.2.0  download", 
            "title": "Release 0.1.0"
        }, 
        {
            "location": "/release-docs/", 
            "text": "Release 0.2.0\n\n\nAnalytics-Zoo 0.2.0 Docs\n\n\nRelease 0.1.0\n\n\nAnalytics-Zoo 0.1.0 Docs", 
            "title": "Documentation"
        }, 
        {
            "location": "/release-docs/#release-020", 
            "text": "Analytics-Zoo 0.2.0 Docs", 
            "title": "Release 0.2.0"
        }, 
        {
            "location": "/release-docs/#release-010", 
            "text": "Analytics-Zoo 0.1.0 Docs", 
            "title": "Release 0.1.0"
        }, 
        {
            "location": "/PythonUserGuide/install/", 
            "text": "For Python users, Analytics Zoo can be installed either \nfrom pip\n or \nwithout pip\n.\n\n\nNOTE\n: Only \nPython 2.7\n, \nPython 3.5\n and \nPython 3.6\n are supported for now.\n\n\n\n\nInstall from pip\n\n\nYou can use the following command to install the latest release version of \nanalytics-zoo\n via pip easily:\n\n\npip install analytics-zoo     # for Python 2.7\npip3 install analytics-zoo    # for Python 3.5 and Python 3.6\n\n\n\n\n\n\nNote that you might need to add \nsudo\n if you don't have the permission for installation.\n\n\n\n\nImportant:\n\n\n\n\n\n\nInstalling analytics-zoo from pip will automatically install \npyspark\n. To avoid possible conflicts, you are highly recommended to \nunset \nSPARK_HOME\n if it exists in your environment.\n\n\n\n\n\n\nPlease always first call \ninit_nncontext()\n at the very beginning of your code after pip install. This will create a SparkContext with optimized performance configuration and initialize the BigDL engine.\n\n\n\n\n\n\nfrom zoo.common.nncontext import *\nsc = init_nncontext()\n\n\n\n\nRemarks:\n\n\n\n\nWe've tested this package with pip 9.0.1. \npip install --upgrade pip\n if necessary.\n\n\nPip install supports \nMac\n and \nLinux\n platforms.\n\n\nPip install only supports \nlocal\n mode. Cluster mode might be supported in the future. For those who want to use Analytics Zoo in cluster mode, please try to \ninstall without pip\n.\n\n\nYou need to install Java \n= JDK8\n before running Analytics Zoo, which is required by \npyspark\n.\n\n\npyspark\n, \nbigdl==0.6.0\n and their dependencies will automatically be installed if they haven't been detected in the current Python environment.\n\n\n\n\n\n\nInstall without pip\n\n\nIf you choose to install Analytics Zoo without pip, you need to prepare Spark and install necessary Python dependencies.\n\n\nSteps:\n\n\n\n\n\n\nDownload Spark\n\n\n\n\nNote that \nPython 3.6\n is only compatible with Spark 1.6.4, 2.0.3, 2.1.1 and \n=2.2.0. See \nthis issue\n for more discussion.\n\n\n\n\n\n\n\n\nYou are recommended to download Analytics Zoo prebuilt release and nightly build package from the \nRelease Page\n and extract it.\nAlternatively, you can also build the Analytics Zoo from \nsource\n.\n\n\n\n\n\n\nInstall Python dependencies. Analytics Zoo only depends on \nnumpy\n and \nsix\n for now.\n\n\n\n\n\n\nFor Spark standalone cluster\n\n\n\n\nRemark\n: If you're running in cluster mode, you need to install Python dependencies on both client and each worker node.\n\n\nInstall numpy: \n\nsudo apt-get install python-numpy\n (Ubuntu)\n\n\nInstall six: \n\nsudo apt-get install python-six\n (Ubuntu)\n\n\n\n\nFor Yarn cluster\n\n\nYou can run Analytics Zoo Python programs on Yarn clusters without changes to the cluster (i.e., no need to pre-install any Python dependency).\n\n\nYou can first package all the required dependencies into a virtual environment on the local node (where you will run the spark-submit command),\nand then directly use spark-submit to run the Analytics Zoo Python program on the Yarn cluster using that virtual environment.\n\n\nFollow the steps below to create the virtual environment: \n\n\n\n\nMake sure you already installed such libraries (python-setuptools, python-dev, gcc, make, zip, pip) for creating the virtual environment. If not, please install them first.\nOn Ubuntu, you can run these commands to install:\n\n\n\n\napt-get update\napt-get install -y python-setuptools python-dev\napt-get install -y gcc make\napt-get install -y zip\neasy_install pip\n\n\n\n\n\n\n\n\nCreate the virtualenv package for dependencies.\n\n\n\n\n\n\nUnder $ANALYTICS_ZOO_HOME (the dist directory under the Analytics Zoo project), you can find \nbin/python_package.sh\n. Run this script to create the dependency virtual environment according to the dependencies listed in \nrequirements.txt\n. You can add your own dependencies into this file if you wish. The current requirements only contain those needed for running Analytics Zoo Python examples and models.\n\n\n\n\n\n\nAfter running this script, there will be \nvenv.zip\n and \nvenv\n directory generated in current directory. You can use them to submit your Python jobs. Please refer to \nhere\n for the commands to submit an Analytics Zoo Python job with the created virtual environment in Yarn cluster.\n\n\n\n\n\n\n\n\n\n\nFAQ\n\n\nIn case you encounter the following errors when you create the environment package using the above command:\n\n\n\n\nvirtualenv ImportError: No module named urllib3\n\n\nUsing python in anaconda to create virtualenv may cause this problem. Try using python default in your system instead of installing virtualenv in anaconda.\n\n\n\n\n\n\nAttributeError: 'module' object has no attribute 'sslwrap'\n\n\nTry upgrading \ngevent\n with \npip install --upgrade gevent\n.", 
            "title": "Install"
        }, 
        {
            "location": "/PythonUserGuide/install/#install-from-pip", 
            "text": "You can use the following command to install the latest release version of  analytics-zoo  via pip easily:  pip install analytics-zoo     # for Python 2.7\npip3 install analytics-zoo    # for Python 3.5 and Python 3.6   Note that you might need to add  sudo  if you don't have the permission for installation.   Important:    Installing analytics-zoo from pip will automatically install  pyspark . To avoid possible conflicts, you are highly recommended to  unset  SPARK_HOME  if it exists in your environment.    Please always first call  init_nncontext()  at the very beginning of your code after pip install. This will create a SparkContext with optimized performance configuration and initialize the BigDL engine.    from zoo.common.nncontext import *\nsc = init_nncontext()  Remarks:   We've tested this package with pip 9.0.1.  pip install --upgrade pip  if necessary.  Pip install supports  Mac  and  Linux  platforms.  Pip install only supports  local  mode. Cluster mode might be supported in the future. For those who want to use Analytics Zoo in cluster mode, please try to  install without pip .  You need to install Java  = JDK8  before running Analytics Zoo, which is required by  pyspark .  pyspark ,  bigdl==0.6.0  and their dependencies will automatically be installed if they haven't been detected in the current Python environment.", 
            "title": "Install from pip"
        }, 
        {
            "location": "/PythonUserGuide/install/#install-without-pip", 
            "text": "If you choose to install Analytics Zoo without pip, you need to prepare Spark and install necessary Python dependencies.  Steps:    Download Spark   Note that  Python 3.6  is only compatible with Spark 1.6.4, 2.0.3, 2.1.1 and  =2.2.0. See  this issue  for more discussion.     You are recommended to download Analytics Zoo prebuilt release and nightly build package from the  Release Page  and extract it.\nAlternatively, you can also build the Analytics Zoo from  source .    Install Python dependencies. Analytics Zoo only depends on  numpy  and  six  for now.", 
            "title": "Install without pip"
        }, 
        {
            "location": "/PythonUserGuide/install/#for-spark-standalone-cluster", 
            "text": "Remark : If you're running in cluster mode, you need to install Python dependencies on both client and each worker node.  Install numpy:  sudo apt-get install python-numpy  (Ubuntu)  Install six:  sudo apt-get install python-six  (Ubuntu)", 
            "title": "For Spark standalone cluster"
        }, 
        {
            "location": "/PythonUserGuide/install/#for-yarn-cluster", 
            "text": "You can run Analytics Zoo Python programs on Yarn clusters without changes to the cluster (i.e., no need to pre-install any Python dependency).  You can first package all the required dependencies into a virtual environment on the local node (where you will run the spark-submit command),\nand then directly use spark-submit to run the Analytics Zoo Python program on the Yarn cluster using that virtual environment.  Follow the steps below to create the virtual environment:    Make sure you already installed such libraries (python-setuptools, python-dev, gcc, make, zip, pip) for creating the virtual environment. If not, please install them first.\nOn Ubuntu, you can run these commands to install:   apt-get update\napt-get install -y python-setuptools python-dev\napt-get install -y gcc make\napt-get install -y zip\neasy_install pip    Create the virtualenv package for dependencies.    Under $ANALYTICS_ZOO_HOME (the dist directory under the Analytics Zoo project), you can find  bin/python_package.sh . Run this script to create the dependency virtual environment according to the dependencies listed in  requirements.txt . You can add your own dependencies into this file if you wish. The current requirements only contain those needed for running Analytics Zoo Python examples and models.    After running this script, there will be  venv.zip  and  venv  directory generated in current directory. You can use them to submit your Python jobs. Please refer to  here  for the commands to submit an Analytics Zoo Python job with the created virtual environment in Yarn cluster.      FAQ  In case you encounter the following errors when you create the environment package using the above command:   virtualenv ImportError: No module named urllib3  Using python in anaconda to create virtualenv may cause this problem. Try using python default in your system instead of installing virtualenv in anaconda.    AttributeError: 'module' object has no attribute 'sslwrap'  Try upgrading  gevent  with  pip install --upgrade gevent .", 
            "title": "For Yarn cluster"
        }, 
        {
            "location": "/PythonUserGuide/run/", 
            "text": "You need to first \ninstall\n analytics-zoo, either \nfrom pip\n or \nwithout pip\n.\n\n\nNOTE\n: Only \nPython 2.7\n, \nPython 3.5\n and \nPython 3.6\n are supported for now.\n\n\n\n\nRun after pip install\n\n\nImportant:\n\n\n\n\n\n\nInstalling analytics-zoo from pip will automatically install \npyspark\n. To avoid possible conflicts, you are highly recommended to \nunset \nSPARK_HOME\n if it exists in your environment.\n\n\n\n\n\n\nPlease always first call \ninit_nncontext()\n at the very beginning of your code after pip install. This will create a SparkContext with optimized performance configuration and initialize the BigDL engine.\n\n\n\n\n\n\nfrom zoo.common.nncontext import *\nsc = init_nncontext()\n\n\n\n\nUse an Interactive Shell\n\n\n\n\nType \npython\n in the command line to start a REPL.\n\n\nTry to run the \nexample code\n to verify the installation.\n\n\n\n\nUse Jupyter Notebook\n\n\n\n\nStart jupyter notebook as you normally do, e.g.\n\n\n\n\njupyter notebook --notebook-dir=./ --ip=* --no-browser\n\n\n\n\n\n\nTry to run the \nexample code\n to verify the installation.\n\n\n\n\nConfigurations\n\n\n\n\nIncrease memory\n\n\n\n\nexport SPARK_DRIVER_MEMORY=20g\n\n\n\n\n\n\nAdd extra jars or python packages\n\n\n\n\n Set the environment variables \nBIGDL_JARS\n and \nBIGDL_PACKAGES\n \nBEFORE\n creating \nSparkContext\n:\n\n\nexport BIGDL_JARS=...\nexport BIGDL_PACKAGES=...\n\n\n\n\n\n\nRun without pip install\n\n\n\n\nNote that \nPython 3.6\n is only compatible with Spark 1.6.4, 2.0.3, 2.1.1 and \n=2.2.0. See \nthis issue\n for more discussion.\n\n\n\n\nSet SPARK_HOME and ANALYTICS_ZOO_HOME\n\n\n\n\nIf you download Analytics Zoo from the \nRelease Page\n:\n\n\n\n\nexport SPARK_HOME=the root directory of Spark\nexport ANALYTICS_ZOO_HOME=the path where you extract the analytics-zoo package\n\n\n\n\n\n\nIf you build Analytics Zoo by yourself:\n\n\n\n\nexport SPARK_HOME=the root directory of Spark\nexport ANALYTICS_ZOO_HOME=the dist directory of Analytics Zoo\n\n\n\n\nUpdate spark-analytics-zoo.conf (Optional)\n\n\nIf you have some customized properties in some files, which will be used with the \n--properties-file\n option\nin \nspark-submit/pyspark\n, you can add these customized properties into ${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf.\n\n\n\n\nRun with pyspark\n\n\n${ANALYTICS_ZOO_HOME}/bin/pyspark-with-zoo.sh --master local[*]\n\n\n\n\n\n\n--master\n set the master URL to connect to\n\n\n--jars\n if there are extra jars needed.\n\n\n--py-files\n if there are extra python packages needed.\n\n\n\n\nYou can also specify other options available for pyspark in the above command if needed.\n\n\nTry to run the \nexample code\n for verification.\n\n\n\n\nRun with spark-submit\n\n\nAn Analytics Zoo Python program runs as a standard pyspark program, which requires all Python dependencies\n(e.g., numpy) used by the program to be installed on each node in the Spark cluster. You can try\nrunning the Analytics Zoo \nObject Detection Python example\n\nas follows:\n\n\n${ANALTICS_ZOO_HOME}/bin/spark-submit-with-zoo.sh --master local[*] predict.py model_path image_path output_path\n\n\n\n\n\n\nRun with Jupyter Notebook\n\n\nWith the full Python API support in Analytics Zoo, users can use our package together with powerful notebooks\n(such as Jupyter Notebook) in a distributed fashion across the cluster, combining Python libraries,\nSpark SQL/DataFrames and MLlib, deep learning models in Analytics Zoo, as well as interactive\nvisualization tools.\n\n\nPrerequisites\n: Install all the necessary libraries on the local node where you will run Jupyter, e.g., \n\n\nsudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud\n\n\n\n\nLaunch the Jupyter Notebook as follows:\n\n\n${ANALYTICS_ZOO_HOME}/bin/jupyter-with-zoo.sh --master local[*]\n\n\n\n\n\n\n--master\n set the master URL to connect to\n\n\n--jars\n if there are extra jars needed.\n\n\n--py-files\n if there are extra python packages needed.\n\n\n\n\nYou can also specify other options available for pyspark in the above command if needed.\n\n\nAfter successfully launching Jupyter, you will be able to navigate to the notebook dashboard using\nyour browser. You can find the exact URL in the console output when you started Jupyter; by default,\nthe dashboard URL is http://your_node:8888/\n\n\nTry to run the \nexample code\n for verification.\n\n\n\n\nRun with virtual environment on Yarn\n\n\nIf you have already created Analytics Zoo dependency virtual environment according to Yarn cluster guide \nhere\n,\nyou can run Python programs using Analytics Zoo using the following command.\n\n\nHere we use Analytics Zoo \nObject Detection Python example\n for illustration.\n\n\n\n\nYarn cluster mode\n\n\n\n\n    SPARK_HOME=the root directory of Spark\n    ANALYTICS_ZOO_ROOT=the root directory of the Analytics Zoo project\n    ANALYTICS_ZOO_HOME=$ANALYTICS_ZOO_ROOT/dist\n    ANALYTICS_ZOO_PY_ZIP=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-python-api.zip\n    ANALYTICS_ZOO_JAR=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-jar-with-dependencies.jar\n    ANALYTICS_ZOO_CONF=${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf\n    PYTHONPATH=${ANALYTICS_ZOO_PY_ZIP}:$PYTHONPATH\n    VENV_HOME=the parent directory of venv.zip and venv folder\n\n    PYSPARK_PYTHON=${VENV_HOME}/venv.zip/venv/bin/python ${SPARK_HOME}/bin/spark-submit \\\n    --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=${VENV_HOME}/venv.zip/venv/bin/python \\\n    --master yarn-cluster \\\n    --executor-memory 10g \\\n    --driver-memory 10g \\\n    --executor-cores 8 \\\n    --num-executors 2 \\\n    --properties-file ${ANALYTICS_ZOO_CONF} \\\n    --jars ${ANALYTICS_ZOO_JAR} \\\n    --py-files ${ANALYTICS_ZOO_PY_ZIP} \\\n    --archives ${VENV_HOME}/venv.zip \\\n    --conf spark.driver.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    --conf spark.executor.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    ${ANALYTICS_ZOO_ROOT}/pyzoo/zoo/examples/objectdetection/predict.py model_path image_path output_path\n\n\n\n\n\n\nYarn client mode\n\n\n\n\n    SPARK_HOME=the root directory of Spark\n    ANALYTICS_ZOO_ROOT=the root directory of the Analytics Zoo project\n    ANALYTICS_ZOO_HOME=$ANALYTICS_ZOO_ROOT/dist\n    ANALYTICS_ZOO_PY_ZIP=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-python-api.zip\n    ANALYTICS_ZOO_JAR=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-jar-with-dependencies.jar\n    ANALYTICS_ZOO_CONF=${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf\n    PYTHONPATH=${ANALYTICS_ZOO_PY_ZIP}:$PYTHONPATH\n    VENV_HOME=the parent directory of venv.zip and venv folder\n\n    PYSPARK_DRIVER_PYTHON=${VENV_HOME}/venv/bin/python PYSPARK_PYTHON=${VENV_HOME}/venv.zip/venv/bin/python ${SPARK_HOME}/bin/spark-submit \\\n    --master yarn \\\n    --deploy-mode client \\\n    --executor-memory 10g \\\n    --driver-memory 10g \\\n    --executor-cores 16 \\\n    --num-executors 2 \\\n    --properties-file ${ANALYTICS_ZOO_CONF} \\\n    --jars ${ANALYTICS_ZOO_JAR} \\\n    --py-files ${ANALYTICS_ZOO_PY_ZIP} \\\n    --archives ${VENV_HOME}/venv.zip \\\n    --conf spark.driver.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    --conf spark.executor.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    ${ANALYTICS_ZOO_ROOT}/pyzoo/zoo/examples/objectdetection/predict.py model_path image_path output_path\n\n\n\n\n\n\nExample code\n\n\nTo verify if Analytics Zoo can run successfully, run the following simple code:\n\n\nimport zoo.version\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.keras.layers import *\n\n# Get the current Analytics Zoo version\nzoo.version.__version__\n# Create a SparkContext and initialize the BigDL engine.\nsc = init_nncontext()\n# Create a Sequential model containing a Dense layer.\nmodel = Sequential()\nmodel.add(Dense(8, input_shape=(10, )))", 
            "title": "Run"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-after-pip-install", 
            "text": "Important:    Installing analytics-zoo from pip will automatically install  pyspark . To avoid possible conflicts, you are highly recommended to  unset  SPARK_HOME  if it exists in your environment.    Please always first call  init_nncontext()  at the very beginning of your code after pip install. This will create a SparkContext with optimized performance configuration and initialize the BigDL engine.    from zoo.common.nncontext import *\nsc = init_nncontext()  Use an Interactive Shell   Type  python  in the command line to start a REPL.  Try to run the  example code  to verify the installation.   Use Jupyter Notebook   Start jupyter notebook as you normally do, e.g.   jupyter notebook --notebook-dir=./ --ip=* --no-browser   Try to run the  example code  to verify the installation.   Configurations   Increase memory   export SPARK_DRIVER_MEMORY=20g   Add extra jars or python packages    Set the environment variables  BIGDL_JARS  and  BIGDL_PACKAGES   BEFORE  creating  SparkContext :  export BIGDL_JARS=...\nexport BIGDL_PACKAGES=...", 
            "title": "Run after pip install"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-without-pip-install", 
            "text": "Note that  Python 3.6  is only compatible with Spark 1.6.4, 2.0.3, 2.1.1 and  =2.2.0. See  this issue  for more discussion.   Set SPARK_HOME and ANALYTICS_ZOO_HOME   If you download Analytics Zoo from the  Release Page :   export SPARK_HOME=the root directory of Spark\nexport ANALYTICS_ZOO_HOME=the path where you extract the analytics-zoo package   If you build Analytics Zoo by yourself:   export SPARK_HOME=the root directory of Spark\nexport ANALYTICS_ZOO_HOME=the dist directory of Analytics Zoo  Update spark-analytics-zoo.conf (Optional)  If you have some customized properties in some files, which will be used with the  --properties-file  option\nin  spark-submit/pyspark , you can add these customized properties into ${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf.", 
            "title": "Run without pip install"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-with-pyspark", 
            "text": "${ANALYTICS_ZOO_HOME}/bin/pyspark-with-zoo.sh --master local[*]   --master  set the master URL to connect to  --jars  if there are extra jars needed.  --py-files  if there are extra python packages needed.   You can also specify other options available for pyspark in the above command if needed.  Try to run the  example code  for verification.", 
            "title": "Run with pyspark"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-with-spark-submit", 
            "text": "An Analytics Zoo Python program runs as a standard pyspark program, which requires all Python dependencies\n(e.g., numpy) used by the program to be installed on each node in the Spark cluster. You can try\nrunning the Analytics Zoo  Object Detection Python example \nas follows:  ${ANALTICS_ZOO_HOME}/bin/spark-submit-with-zoo.sh --master local[*] predict.py model_path image_path output_path", 
            "title": "Run with spark-submit"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-with-jupyter-notebook", 
            "text": "With the full Python API support in Analytics Zoo, users can use our package together with powerful notebooks\n(such as Jupyter Notebook) in a distributed fashion across the cluster, combining Python libraries,\nSpark SQL/DataFrames and MLlib, deep learning models in Analytics Zoo, as well as interactive\nvisualization tools.  Prerequisites : Install all the necessary libraries on the local node where you will run Jupyter, e.g.,   sudo apt install python\nsudo apt install python-pip\nsudo pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud  Launch the Jupyter Notebook as follows:  ${ANALYTICS_ZOO_HOME}/bin/jupyter-with-zoo.sh --master local[*]   --master  set the master URL to connect to  --jars  if there are extra jars needed.  --py-files  if there are extra python packages needed.   You can also specify other options available for pyspark in the above command if needed.  After successfully launching Jupyter, you will be able to navigate to the notebook dashboard using\nyour browser. You can find the exact URL in the console output when you started Jupyter; by default,\nthe dashboard URL is http://your_node:8888/  Try to run the  example code  for verification.", 
            "title": "Run with Jupyter Notebook"
        }, 
        {
            "location": "/PythonUserGuide/run/#run-with-virtual-environment-on-yarn", 
            "text": "If you have already created Analytics Zoo dependency virtual environment according to Yarn cluster guide  here ,\nyou can run Python programs using Analytics Zoo using the following command.  Here we use Analytics Zoo  Object Detection Python example  for illustration.   Yarn cluster mode       SPARK_HOME=the root directory of Spark\n    ANALYTICS_ZOO_ROOT=the root directory of the Analytics Zoo project\n    ANALYTICS_ZOO_HOME=$ANALYTICS_ZOO_ROOT/dist\n    ANALYTICS_ZOO_PY_ZIP=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-python-api.zip\n    ANALYTICS_ZOO_JAR=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-jar-with-dependencies.jar\n    ANALYTICS_ZOO_CONF=${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf\n    PYTHONPATH=${ANALYTICS_ZOO_PY_ZIP}:$PYTHONPATH\n    VENV_HOME=the parent directory of venv.zip and venv folder\n\n    PYSPARK_PYTHON=${VENV_HOME}/venv.zip/venv/bin/python ${SPARK_HOME}/bin/spark-submit \\\n    --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=${VENV_HOME}/venv.zip/venv/bin/python \\\n    --master yarn-cluster \\\n    --executor-memory 10g \\\n    --driver-memory 10g \\\n    --executor-cores 8 \\\n    --num-executors 2 \\\n    --properties-file ${ANALYTICS_ZOO_CONF} \\\n    --jars ${ANALYTICS_ZOO_JAR} \\\n    --py-files ${ANALYTICS_ZOO_PY_ZIP} \\\n    --archives ${VENV_HOME}/venv.zip \\\n    --conf spark.driver.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    --conf spark.executor.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    ${ANALYTICS_ZOO_ROOT}/pyzoo/zoo/examples/objectdetection/predict.py model_path image_path output_path   Yarn client mode       SPARK_HOME=the root directory of Spark\n    ANALYTICS_ZOO_ROOT=the root directory of the Analytics Zoo project\n    ANALYTICS_ZOO_HOME=$ANALYTICS_ZOO_ROOT/dist\n    ANALYTICS_ZOO_PY_ZIP=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-python-api.zip\n    ANALYTICS_ZOO_JAR=${ANALYTICS_ZOO_HOME}/lib/analytics-zoo-VERSION-jar-with-dependencies.jar\n    ANALYTICS_ZOO_CONF=${ANALYTICS_ZOO_HOME}/conf/spark-analytics-zoo.conf\n    PYTHONPATH=${ANALYTICS_ZOO_PY_ZIP}:$PYTHONPATH\n    VENV_HOME=the parent directory of venv.zip and venv folder\n\n    PYSPARK_DRIVER_PYTHON=${VENV_HOME}/venv/bin/python PYSPARK_PYTHON=${VENV_HOME}/venv.zip/venv/bin/python ${SPARK_HOME}/bin/spark-submit \\\n    --master yarn \\\n    --deploy-mode client \\\n    --executor-memory 10g \\\n    --driver-memory 10g \\\n    --executor-cores 16 \\\n    --num-executors 2 \\\n    --properties-file ${ANALYTICS_ZOO_CONF} \\\n    --jars ${ANALYTICS_ZOO_JAR} \\\n    --py-files ${ANALYTICS_ZOO_PY_ZIP} \\\n    --archives ${VENV_HOME}/venv.zip \\\n    --conf spark.driver.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    --conf spark.executor.extraClassPath=${ANALYTICS_ZOO_JAR} \\\n    ${ANALYTICS_ZOO_ROOT}/pyzoo/zoo/examples/objectdetection/predict.py model_path image_path output_path", 
            "title": "Run with virtual environment on Yarn"
        }, 
        {
            "location": "/PythonUserGuide/run/#example-code", 
            "text": "To verify if Analytics Zoo can run successfully, run the following simple code:  import zoo.version\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.keras.layers import *\n\n# Get the current Analytics Zoo version\nzoo.version.__version__\n# Create a SparkContext and initialize the BigDL engine.\nsc = init_nncontext()\n# Create a Sequential model containing a Dense layer.\nmodel = Sequential()\nmodel.add(Dense(8, input_shape=(10, )))", 
            "title": "Example code"
        }, 
        {
            "location": "/PythonUserGuide/examples/", 
            "text": "Analytics Zoo provides plenty of examples and notebooks ready for re-use as listed below.\n\n\n\n\nImage Classification\n: This example illustrates how to classify images with a pre-trained model.\n\n\nObject Detection\n: This example illustrates how to detect objects in images with a pre-trained model.\n\n\nRecommendation\n: There are two Python notebooks for recommendation models, including wide and deep(WND) model and Neural network-based Collaborative Filtering(NCF) model.\n\n\nText Classification\n: This example uses pre-trained GloVe embeddings to convert words to vectors and trains a CNN, LSTM or GRU TextClassifier model on 20 Newsgroup dataset.\n\n\nDataFrame\n: There are three examples to show how to perform transfer learning and model inference using pre-trained Inception v1 model with DataFrame-based API.\n\n\nTFNet\n: This example illustrates how to use a pre-trained TensorFlow object detection model to make inferences.\n\n\nSee \nhere\n for more notebooks on user applications and demos.", 
            "title": "Examples"
        }, 
        {
            "location": "/PythonUserGuide/python-faq/", 
            "text": "This page lists solutions to some common questions.\n\n\n\n\n\n\nImportError\n: from zoo.pipeline.api.keras.layers import *\n\n\n\n\nCheck if the path is pointing to python-api.zip: \n--py-files ${ANALYTICS_ZOO_PY_ZIP}\n\n\nCheck if the path is pointing to python-api.zip:\n\n\n\n\nexport PYTHONPATH=${ANALYTICS_ZOO_PY_ZIP}:$PYTHONPATH\n\n\n\n\n\n\nPython in worker has a different version 2.7 than that in driver 3.5\n\n\n\n\nexport PYSPARK_PYTHON=/usr/local/bin/python3.5\n  This path should be valid on every worker node.\n\n\nexport PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3.5\n  This path should be valid on every driver node.\n\n\n\n\n\n\n\n\nTypeError\n: 'JavaPackage' object is not callable\n\n\n\n\nCheck if every path within the launch script is valid especially the path that ends with jar.\n\n\nIf there are extra jars involved, check if the Spark version Analytics Zoo is built and the Spark version the extra jar is built are compatible.\n\n\n\n\n\n\n\n\njava.lang.\nNoSuchMethodError\n:XXX or \nPy4JError\n: ofFloat does not exist in the JVM\n\n\n\n\nCheck if the Spark version matches, i.e check if you are using Spark 2.x but the underneath Analytics Zoo is compiled with Spark 1.6.\n\n\nIf there are extra jars involved, also check if the Spark version matches.", 
            "title": "FAQ"
        }, 
        {
            "location": "/ScalaUserGuide/install/", 
            "text": "Download a pre-built library\n\n\nYou can download the Analytics Zoo release and nightly build from the \nRelease Page\n\n\n\n\nLink with a release version\n\n\nCurrently, Analytics Zoo releases are hosted on maven central; here's an example to add the Analytics Zoo dependency to your own project:\n\n\ndependency\n\n    \ngroupId\ncom.intel.analytics.zoo\n/groupId\n\n    \nartifactId\nanalytics-zoo-bigdl_0.6.0-[spark_1.6.2|spark_2.1.1|spark_2.2.0|spark_2.3.1]\n/artifactId\n\n    \nversion\n${ANALYTICS_ZOO_VERSION}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT developers can use\n\n\nlibraryDependencies += \ncom.intel.analytics.zoo\n % \nanalytics-zoo-bigdl_0.6.0-[spark_1.6.2|spark_2.1.1|spark_2.2.0|spark_2.3.1]\n % \n${ANALYTICS_ZOO_VERSION}\n\n\n\n\n\nRemarks:\n\n\n\n\nPlease choose the available suffix above according to your Spark platform and the BigDL version you want to use.\n\n\nYou don't need to add the BigDL dependency to your project as it has already been packaged within Analytics Zoo.\n\n\nYou can find the option \n${ANALYTICS_ZOO_VERSION}\n from the \nRelease Page\n.\n\n\n\n\n\n\nLink with a development version\n\n\nCurrently, Analytics Zoo development version is hosted on \nSonaType\n.\n\n\nTo link your application with the latest Analytics Zoo development version, you should add some dependencies like \nLinking with Analytics Zoo releases\n, but set \n${ANALYTICS_ZOO_VERSION}\n to latest version, and add below repository to your pom.xml.\n\n\nrepository\n\n    \nid\nsonatype\n/id\n\n    \nname\nsonatype repository\n/name\n\n    \nurl\nhttps://oss.sonatype.org/content/groups/public/\n/url\n\n    \nreleases\n\n        \nenabled\ntrue\n/enabled\n\n    \n/releases\n\n    \nsnapshots\n\n        \nenabled\ntrue\n/enabled\n\n    \n/snapshots\n\n\n/repository\n\n\n\n\n\nSBT developers can use\n\n\nresolvers += \nossrh repository\n at \nhttps://oss.sonatype.org/content/repositories/snapshots/\n\n\n\n\n\nDownload Analytics Zoo Source\n\n\nAnalytics Zoo source code is available at \nGitHub\n\n\n$ git clone https://github.com/intel-analytics/analytics-zoo.git\n\n\n\n\nBy default, \ngit clone\n will download the development version of Analytics Zoo, if you want a release version, you can use command \ngit checkout\n to change the version.\n\n\nSetup Build Environment\n\n\nThe following instructions are aligned with master code.\n\n\nMaven 3 is needed to build Analytics Zoo, you can download it from the \nmaven website\n.\n\n\nAfter installing Maven 3, please set the environment variable MAVEN_OPTS as follows:\n\n\n$ export MAVEN_OPTS=\n-Xmx2g -XX:ReservedCodeCacheSize=512m\n\n\n\n\n\nWhen compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d.\n\n\nBuild with script (Recommended)\n\n\nIt is highly recommended that you build Analytics Zoo using the \nmake-dist.sh script\n. And it will handle the MAVEN_OPTS variable.\n\n\nOnce downloaded, you can build Analytics Zoo with the following commands:\n\n\n$ bash make-dist.sh\n\n\n\n\nAfter that, you can find a \ndist\n folder, which contains all the needed files to run a Analytics Zoo program. The files in \ndist\n include:\n\n\n\n\ndist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar\n: This jar package contains all dependencies except Spark classes.\n\n\ndist/lib/analytics-zoo-VERSION-python-api.zip\n: This zip package contains all Python files of Analytics Zoo.\n\n\n\n\nThe instructions above will build Analytics Zoo with Spark 2.0(using Scala 2.11). It is highly recommended to use \nJava 8\n when running with Spark 2.x; otherwise you may observe very poor performance.\n\n\nBuild for Spark 1.6\n\n\nTo build for Spark 1.6(which uses Scala 2.10 by default), pass \n-P spark_1.6\n to the \nmake-dist.sh\n script:\n\n\n$ bash make-dist.sh -P spark_1.6\n\n\n\n\nBuild for Scala 2.10 or 2.11\n\n\nBy default, \nmake-dist.sh\n uses Scala 2.11 for Spark 2.1, and Scala 2.10 for Spark 1.6. To override the default behaviors, you can pass \n-P scala_2.10\n or \n-P scala_2.11\n to \nmake-dist.sh\n as appropriate.\n\n\n\n\nBuild with Maven\n\n\nTo build Analytics Zoo directly using Maven, run the command below:\n\n\n$ mvn clean package -DskipTests\n\n\n\n\nAfter that, you can find that jar packages in \nPATH_TO_ANALYTICS_ZOO\n/target/, where \nPATH_TO_ANALYTICS_ZOO\n is the path to the directory of the Analytics Zoo.\n\n\nNote that the instructions above will build Analytics Zoo with Spark 2.0 (using Scala 2.11) for Linux. Similarly, you may customize the default behaviors by passing the following parameters to maven:\n\n\n\n\n-P spark_1.6\n: build for Spark 1.6 (using Scala 2.10).\n\n\n-P scala_2.10\n (or \n-P scala_2.11\n): build using Scala 2.10 (or Scala 2.11)\n\n\n\n\n\n\nSetup IDE\n\n\nWe set the scope of spark related library to \nprovided\n in pom.xml. The reason is that we don't want package spark related jars which will make analytics zoo a huge jar, and generally as analytics zoo is invoked by spark-submit, these dependencies will be provided by spark at run-time.\n\n\nThis will cause a problem in IDE. When you run applications, it will throw \nNoClassDefFoundError\n because the library scope is \nprovided\n.\n\n\nYou can easily change the scopes by the \nall-in-one\n profile.\n\n\n\n\nIn Intellij, go to View -\n Tools Windows -\n Maven Projects. Then in the Maven Projects panel, Profiles -\n click \"all-in-one\".", 
            "title": "Install"
        }, 
        {
            "location": "/ScalaUserGuide/install/#download-a-pre-built-library", 
            "text": "You can download the Analytics Zoo release and nightly build from the  Release Page", 
            "title": "Download a pre-built library"
        }, 
        {
            "location": "/ScalaUserGuide/install/#link-with-a-release-version", 
            "text": "Currently, Analytics Zoo releases are hosted on maven central; here's an example to add the Analytics Zoo dependency to your own project:  dependency \n     groupId com.intel.analytics.zoo /groupId \n     artifactId analytics-zoo-bigdl_0.6.0-[spark_1.6.2|spark_2.1.1|spark_2.2.0|spark_2.3.1] /artifactId \n     version ${ANALYTICS_ZOO_VERSION} /version  /dependency   SBT developers can use  libraryDependencies +=  com.intel.analytics.zoo  %  analytics-zoo-bigdl_0.6.0-[spark_1.6.2|spark_2.1.1|spark_2.2.0|spark_2.3.1]  %  ${ANALYTICS_ZOO_VERSION}   Remarks:   Please choose the available suffix above according to your Spark platform and the BigDL version you want to use.  You don't need to add the BigDL dependency to your project as it has already been packaged within Analytics Zoo.  You can find the option  ${ANALYTICS_ZOO_VERSION}  from the  Release Page .", 
            "title": "Link with a release version"
        }, 
        {
            "location": "/ScalaUserGuide/install/#link-with-a-development-version", 
            "text": "Currently, Analytics Zoo development version is hosted on  SonaType .  To link your application with the latest Analytics Zoo development version, you should add some dependencies like  Linking with Analytics Zoo releases , but set  ${ANALYTICS_ZOO_VERSION}  to latest version, and add below repository to your pom.xml.  repository \n     id sonatype /id \n     name sonatype repository /name \n     url https://oss.sonatype.org/content/groups/public/ /url \n     releases \n         enabled true /enabled \n     /releases \n     snapshots \n         enabled true /enabled \n     /snapshots  /repository   SBT developers can use  resolvers +=  ossrh repository  at  https://oss.sonatype.org/content/repositories/snapshots/", 
            "title": "Link with a development version"
        }, 
        {
            "location": "/ScalaUserGuide/install/#download-analytics-zoo-source", 
            "text": "Analytics Zoo source code is available at  GitHub  $ git clone https://github.com/intel-analytics/analytics-zoo.git  By default,  git clone  will download the development version of Analytics Zoo, if you want a release version, you can use command  git checkout  to change the version.", 
            "title": "Download Analytics Zoo Source"
        }, 
        {
            "location": "/ScalaUserGuide/install/#setup-build-environment", 
            "text": "The following instructions are aligned with master code.  Maven 3 is needed to build Analytics Zoo, you can download it from the  maven website .  After installing Maven 3, please set the environment variable MAVEN_OPTS as follows:  $ export MAVEN_OPTS= -Xmx2g -XX:ReservedCodeCacheSize=512m   When compiling with Java 7, you need to add the option \u201c-XX:MaxPermSize=1G\u201d.", 
            "title": "Setup Build Environment"
        }, 
        {
            "location": "/ScalaUserGuide/install/#build-with-script-recommended", 
            "text": "It is highly recommended that you build Analytics Zoo using the  make-dist.sh script . And it will handle the MAVEN_OPTS variable.  Once downloaded, you can build Analytics Zoo with the following commands:  $ bash make-dist.sh  After that, you can find a  dist  folder, which contains all the needed files to run a Analytics Zoo program. The files in  dist  include:   dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar : This jar package contains all dependencies except Spark classes.  dist/lib/analytics-zoo-VERSION-python-api.zip : This zip package contains all Python files of Analytics Zoo.   The instructions above will build Analytics Zoo with Spark 2.0(using Scala 2.11). It is highly recommended to use  Java 8  when running with Spark 2.x; otherwise you may observe very poor performance.", 
            "title": "Build with script (Recommended)"
        }, 
        {
            "location": "/ScalaUserGuide/install/#build-for-spark-16", 
            "text": "To build for Spark 1.6(which uses Scala 2.10 by default), pass  -P spark_1.6  to the  make-dist.sh  script:  $ bash make-dist.sh -P spark_1.6", 
            "title": "Build for Spark 1.6"
        }, 
        {
            "location": "/ScalaUserGuide/install/#build-for-scala-210-or-211", 
            "text": "By default,  make-dist.sh  uses Scala 2.11 for Spark 2.1, and Scala 2.10 for Spark 1.6. To override the default behaviors, you can pass  -P scala_2.10  or  -P scala_2.11  to  make-dist.sh  as appropriate.", 
            "title": "Build for Scala 2.10 or 2.11"
        }, 
        {
            "location": "/ScalaUserGuide/install/#build-with-maven", 
            "text": "To build Analytics Zoo directly using Maven, run the command below:  $ mvn clean package -DskipTests  After that, you can find that jar packages in  PATH_TO_ANALYTICS_ZOO /target/, where  PATH_TO_ANALYTICS_ZOO  is the path to the directory of the Analytics Zoo.  Note that the instructions above will build Analytics Zoo with Spark 2.0 (using Scala 2.11) for Linux. Similarly, you may customize the default behaviors by passing the following parameters to maven:   -P spark_1.6 : build for Spark 1.6 (using Scala 2.10).  -P scala_2.10  (or  -P scala_2.11 ): build using Scala 2.10 (or Scala 2.11)", 
            "title": "Build with Maven"
        }, 
        {
            "location": "/ScalaUserGuide/install/#setup-ide", 
            "text": "We set the scope of spark related library to  provided  in pom.xml. The reason is that we don't want package spark related jars which will make analytics zoo a huge jar, and generally as analytics zoo is invoked by spark-submit, these dependencies will be provided by spark at run-time.  This will cause a problem in IDE. When you run applications, it will throw  NoClassDefFoundError  because the library scope is  provided .  You can easily change the scopes by the  all-in-one  profile.   In Intellij, go to View -  Tools Windows -  Maven Projects. Then in the Maven Projects panel, Profiles -  click \"all-in-one\".", 
            "title": "Setup IDE"
        }, 
        {
            "location": "/ScalaUserGuide/run/", 
            "text": "Set Environment Variables\n\n\nSet \nANALYTICS_ZOO_HOME\n and \nSPARK_HOME\n:\n\n\n\n\nIf you download Analytics Zoo from the \nRelease Page\n\n\n\n\nexport SPARK_HOME=folder path where you extract the spark package\nexport ANALYTICS_ZOO_HOME=folder path where you extract the analytics zoo package\n\n\n\n\n\n\nIf you build Analytics Zoo by yourself\n\n\n\n\nexport SPARK_HOME=folder path where you extract the spark package\nexport ANALYTICS_ZOO_HOME=the dist folder generated by the build process, which is under the top level of the source folder\n\n\n\n\n\n\nUse Interactive Spark Shell\n\n\nYou can try Analytics Zoo easily using the Spark interactive shell. Run below command to start spark shell with Analytics Zoo support:\n\n\n${ANALYTICS_ZOO_HOME}/bin/spark-shell-with-zoo.sh --master local[*]\n\n\n\n\nYou will see a welcome message looking like below:\n\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala\n\n\n\n\n\nNow you'll be able to play with Analytics Zoo API's.\nFor instance, to load a pre-trained object detection model, you may try below code:\n\n\nscala\n import com.intel.analytics.zoo.models.image.objectdetection.ObjectDetector\nimport com.intel.analytics.zoo.models.image.objectdetection.ObjectDetector\n\nscala\n ObjectDetector.loadModel[Float](params.modelPath)\n\n\n\n\n\n\nRun as a Spark Program\n\n\nYou can run a analytics zoo program, e.g., the \nObject Detection\n, as a standard Spark program (running in either local mode or cluster mode) as follows:\n\n\n\n\nDownload the pre-trained model from \nhere\n.\n\n\nPrepare predict images\n\n\nRun the following command:\n\n\n\n\n  # Spark local mode\n  spark-submit --master local[core_number] --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n  # Spark standalone mode\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n  # Spark yarn client mode\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n  # Spark yarn cluster mode\n  spark-submit --master yarn --deploy-mode cluster \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n\n\n\nIf you are to run your own program, do remember to create SparkContext and initialize before calling other Analytics Zoo API's, as shown below.\n\n\nimport com.intel.analytics.zoo.common.NNContext\nval sc = NNContext.initNNContext(conf)", 
            "title": "Run"
        }, 
        {
            "location": "/ScalaUserGuide/run/#set-environment-variables", 
            "text": "Set  ANALYTICS_ZOO_HOME  and  SPARK_HOME :   If you download Analytics Zoo from the  Release Page   export SPARK_HOME=folder path where you extract the spark package\nexport ANALYTICS_ZOO_HOME=folder path where you extract the analytics zoo package   If you build Analytics Zoo by yourself   export SPARK_HOME=folder path where you extract the spark package\nexport ANALYTICS_ZOO_HOME=the dist folder generated by the build process, which is under the top level of the source folder", 
            "title": "Set Environment Variables"
        }, 
        {
            "location": "/ScalaUserGuide/run/#use-interactive-spark-shell", 
            "text": "You can try Analytics Zoo easily using the Spark interactive shell. Run below command to start spark shell with Analytics Zoo support:  ${ANALYTICS_ZOO_HOME}/bin/spark-shell-with-zoo.sh --master local[*]  You will see a welcome message looking like below:  Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n      /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_79)\nSpark context available as sc.\nscala   Now you'll be able to play with Analytics Zoo API's.\nFor instance, to load a pre-trained object detection model, you may try below code:  scala  import com.intel.analytics.zoo.models.image.objectdetection.ObjectDetector\nimport com.intel.analytics.zoo.models.image.objectdetection.ObjectDetector\n\nscala  ObjectDetector.loadModel[Float](params.modelPath)", 
            "title": "Use Interactive Spark Shell"
        }, 
        {
            "location": "/ScalaUserGuide/run/#run-as-a-spark-program", 
            "text": "You can run a analytics zoo program, e.g., the  Object Detection , as a standard Spark program (running in either local mode or cluster mode) as follows:   Download the pre-trained model from  here .  Prepare predict images  Run the following command:     # Spark local mode\n  spark-submit --master local[core_number] --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n  # Spark standalone mode\n  spark-submit --master spark://... --executor-cores cores_per_executor \\\n  --total-executor-cores total_cores_for_the_job \\\n  --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n  # Spark yarn client mode\n  spark-submit --master yarn --deploy-mode client \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model\n\n  # Spark yarn cluster mode\n  spark-submit --master yarn --deploy-mode cluster \\\n  --executor-cores cores_per_executor \\\n  --num-executors executors_number \\\n  --class com.intel.analytics.zoo.examples.objectdetection.Predict \\\n  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \\\n  --image path_to_your_images --output path_to_output --model path_to_model  If you are to run your own program, do remember to create SparkContext and initialize before calling other Analytics Zoo API's, as shown below.  import com.intel.analytics.zoo.common.NNContext\nval sc = NNContext.initNNContext(conf)", 
            "title": "Run as a Spark Program"
        }, 
        {
            "location": "/ScalaUserGuide/examples/", 
            "text": "Analytics Zoo provides plenty of examples ready for re-use as listed below.\n\n\n\n\nImage Classification\n: This example illustrates how to do the image classification with pre-trained model.\n\n\nObject Detection\n: This example illustrates how to detect objects in image with pre-trained model.\n\n\nRecommendation\n: There are two Scala examples for recommender models, including wide and deep(WND) model and Neural network-based Collaborative Filtering(NCF) model.\n\n\nText Classification\n: This example uses pre-trained GloVe embeddings to convert words to vectors and trains a CNN, LSTM or GRU \nTextClassifier\n model on 20 Newsgroup dataset\n\n\nDataFrame\n: There are three examples to show how to perform transfer learning/model inference using pre-trained Inception v1 model with DataFrame-based API.\n\n\nTFNet\n: TFNet can encapsulate a frozen TensorFlow graph as an Analytics Zoo layer for inference. This example illustrates how to use a pre-trained TensorFlow object detection model to make inferences using Analytics Zoo on Spark.", 
            "title": "Examples"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/", 
            "text": "Overview\n\n\nNNFrames is a package in Analytics Zoo aiming to provide DataFrame-based high level API to\nfacilitate Spark users and speed-up development. It supports native integration with Spark ML\nPipeline, which allows user to combine the power of Analytics Zoo, BigDL and Apache Spark MLlib.\nNNFrames provides both Python and Scala interfaces, and is compatible with both Spark 1.6 and\nSpark 2.x.\n\n\nHighlights\n\n\n\n\n\n\nEasy-to-use DataFrame(DataSet)-based API for training, prediction and evaluation with deep learning models.\n\n\n\n\n\n\nEffortless integration with Spark ML pipeline and compatibility with other feature transformers and algorithms in Spark ML.\n\n\n\n\n\n\nIn a few lines, run large scale inference or transfer learning from pre-trained models of Caffe, Keras, Tensorflow or BigDL.\n\n\n\n\n\n\nTraining of customized model or BigDL built-in neural models (e.g. Inception, ResNet, Wide And Deep).\n\n\n\n\n\n\nRich toolset for feature extraction and processing, including image, audio and texts.\n\n\n\n\n\n\nExamples:\n\n\nThe examples are included in the Analytics Zoo source code.\n\n\n\n\nimage classification: model inference using pre-trained Inception v1 model.\n    \nScala version\n\n    \nPython version\n\n\nimage classification: transfer learning from pre-trained Inception v1 model.\n    \nScala version\n\n    \nPython version\n\n\n\n\nPrimary APIs\n\n\nNNEstimator and NNModel\n\n\nAnalytics Zoo provides \nNNEstimator\n for model training with Spark DataFrame, which\nprovides high level API for training a BigDL Model with the Apache Spark\n\nEstimator\n/\n\nTransfomer\n\npattern, thus users can conveniently fit Analytics Zoo into a ML pipeline. The fit result of\n\nNNEstimator\n is a NNModel, which is a Spark ML Transformer.\n\n\nplease check our\n\nNNEstimator API\n for detailed usage.\n\n\nNNClassifier and NNClassifierModel\n\n\nNNClassifier\n and \nNNClassifierModel\nextends \nNNEstimator\n and \nNNModel\n and focus on \nclassification tasks, where both label column and prediction column are of Double type.\n\n\nNNImageReader\n\nNNImageReader loads image into Spark DataFrame.\n\n\nplease check our\n\nImageProcessing\n for detailed usage.", 
            "title": "DataFrame and ML Pipeline"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/#overview", 
            "text": "NNFrames is a package in Analytics Zoo aiming to provide DataFrame-based high level API to\nfacilitate Spark users and speed-up development. It supports native integration with Spark ML\nPipeline, which allows user to combine the power of Analytics Zoo, BigDL and Apache Spark MLlib.\nNNFrames provides both Python and Scala interfaces, and is compatible with both Spark 1.6 and\nSpark 2.x.  Highlights    Easy-to-use DataFrame(DataSet)-based API for training, prediction and evaluation with deep learning models.    Effortless integration with Spark ML pipeline and compatibility with other feature transformers and algorithms in Spark ML.    In a few lines, run large scale inference or transfer learning from pre-trained models of Caffe, Keras, Tensorflow or BigDL.    Training of customized model or BigDL built-in neural models (e.g. Inception, ResNet, Wide And Deep).    Rich toolset for feature extraction and processing, including image, audio and texts.", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/#examples", 
            "text": "The examples are included in the Analytics Zoo source code.   image classification: model inference using pre-trained Inception v1 model.\n     Scala version \n     Python version  image classification: transfer learning from pre-trained Inception v1 model.\n     Scala version \n     Python version", 
            "title": "Examples:"
        }, 
        {
            "location": "/ProgrammingGuide/nnframes/#primary-apis", 
            "text": "NNEstimator and NNModel  Analytics Zoo provides  NNEstimator  for model training with Spark DataFrame, which\nprovides high level API for training a BigDL Model with the Apache Spark Estimator / Transfomer \npattern, thus users can conveniently fit Analytics Zoo into a ML pipeline. The fit result of NNEstimator  is a NNModel, which is a Spark ML Transformer.  please check our NNEstimator API  for detailed usage.  NNClassifier and NNClassifierModel  NNClassifier  and  NNClassifierModel extends  NNEstimator  and  NNModel  and focus on \nclassification tasks, where both label column and prediction column are of Double type.  NNImageReader \nNNImageReader loads image into Spark DataFrame.  please check our ImageProcessing  for detailed usage.", 
            "title": "Primary APIs"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/", 
            "text": "Overview\n\n\nautograd\n provides automatic differentiation for math operations, so that you can easily build your own \ncustom loss and layer\n (in both Python and Scala), as illustracted below. (See more examples \nhere\n). Conceptually we use reverse mode together with the chain rule for automatic differentiation. \nVariable\n is used to record the linkage of the operation history, which would generated a static directed acyclic graph for the backward execution. Within the execution graph, leaves are the input \nvariables\n and roots are the output \nvariables\n.\n\n\nCustomLoss\n\n\n1.Define a custom function using \nautograd\n\n\nfrom zoo.pipeline.api.autograd import *\n\ndef mean_absolute_error(y_true, y_pred):\n   return mean(abs(y_true - y_pred), axis=1)\n\n\n\n\n\n\nUse \nCustomLoss\n in \ncompile\n method.\n\n\n\n\n# You can pass the loss function directly into `loss`\nmodel.compile(optimizer = SGD(), loss = mean_absolute_error)\nmodel.fit(x = ..., y = ...)\n\n\n\n\n\n\nUse \nCustomLoss\n in \nnnframe\n pipeline.\n\n\n\n\n# 1) Create a CustomLoss object from function.\nloss = CustomLoss(mean_absolute_error, y_pred_shape=[2], y_true_shape=[2])\n# 2) Passing the CustomLoss object to NNClassifier.\nclassifier = NNClassifier(lrModel, loss, SeqToTensor([1000]))\n\n\n\n\n\n\nUse \nforward\n and \nbackward\n to evaluate a \nCustomLoss\n for debugging.\n\n\n\n\n# y_pred_shape=[2] is a shape without batch\nloss = CustomLoss(mean_absolute_error, y_pred_shape=[2], y_true_shape=[2])\nerror = loss.forward(y_true=np.random.uniform(0, 1, shape[3, 2]), y_pred=np.random.uniform(0, 1, shape[3, 2]))\ngrad = loss.backward(y_true=np.random.uniform(0, 1, shape[3, 2]), y_pred=np.random.uniform(0, 1, shape[3, 2]))\n\n\n\n\nLambda layer\n\n\n1.Define custom function using \nautograd\n\n\nfrom zoo.pipeline.api.autograd import *\ndef add_one_func(x):\n   return x + 1.0\n\n\n\n\n2.Define model using Keras-style API and \ncustom \nLambda\n layer\n\n\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nmodel = Sequential().add(Dense(1, input_shape=(2,))) \\\n                   .add(Lambda(function=add_one_func))\n# Evaluation for debug purpose.\nmodel.forward(np.random.uniform(0, 1, shape[3, 2])) # 3 is the batch size\n\n\n\n\n\nConstruct variable computation without \nLambda\n layer\n\n\n\n\nThe returning type for each operation is a \nVariable\n, so you can connect those \nVariable\n together freely without using \nLambda\n. i.e \nDense[Float](3).from(input2)\n or \ninput1 + input2\n\n\nShape inference is supported as well, which means you can check the output shape of a \nVariable\n by calling \nget_output_shape()\n\n\n\n\nPython\n\n\nimport zoo.pipeline.api.autograd as auto\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\n\ninput = Input(shape=[2, 20]) # create a variable\ntime = TimeDistributed(layer=Dense(30))(input) # time is a variable\nt1 = time.index_select(1, 0) # t1 is a variable\nt2 = time.index_select(1, 1)\ndiff = auto.abs(t1 - t2)\nassert diff.get_output_shape() == (None, 30)\nassert diff.get_input_shape() == (None, 30)\nmodel = Model(input, diff)\ndata = np.random.uniform(0, 1, [10, 2, 20])\noutput = model.forward(data)\n\n\n\n\nScala\n- In respect of backward compatibility, the scala API is slightly different with the python API.\n- \nlayer.inputs(node)\n would return a node(backward compatibility).\n- \nlayer.from(variable)\n would return a variable.(You may want to use this style as it can support autograd.)\n\n\nimport com.intel.analytics.zoo.pipeline.api.autograd.Variable\nimport com.intel.analytics.zoo.pipeline.api.autograd.AutoGrad\nimport com.intel.analytics.zoo.pipeline.api.keras.models.Model\nimport com.intel.analytics.zoo.pipeline.api.keras.layers._\n\nval input1 = Variable[Float](inputShape = Shape(3))\nval input2 = Variable[Float](inputShape = Shape(3))\nval diff = AutoGrad.abs(input1 - Dense[Float](3).from(input2))\nval model = Model[Float](input = Array(input1, input2), output = diff)\nval inputValue = Tensor[Float](1, 3).randn()\n// In scala, we use Table for multiple inputs. `T` is a short-cut for creating a Table.\nval out = model.forward(T(inputValue, inputValue)).toTensor[Float]\n\n\n\n\nDefine a model using trainable Parameter\n\n\nBuild a \nLinear\n Model (Wx + b) by using trainable \nParameter\n which is equivalent to use \nDense\n layer.\n* Scala\n\n\nimport com.intel.analytics.zoo.pipeline.api.autograd.{AutoGrad, Parameter, Variable}\nimport com.intel.analytics.zoo.pipeline.api.keras.models.Model\nval input = Variable[Float](Shape(3))\nval w = Parameter[Float](Shape(2, 3)) // outputSize * inputSize\nval bias = Parameter[Float](Shape(2))\nval cDense = AutoGrad.mm(input, w, axes = List(1, 1)) + bias\nval model = Model[Float](input = input, output = cDense)\n\n\n\n\n\n\n\nPython\n\n\n\n\nfrom zoo.pipeline.api.autograd import *\nfrom zoo.pipeline.api.keras.models import *\ninput = Variable((3,))\nw = Parameter((2, 3)) # outputSize * inputSize\nbias = Parameter((2,))\ncDense = mm(input, w, axes = (1, 1)) + bias\nmodel = Model(input = input, output = cDense)", 
            "title": "Autograd"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/#overview", 
            "text": "autograd  provides automatic differentiation for math operations, so that you can easily build your own  custom loss and layer  (in both Python and Scala), as illustracted below. (See more examples  here ). Conceptually we use reverse mode together with the chain rule for automatic differentiation.  Variable  is used to record the linkage of the operation history, which would generated a static directed acyclic graph for the backward execution. Within the execution graph, leaves are the input  variables  and roots are the output  variables .", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/#customloss", 
            "text": "1.Define a custom function using  autograd  from zoo.pipeline.api.autograd import *\n\ndef mean_absolute_error(y_true, y_pred):\n   return mean(abs(y_true - y_pred), axis=1)   Use  CustomLoss  in  compile  method.   # You can pass the loss function directly into `loss`\nmodel.compile(optimizer = SGD(), loss = mean_absolute_error)\nmodel.fit(x = ..., y = ...)   Use  CustomLoss  in  nnframe  pipeline.   # 1) Create a CustomLoss object from function.\nloss = CustomLoss(mean_absolute_error, y_pred_shape=[2], y_true_shape=[2])\n# 2) Passing the CustomLoss object to NNClassifier.\nclassifier = NNClassifier(lrModel, loss, SeqToTensor([1000]))   Use  forward  and  backward  to evaluate a  CustomLoss  for debugging.   # y_pred_shape=[2] is a shape without batch\nloss = CustomLoss(mean_absolute_error, y_pred_shape=[2], y_true_shape=[2])\nerror = loss.forward(y_true=np.random.uniform(0, 1, shape[3, 2]), y_pred=np.random.uniform(0, 1, shape[3, 2]))\ngrad = loss.backward(y_true=np.random.uniform(0, 1, shape[3, 2]), y_pred=np.random.uniform(0, 1, shape[3, 2]))", 
            "title": "CustomLoss"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/#lambda-layer", 
            "text": "1.Define custom function using  autograd  from zoo.pipeline.api.autograd import *\ndef add_one_func(x):\n   return x + 1.0  2.Define model using Keras-style API and  custom  Lambda  layer  from zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nmodel = Sequential().add(Dense(1, input_shape=(2,))) \\\n                   .add(Lambda(function=add_one_func))\n# Evaluation for debug purpose.\nmodel.forward(np.random.uniform(0, 1, shape[3, 2])) # 3 is the batch size", 
            "title": "Lambda layer"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/#construct-variable-computation-without-lambda-layer", 
            "text": "The returning type for each operation is a  Variable , so you can connect those  Variable  together freely without using  Lambda . i.e  Dense[Float](3).from(input2)  or  input1 + input2  Shape inference is supported as well, which means you can check the output shape of a  Variable  by calling  get_output_shape()   Python  import zoo.pipeline.api.autograd as auto\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\n\ninput = Input(shape=[2, 20]) # create a variable\ntime = TimeDistributed(layer=Dense(30))(input) # time is a variable\nt1 = time.index_select(1, 0) # t1 is a variable\nt2 = time.index_select(1, 1)\ndiff = auto.abs(t1 - t2)\nassert diff.get_output_shape() == (None, 30)\nassert diff.get_input_shape() == (None, 30)\nmodel = Model(input, diff)\ndata = np.random.uniform(0, 1, [10, 2, 20])\noutput = model.forward(data)  Scala\n- In respect of backward compatibility, the scala API is slightly different with the python API.\n-  layer.inputs(node)  would return a node(backward compatibility).\n-  layer.from(variable)  would return a variable.(You may want to use this style as it can support autograd.)  import com.intel.analytics.zoo.pipeline.api.autograd.Variable\nimport com.intel.analytics.zoo.pipeline.api.autograd.AutoGrad\nimport com.intel.analytics.zoo.pipeline.api.keras.models.Model\nimport com.intel.analytics.zoo.pipeline.api.keras.layers._\n\nval input1 = Variable[Float](inputShape = Shape(3))\nval input2 = Variable[Float](inputShape = Shape(3))\nval diff = AutoGrad.abs(input1 - Dense[Float](3).from(input2))\nval model = Model[Float](input = Array(input1, input2), output = diff)\nval inputValue = Tensor[Float](1, 3).randn()\n// In scala, we use Table for multiple inputs. `T` is a short-cut for creating a Table.\nval out = model.forward(T(inputValue, inputValue)).toTensor[Float]", 
            "title": "Construct variable computation without Lambda layer"
        }, 
        {
            "location": "/ProgrammingGuide/autograd/#define-a-model-using-trainable-parameter", 
            "text": "Build a  Linear  Model (Wx + b) by using trainable  Parameter  which is equivalent to use  Dense  layer.\n* Scala  import com.intel.analytics.zoo.pipeline.api.autograd.{AutoGrad, Parameter, Variable}\nimport com.intel.analytics.zoo.pipeline.api.keras.models.Model\nval input = Variable[Float](Shape(3))\nval w = Parameter[Float](Shape(2, 3)) // outputSize * inputSize\nval bias = Parameter[Float](Shape(2))\nval cDense = AutoGrad.mm(input, w, axes = List(1, 1)) + bias\nval model = Model[Float](input = input, output = cDense)   Python   from zoo.pipeline.api.autograd import *\nfrom zoo.pipeline.api.keras.models import *\ninput = Variable((3,))\nw = Parameter((2, 3)) # outputSize * inputSize\nbias = Parameter((2,))\ncDense = mm(input, w, axes = (1, 1)) + bias\nmodel = Model(input = input, output = cDense)", 
            "title": "Define a model using trainable Parameter"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/", 
            "text": "Overview\n\n\nAnalytics Zoo provides some useful utilities for transfer learning.\n\n\nLoading a pre-trained model\n\n\nWe can use the \nNet\n api to load a pre-trained model, including models saved by Analytics Zoo,\nBigDL, Torch, Caffe and Tensorflow. Please refer to \nNet API Guide\n\n\nRemove the last a few layers\n\n\nWhen a model is loaded using \nNet\n, we can use the \nnewGraph(output)\n api to define a Model with\nthe output specified by the parameter.\n\n\nFor example, \n\n\nIn scala:\n\n\nval inception = Net.loadBigDL[Float](inception_path)\n      .newGraph(output = \npool5/drop_7x7_s1\n)\n\n\n\n\n\nIn python:\n\n\nfull_model = Net.load_bigdl(model_path)\n# create a new model by remove layers after pool5/drop_7x7_s1\nmodel = full_model.new_graph([\npool5/drop_7x7_s1\n])\n\n\n\n\nThe returning model's output layer is \"pool5/drop_7x7_s1\".\n\n\nFreeze some layers\n\n\nIn transfer learning, we often want to freeze some layers to prevent overfitting. In Analytics Zoo,\nwe can use the \nfreezeUpTo(endPoint)\n api to do that.\n\n\nFor example,\n\n\nIn scala:\n\n\ninception.freezeUpTo(\npool4/3x3_s2\n) // freeze layer pool4/3x3_s2 and the layers before it\n\n\n\n\nIn python:\n\n\n# freeze layers from input to pool4/3x3_s2 inclusive\nmodel.freeze_up_to([\npool4/3x3_s2\n])\n\n\n\n\nThis will freeze all the layers from the input layer to \"pool4/3x3_s2\"\n\n\nExample\n\n\nFor a complete example, refer to the \nscala transfer learning example\n\nand \npython transfer learning example", 
            "title": "Transfer Learning"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/#overview", 
            "text": "Analytics Zoo provides some useful utilities for transfer learning.", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/#loading-a-pre-trained-model", 
            "text": "We can use the  Net  api to load a pre-trained model, including models saved by Analytics Zoo,\nBigDL, Torch, Caffe and Tensorflow. Please refer to  Net API Guide", 
            "title": "Loading a pre-trained model"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/#remove-the-last-a-few-layers", 
            "text": "When a model is loaded using  Net , we can use the  newGraph(output)  api to define a Model with\nthe output specified by the parameter.  For example,   In scala:  val inception = Net.loadBigDL[Float](inception_path)\n      .newGraph(output =  pool5/drop_7x7_s1 )  In python:  full_model = Net.load_bigdl(model_path)\n# create a new model by remove layers after pool5/drop_7x7_s1\nmodel = full_model.new_graph([ pool5/drop_7x7_s1 ])  The returning model's output layer is \"pool5/drop_7x7_s1\".", 
            "title": "Remove the last a few layers"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/#freeze-some-layers", 
            "text": "In transfer learning, we often want to freeze some layers to prevent overfitting. In Analytics Zoo,\nwe can use the  freezeUpTo(endPoint)  api to do that.  For example,  In scala:  inception.freezeUpTo( pool4/3x3_s2 ) // freeze layer pool4/3x3_s2 and the layers before it  In python:  # freeze layers from input to pool4/3x3_s2 inclusive\nmodel.freeze_up_to([ pool4/3x3_s2 ])  This will freeze all the layers from the input layer to \"pool4/3x3_s2\"", 
            "title": "Freeze some layers"
        }, 
        {
            "location": "/ProgrammingGuide/transferlearning/#example", 
            "text": "For a complete example, refer to the  scala transfer learning example \nand  python transfer learning example", 
            "title": "Example"
        }, 
        {
            "location": "/ProgrammingGuide/inference/", 
            "text": "Inference Model\n\n\nOverview\n\n\nInference is a package in Analytics Zoo aiming to provide high level APIs to speed-up development. It \nallows user to conveniently use pre-trained models from Analytics Zoo, Tensorflow and Caffe.\nInference provides multiple Java/Scala interfaces.\n\n\nHighlights\n\n\n\n\n\n\nEasy-to-use java/scala API for loading and prediction with deep learning models.\n\n\n\n\n\n\nSupport transformation of various input data type, thus supporting future prediction tasks\n\n\n\n\n\n\nJava Example\n\n\nIt's very easy to apply abstract inference model for inference with below code piece.\nYou will need to write a subclass that extends AbstractinferenceModel.\n\n\nimport com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;\nimport com.intel.analytics.zoo.pipeline.inference.JTensor;\n\npublic class TextClassificationModel extends AbstractInferenceModel {\n    public TextClassificationModel() {\n        super();\n    }\n }\nTextClassificationModel model = new TextClassificationModel();\nmodel.load(modelPath, weightPath);\nList\nList\nJTensor\n result = model.predict(inputList);\n\n\n\n\nScala Example\n\n\nimport com.intel.analytics.zoo.pipeline.inference.FloatInferenceModel\n\nclass TextClassificationModel(var model: AbstractModule[Activity, Activity, Float],\n                                @transient var predictor: LocalPredictor[Float]) extends FloatInferenceModel {\n\n}\n\nTextClassificationModel model = new TextClassificationModel(model, predictor)\nval result = model.predict(inputList)", 
            "title": "Model Serving"
        }, 
        {
            "location": "/ProgrammingGuide/inference/#inference-model", 
            "text": "", 
            "title": "Inference Model"
        }, 
        {
            "location": "/ProgrammingGuide/inference/#overview", 
            "text": "Inference is a package in Analytics Zoo aiming to provide high level APIs to speed-up development. It \nallows user to conveniently use pre-trained models from Analytics Zoo, Tensorflow and Caffe.\nInference provides multiple Java/Scala interfaces.", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/inference/#highlights", 
            "text": "Easy-to-use java/scala API for loading and prediction with deep learning models.    Support transformation of various input data type, thus supporting future prediction tasks", 
            "title": "Highlights"
        }, 
        {
            "location": "/ProgrammingGuide/inference/#java-example", 
            "text": "It's very easy to apply abstract inference model for inference with below code piece.\nYou will need to write a subclass that extends AbstractinferenceModel.  import com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;\nimport com.intel.analytics.zoo.pipeline.inference.JTensor;\n\npublic class TextClassificationModel extends AbstractInferenceModel {\n    public TextClassificationModel() {\n        super();\n    }\n }\nTextClassificationModel model = new TextClassificationModel();\nmodel.load(modelPath, weightPath);\nList List JTensor  result = model.predict(inputList);", 
            "title": "Java Example"
        }, 
        {
            "location": "/ProgrammingGuide/inference/#scala-example", 
            "text": "import com.intel.analytics.zoo.pipeline.inference.FloatInferenceModel\n\nclass TextClassificationModel(var model: AbstractModule[Activity, Activity, Float],\n                                @transient var predictor: LocalPredictor[Float]) extends FloatInferenceModel {\n\n}\n\nTextClassificationModel model = new TextClassificationModel(model, predictor)\nval result = model.predict(inputList)", 
            "title": "Scala Example"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow/", 
            "text": "Analytics-Zoo provides a set APIs for running tensorflow model on Spark in a distributed fashion.\n\n\nConcepts\n\n\n\n\n\n\nTFDatasets\n represents a distributed collection of elements to be fed into a Tensorflow graph.\nTFDatasets can be created directly from an RDD; each record in the RDD should be a list of numpy.ndarray\nrepresenting the input data. TFDatasets must be used with the TFOptimizer or TFPredictor (to be described next).\n\n\n\n\n\n\nTFOptimizer\n is the class that does all the hard work in distributed training, such as model\ndistribution and parameter synchronization. It takes the user specified \nloss\n (a TensorFlow scalar tensor) as\nan argument and runs stochastic gradient descent using the given \noptimMethod\n on all the \nVariables\n that\ncontribute to this loss.\n\n\n\n\n\n\nTFPredictor\n takes a list of user specified TensorFlow tensors as the model outputs, and feed all the\nelements in TFDatasets to produce those outputs; it returns a Spark RDD with each of its records representing the\nmodel prediction for the corresponding input elements.\n\n\n\n\n\n\nTraining\n\n\n1.Data wrangling and analysis using PySpark\n\n\nfrom zoo import init_nncontext\nfrom zoo.pipeline.api.net import TFDataset\nfrom tensorflow as tf\n\nsc = init_nncontext()\n\n# Each record in the train_rdd consists of a list of NumPy ndrrays\ntrain_rdd = sc.parallelize(file_list)\n  .map(lambda x: read_image_and_label(x))\n  .map(lambda image_label: decode_to_ndarrays(image_label))\n\n# TFDataset represents a distributed set of elements,\n# in which each element contains one or more Tensorflow Tensor objects. \ndataset = TFDataset.from_rdd(train_rdd,\n                             names=[\nfeatures\n, \nlabels\n],\n                             shapes=[[28, 28, 1], [1]],\n                             types=[tf.float32, tf.int32],\n                             batch_size=BATCH_SIZE)\n\n\n\n\n2.Deep learning model development using TensorFlow\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\nimages, labels = dataset.tensors\nsqueezed_labels = tf.squeeze(labels)\nwith slim.arg_scope(lenet.lenet_arg_scope()):\n     logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)\n\nloss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=squeezed_labels))\n\n\n\n\n3.Distributed training on Spark and BigDL\n\n\nfrom zoo.pipeline.api.net import TFOptimizer\nfrom bigdl.optim.optimizer import MaxIteration, Adam, MaxEpoch, TrainSummary\n\noptimizer = TFOptimizer(loss, Adam(1e-3))\noptimizer.set_train_summary(TrainSummary(\n/tmp/az_lenet\n, \nlenet\n))\noptimizer.optimize(end_trigger=MaxEpoch(5))\n\n\n\n\n4.Save the variable to checkpoint\n\n\nsaver = tf.train.Saver()\nsaver.save(optimizer.sess, \n/tmp/lenet/\n)\n\n\n\n\nInference\n\n\n1.Data processing using PySpark\n\n\nfrom zoo import init_nncontext\nfrom zoo.pipeline.api.net import TFDataset\nfrom tensorflow as tf\n\nsc = init_nncontext()\n\n# Each record in the train_rdd consists of a list of NumPy ndrrays\ntesting_rdd = sc.parallelize(file_list)\n  .map(lambda x: read_image_and_label(x))\n  .map(lambda image_label: decode_to_ndarrays(image_label))\n\n# TFDataset represents a distributed set of elements,\n# in which each element contains one or more Tensorflow Tensor objects. \ndataset = TFDataset.from_rdd(testing_rdd,\n                             names=[\nfeatures\n],\n                             shapes=[[28, 28, 1]],\n                             types=[tf.float32])\n\n\n\n\n2.Reconstruct the model for inference and load the checkpoint\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\nimages, labels = dataset.tensors\nwith slim.arg_scope(lenet.lenet_arg_scope()):\n     logits, end_points = lenet.lenet(images, num_classes=10, is_training=False)\n\nsess = tf.Session()\nsaver = tf.train.Saver()\nsaver.restore(sess, \n/tmp/lenet\n)\n\n\n\n\n3.Run predictions\n\n\npredictor = TFPredictor(sess, [logits])\npredictions_rdd = predictor.predict()\n\n\n\n\nRelationship to TFNet\n\n\nTFNet\n is a layer representing a TensorFlow sub-graph (specified by the input and output TensorFlow tensors).\nIt implements the standard BigDL layer API, and can be used with other Analytics-Zoo/BigDL layers\nto construct more complex models for training or inference using the standard Analytics-Zoo/BigDL API. \n\n\nYou can think of \nTFDatasets\n, \nTFOptimizer\n, \nTFPredictor\n as a set API for training/testing TensorFlow models\non Spark/BigDL, while \nTFNet\n as an Analytics-Zoo/BigDL layer initialized using TensorFlow graph.\n\n\nFor more information on TFNet, please refer to the \nAPI Guide", 
            "title": "Distributed Tensoflow on Spark/BigDL"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow/#concepts", 
            "text": "TFDatasets  represents a distributed collection of elements to be fed into a Tensorflow graph.\nTFDatasets can be created directly from an RDD; each record in the RDD should be a list of numpy.ndarray\nrepresenting the input data. TFDatasets must be used with the TFOptimizer or TFPredictor (to be described next).    TFOptimizer  is the class that does all the hard work in distributed training, such as model\ndistribution and parameter synchronization. It takes the user specified  loss  (a TensorFlow scalar tensor) as\nan argument and runs stochastic gradient descent using the given  optimMethod  on all the  Variables  that\ncontribute to this loss.    TFPredictor  takes a list of user specified TensorFlow tensors as the model outputs, and feed all the\nelements in TFDatasets to produce those outputs; it returns a Spark RDD with each of its records representing the\nmodel prediction for the corresponding input elements.", 
            "title": "Concepts"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow/#training", 
            "text": "1.Data wrangling and analysis using PySpark  from zoo import init_nncontext\nfrom zoo.pipeline.api.net import TFDataset\nfrom tensorflow as tf\n\nsc = init_nncontext()\n\n# Each record in the train_rdd consists of a list of NumPy ndrrays\ntrain_rdd = sc.parallelize(file_list)\n  .map(lambda x: read_image_and_label(x))\n  .map(lambda image_label: decode_to_ndarrays(image_label))\n\n# TFDataset represents a distributed set of elements,\n# in which each element contains one or more Tensorflow Tensor objects. \ndataset = TFDataset.from_rdd(train_rdd,\n                             names=[ features ,  labels ],\n                             shapes=[[28, 28, 1], [1]],\n                             types=[tf.float32, tf.int32],\n                             batch_size=BATCH_SIZE)  2.Deep learning model development using TensorFlow  import tensorflow as tf\n\nslim = tf.contrib.slim\n\nimages, labels = dataset.tensors\nsqueezed_labels = tf.squeeze(labels)\nwith slim.arg_scope(lenet.lenet_arg_scope()):\n     logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)\n\nloss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=squeezed_labels))  3.Distributed training on Spark and BigDL  from zoo.pipeline.api.net import TFOptimizer\nfrom bigdl.optim.optimizer import MaxIteration, Adam, MaxEpoch, TrainSummary\n\noptimizer = TFOptimizer(loss, Adam(1e-3))\noptimizer.set_train_summary(TrainSummary( /tmp/az_lenet ,  lenet ))\noptimizer.optimize(end_trigger=MaxEpoch(5))  4.Save the variable to checkpoint  saver = tf.train.Saver()\nsaver.save(optimizer.sess,  /tmp/lenet/ )", 
            "title": "Training"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow/#inference", 
            "text": "1.Data processing using PySpark  from zoo import init_nncontext\nfrom zoo.pipeline.api.net import TFDataset\nfrom tensorflow as tf\n\nsc = init_nncontext()\n\n# Each record in the train_rdd consists of a list of NumPy ndrrays\ntesting_rdd = sc.parallelize(file_list)\n  .map(lambda x: read_image_and_label(x))\n  .map(lambda image_label: decode_to_ndarrays(image_label))\n\n# TFDataset represents a distributed set of elements,\n# in which each element contains one or more Tensorflow Tensor objects. \ndataset = TFDataset.from_rdd(testing_rdd,\n                             names=[ features ],\n                             shapes=[[28, 28, 1]],\n                             types=[tf.float32])  2.Reconstruct the model for inference and load the checkpoint  import tensorflow as tf\n\nslim = tf.contrib.slim\n\nimages, labels = dataset.tensors\nwith slim.arg_scope(lenet.lenet_arg_scope()):\n     logits, end_points = lenet.lenet(images, num_classes=10, is_training=False)\n\nsess = tf.Session()\nsaver = tf.train.Saver()\nsaver.restore(sess,  /tmp/lenet )  3.Run predictions  predictor = TFPredictor(sess, [logits])\npredictions_rdd = predictor.predict()", 
            "title": "Inference"
        }, 
        {
            "location": "/ProgrammingGuide/tensorflow/#relationship-to-tfnet", 
            "text": "TFNet  is a layer representing a TensorFlow sub-graph (specified by the input and output TensorFlow tensors).\nIt implements the standard BigDL layer API, and can be used with other Analytics-Zoo/BigDL layers\nto construct more complex models for training or inference using the standard Analytics-Zoo/BigDL API.   You can think of  TFDatasets ,  TFOptimizer ,  TFPredictor  as a set API for training/testing TensorFlow models\non Spark/BigDL, while  TFNet  as an Analytics-Zoo/BigDL layer initialized using TensorFlow graph.  For more information on TFNet, please refer to the  API Guide", 
            "title": "Relationship to TFNet"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/", 
            "text": "Analytics Zoo provides supports for end-to-end image processing pipeline, including image loading, pre-processing, inference/training and some utilities on different formats.\n\n\nLoad Image\n\n\nAnalytics Zoo provides APIs to read image to different formats:\n\n\nLoad to Data Frame\n\n\nAnalytics Zoo can process image data as Spark Data Frame.\n\nNNImageReader\n is the primary DataFrame-based image loading interface to read images into DataFrame.\n\n\nScala example:\n\n\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.pipeline.nnframes.NNImageReader\n\nval sc = NNContext.initNNContext(\napp\n)\nval imageDF1 = NNImageReader.readImages(\n/tmp\n, sc)\nval imageDF2 = NNImageReader.readImages(\n/tmp/*.jpg\n, sc)\nval imageDF3 = NNImageReader.readImages(\n/tmp/a.jpg, /tmp/b.jpg\n, sc)\n\n\n\n\n\nPython:\n\n\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.nnframes import *\n\nsc = init_nncontext(\napp\n)\nimageDF1 = NNImageReader.readImages(\n/tmp\n, sc)\nimageDF2 = NNImageReader.readImages(\n/tmp/*.jpg\n, sc)\nimageDF3 = NNImageReader.readImages(\n/tmp/a.jpg, /tmp/b.jpg\n, sc)\n\n\n\n\nThe output DataFrame contains a sinlge column named \"image\". The schema of \"image\" column can be\naccessed from \ncom.intel.analytics.zoo.pipeline.nnframes.DLImageSchema.byteSchema\n.\nEach record in \"image\" column represents one image record, in the format of\nRow(origin, height, width, num of channels, mode, data), where origin contains the URI for the image file,\nand \ndata\n holds the original file bytes for the image file. \nmode\n represents the OpenCV-compatible\ntype: CV_8UC3, CV_8UC1 in most cases.\n\n\n  val byteSchema = StructType(\n    StructField(\norigin\n, StringType, true) ::\n      StructField(\nheight\n, IntegerType, false) ::\n      StructField(\nwidth\n, IntegerType, false) ::\n      StructField(\nnChannels\n, IntegerType, false) ::\n      // OpenCV-compatible type: CV_8UC3, CV_32FC3 in most cases\n      StructField(\nmode\n, IntegerType, false) ::\n      // Bytes in OpenCV-compatible order: row-wise BGR in most cases\n      StructField(\ndata\n, BinaryType, false) :: Nil)\n\n\n\n\nAfter loading the image, user can compose the preprocess steps with the \nPreprocessing\n defined\nin \ncom.intel.analytics.zoo.feature.image\n.\n\n\nLoad to ImageSet\n\n\nImageSet\n is a collection of \nImageFeature\n. It can be a \nDistributedImageSet\n for distributed image RDD or\n \nLocalImageSet\n for local image array.\nYou can read image data as \nImageSet\n from local/distributed image path, or you can directly construct a ImageSet from RDD[ImageFeature] or Array[ImageFeature].\n\n\nScala example:\n\n\n// create LocalImageSet from an image folder\nval localImageSet = ImageSet.read(\n/tmp/image/\n)\n\n// create DistributedImageSet from an image folder\nval distributedImageSet2 = ImageSet.read(\n/tmp/image/\n, sc, 2)\n\n\n\n\nPython example:\n\n\n# create LocalImageSet from an image folder\nlocal_image_frame2 = ImageSet.read(\n/tmp/image/\n)\n\n# create DistributedImageSet from an image folder\ndistributed_image_frame = ImageSet.read(\n/tmp/image/\n, sc, 2)\n\n\n\n\nImage Transformer\n\n\nAnalytics Zoo has many pre-defined image processing transformers built on top of OpenCV:\n\n\n\n\nImageBrightness\n: Adjust the image brightness.\n\n\nImageHue\n: Adjust the image hue.\n\n\nImageSaturation\n: Adjust the image Saturation.\n\n\nImageContrast\n: Adjust the image Contrast.\n\n\nImageChannelOrder\n: Random change the channel order of an image\n\n\nImageColorJitter\n: Random adjust brightness, contrast, hue, saturation\n\n\nImageResize\n: Resize image\n\n\nImageAspectScale\n: Resize the image, keep the aspect ratio. scale according to the short edge\n\n\nImageRandomAspectScale\n: Resize the image by randomly choosing a scale\n\n\nImageChannelNormalize\n: Image channel normalize\n\n\nImagePixelNormalizer\n: Pixel level normalizer\n\n\nImageCenterCrop\n: Crop a \ncropWidth\n x \ncropHeight\n patch from center of image.\n\n\nImageRandomCrop\n: Random crop a \ncropWidth\n x \ncropHeight\n patch from an image.\n\n\nImageFixedCrop\n: Crop a fixed area of image\n\n\nImageDetectionCrop\n: Crop from object detections, each image should has a tensor detection,\n\n\nImageExpand\n: Expand image, fill the blank part with the meanR, meanG, meanB\n\n\nImageFiller\n: Fill part of image with certain pixel value\n\n\nImageHFlip\n: Flip the image horizontally\n\n\nImageRandomTransformer\n: It is a wrapper for transformers to control the transform probability\n\n\nImageBytesToMat\n: Transform byte array(original image file in byte) to OpenCVMat\n\n\nImageMatToFloats\n: Transform OpenCVMat to float array, note that in this transformer, the mat is released.\n\n\nImageMatToTensor\n: Transform opencv mat to tensor, note that in this transformer, the mat is released.\n\n\nImageSetToSample\n: Transforms tensors that map inputKeys and targetKeys to sample, note that in this transformer, the mat has been released.\n\n\n\n\nMore examples can be found \nhere\n\n\nYou can also define your own Transformer by extending \nImageProcessing\n,\nand override the function \ntransformMat\n to do the actual transformation to \nImageFeature\n.\n\n\nBuild Image Transformation Pipeline\n\n\nYou can easily build the image transformation pipeline by chaining transformers.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.zoo.feature.image._\n\n\nval imgAug = ImageBytesToMat() -\n ImageResize(256, 256)-\n ImageCenterCrop(224, 224) -\n\n             ImageChannelNormalize(123, 117, 104) -\n\n             ImageMatToTensor[Float]() -\n\n             ImageSetToSample[Float]()\n\n\n\n\nIn the above example, the transformations will perform sequentially.\n\n\nAssume you have an ImageSet containing original bytes array,\n\n\n\n\n\n\nImageBytesToMat\n will transform the bytes array to \nOpenCVMat\n.\n\n\n\n\n\n\nImageColorJitter\n, \nImageExpand\n, \nImageResize\n, \nImageHFlip\n and \nImageChannelNormalize\n will transform over \nOpenCVMat\n,\nnote that \nOpenCVMat\n is overwrite by default.\n\n\n\n\n\n\nImageMatToTensor\n transform \nOpenCVMat\n to \nTensor\n, and \nOpenCVMat\n is released in this step.\n\n\n\n\n\n\nImageSetToSample\n transform the tensors that map inputKeys and targetKeys to sample,\nwhich can be used by the following prediction or training tasks.\n\n\n\n\n\n\nPython example:\n\n\nfrom zoo.feature.image.imagePreprocessing import *\n\nimg_aug = ChainedPreprocessing([ImageBytesToMat(),\n      ImageColorJitter(),\n      ImageExpand(),\n      ImageResize(300, 300, -1),\n      ImageHFlip(),\n      ImageChannelNormalize(123.0, 117.0, 104.0),\n      ImageMatToTensor(),\n      ImageSetToSample()])\n\n\n\n\nImage Train\n\n\nTrain with Image DataFrame\n\n\nYou can use NNEstimator/NNCLassifier to train Zoo Keras/BigDL model with Image DataFrame. You can pass in image preprocessing to NNEstimator/NNClassifier to do image preprocessing before training. Then call \nfit\n method to let Analytics Zoo train the model\n\n\nFor detail APIs, please refer to: \nNNFrames\n\n\nScala example:\n\n\nval batchsize = 128\nval nEpochs = 10\nval featureTransformer = RowToImageFeature() -\n ImageResize(256, 256) -\n\n                                   ImageCenterCrop(224, 224) -\n\n                                   ImageChannelNormalize(123, 117, 104) -\n\n                                   ImageMatToTensor() -\n\n                                   ImageFeatureToTensor()\nval classifier = NNClassifier(model, CrossEntropyCriterion[Float](), featureTransformer)\n        .setFeaturesCol(\nimage\n)\n        .setLearningRate(0.003)\n        .setBatchSize(batchsize)\n        .setMaxEpoch(nEpochs)\n        .setValidation(Trigger.everyEpoch, valDf, Array(new Top1Accuracy()), batchsize)\nval trainedModel = classifier.fit(trainDf)\n\n\n\n\nPython example:\n\n\nbatchsize = 128\nnEpochs = 10\nfeatureTransformer = ChainedPreprocessing([RowToImageFeature(), ImageResize(256, 256),\n                                   ImageCenterCrop(224, 224),\n                                   ImageChannelNormalize(123, 117, 104),\n                                   ImageMatToTensor(),\n                                   ImageFeatureToTensor()])\nclassifier = NNClassifier(model, CrossEntropyCriterion(), featureTransformer)\\\n        .setFeaturesCol(\nimage\n)\\\n        .setLearningRate(0.003)\\\n        .setBatchSize(batchsize)\\\n        .setMaxEpoch(nEpochs)\\\n        .setValidation(EveryEpoch(), valDf, [Top1Accuracy()], batch_size)\ntrainedModel = classifier.fit(trainDf)\n\n\n\n\nTrain with ImageSet\n\n\nYou can train Zoo Keras model with ImageSet. Just call \nfit\n method to let Analytics Zoo train the model.\n\n\nPython example:\n\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.common import *\nfrom zoo.feature.image.imagePreprocessing import *\nfrom zoo.pipeline.api.keras.layers import Dense, Input, Flatten\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.net import *\nfrom bigdl.optim.optimizer import *\n\nsc = init_nncontext(\ntrain keras\n)\nimg_path=\n/tmp/image\n\nimage_set = ImageSet.read(img_path,sc, min_partitions=1)\ntransformer = ChainedPreprocessing(\n        [ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n         ImageSetToSample()])\nimage_data = transformer(image_set)\nlabels = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\nlabel_rdd = sc.parallelize(labels, 1)\nsamples = image_data.get_image().zip(label_rdd).map(\n        lambda tuple: Sample.from_ndarray(tuple[0], tuple[1]))\n# create model\nmodel_path=\n/tmp/bigdl_inception-v1_imagenet_0.4.0.model\n\nfull_model = Net.load_bigdl(model_path)\n# create a new model by remove layers after pool5/drop_7x7_s1\nmodel = full_model.new_graph([\npool5/drop_7x7_s1\n])\n# freeze layers from input to pool4/3x3_s2 inclusive\nmodel.freeze_up_to([\npool4/3x3_s2\n])\n\ninputNode = Input(name=\ninput\n, shape=(3, 224, 224))\ninception = model.to_keras()(inputNode)\nflatten = Flatten()(inception)\nlogits = Dense(2)(flatten)\nlrModel = Model(inputNode, logits)\n\nbatchsize = 4\nnEpochs = 10\nlrModel.compile(optimizer=Adam(learningrate=1e-4),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\nlrModel.fit(x = samples, batch_size=batchsize, nb_epoch=nEpochs)\n\n\n\n\nImage Predict\n\n\nPredict with Image DataFrame\n\n\nAfter training with \nNNEstimator/NNCLassifier\n, you'll get a trained \nNNModel/NNClassifierModel\n . You can call \ntransform\n to predict Image DataFrame with this \nNNModel/NNClassifierModel\n . Or you can load pre-trained \nAnalytics-Zoo/BigDL/Caffe/Torch/Tensorflow\n  model and create \nNNModel/NNClassifierModel\n with this model. Then call to \ntransform\n to Image DataFrame.\n\n\nAfter prediction, there is a new column \nprediction\n in the prediction image dataframe.\n\n\nScala example:\n\n\n val batchsize = 128\n val nEpochs = 10\n val featureTransformer = RowToImageFeature() -\n ImageResize(256, 256) -\n\n                                    ImageCenterCrop(224, 224) -\n\n                                    ImageChannelNormalize(123, 117, 104) -\n\n                                    ImageMatToTensor() -\n\n                                    ImageFeatureToTensor()\n val classifier = NNClassifier(model, CrossEntropyCriterion[Float](), featureTransformer)\n         .setFeaturesCol(\nimage\n)\n         .setLearningRate(0.003)\n         .setBatchSize(batchsize)\n         .setMaxEpoch(nEpochs)\n         .setValidation(Trigger.everyEpoch, valDf, Array(new Top1Accuracy()), batchsize)\n val trainedModel = classifier.fit(trainDf)\n // predict with trained model\n val predictions = trainedModel.transform(testDf)\n predictions.select(col(\nimage\n), col(\nlabel\n), col(\nprediction\n)).show(false)\n\n // predict with loaded pre-trained model\n val model = Module.loadModule[Float](modelPath)\n val dlmodel = NNClassifierModel(model, featureTransformer)\n         .setBatchSize(batchsize)\n         .setFeaturesCol(\nimage\n)\n         .setPredictionCol(\nprediction\n) \n val resultDF = dlmodel.transform(testDf)\n\n\n\n\nPython example:\n\n\n batchsize = 128\n nEpochs = 10\n featureTransformer = ChainedPreprocessing([RowToImageFeature(), ImageResize(256, 256),\n                                    ImageCenterCrop(224, 224),\n                                    ImageChannelNormalize(123, 117, 104),\n                                    ImageMatToTensor(),\n                                    ImageFeatureToTensor()])\n classifier = NNClassifier(model, CrossEntropyCriterion(), featureTransformer)\\\n         .setFeaturesCol(\nimage\n)\\\n         .setLearningRate(0.003)\\\n         .setBatchSize(batchsize)\\\n         .setMaxEpoch(nEpochs)\\\n         .setValidation(EveryEpoch(), valDf, [Top1Accuracy()], batch_size)\ntrainedModel = classifier.fit(trainDf)\n# predict with trained model\npredictions = trainedModel.transform(testDf)\npredictions.select(\nimage\n, \nlabel\n,\nprediction\n).show(False)\n\n# predict with loaded pre-trained model\nmodel = Model.loadModel(model_path)\ndlmodel = NNClassifierModel(model, featureTransformer)\\\n         .setBatchSize(batchsize)\\\n         .setFeaturesCol(\nimage\n)\\\n         .setPredictionCol(\nprediction\n) \nresultDF = dlmodel.transform(testDf)\n\n\n\n\nPredict with ImageSet\n\n\nAfter training Zoo Keras model, you can call \npredict\n to predict ImageSet.\nOr you can load pre-trained Analytics-Zoo/BigDL model. Then call to \npredictImageSet\n to predict ImageSet.\n\n\nPredict with trained Zoo Keras Model\n\n\nPython example:\n\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.common import *\nfrom zoo.feature.image.imagePreprocessing import *\nfrom zoo.pipeline.api.keras.layers import Dense, Input, Flatten\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.net import *\nfrom bigdl.optim.optimizer import *\n\nsc = init_nncontext(\ntrain keras\n)\nimg_path=\n/tmp/image\n\nimage_set = ImageSet.read(img_path,sc, min_partitions=1)\ntransformer = ChainedPreprocessing(\n        [ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n         ImageSetToSample()])\nimage_data = transformer(image_set)\nlabels = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\nlabel_rdd = sc.parallelize(labels, 1)\nsamples = image_data.get_image().zip(label_rdd).map(\n        lambda tuple: Sample.from_ndarray(tuple[0], tuple[1]))\n# create model\nmodel_path=\n/tmp/bigdl_inception-v1_imagenet_0.4.0.model\n\nfull_model = Net.load_bigdl(model_path)\n# create a new model by remove layers after pool5/drop_7x7_s1\nmodel = full_model.new_graph([\npool5/drop_7x7_s1\n])\n# freeze layers from input to pool4/3x3_s2 inclusive\nmodel.freeze_up_to([\npool4/3x3_s2\n])\n\ninputNode = Input(name=\ninput\n, shape=(3, 224, 224))\ninception = model.to_keras()(inputNode)\nflatten = Flatten()(inception)\nlogits = Dense(2)(flatten)\nlrModel = Model(inputNode, logits)\n\nbatchsize = 4\nnEpochs = 10\nlrModel.compile(optimizer=Adam(learningrate=1e-4),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\nlrModel.fit(x = samples, batch_size=batchsize, nb_epoch=nEpochs)\nprediction = lrModel.predict(samples)\nresult = prediction.collect()\n\n\n\n\nPredict with loaded Model\n\n\nYou can load pre-trained Analytics-Zoo/BigDL model. Then call to \npredictImageSet\n to predict ImageSet.\n\n\nFor details, you can check guide of \nimage classificaion\n or \nobject detection\n\n\n3D Image Support\n\n\nFor 3D images, we can support above operations based on ImageSet. For details, please refer to \nimage API guide", 
            "title": "Working with Images"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#load-image", 
            "text": "Analytics Zoo provides APIs to read image to different formats:", 
            "title": "Load Image"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#load-to-data-frame", 
            "text": "Analytics Zoo can process image data as Spark Data Frame. NNImageReader  is the primary DataFrame-based image loading interface to read images into DataFrame.  Scala example:  import com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.pipeline.nnframes.NNImageReader\n\nval sc = NNContext.initNNContext( app )\nval imageDF1 = NNImageReader.readImages( /tmp , sc)\nval imageDF2 = NNImageReader.readImages( /tmp/*.jpg , sc)\nval imageDF3 = NNImageReader.readImages( /tmp/a.jpg, /tmp/b.jpg , sc)  Python:  from zoo.common.nncontext import *\nfrom zoo.pipeline.nnframes import *\n\nsc = init_nncontext( app )\nimageDF1 = NNImageReader.readImages( /tmp , sc)\nimageDF2 = NNImageReader.readImages( /tmp/*.jpg , sc)\nimageDF3 = NNImageReader.readImages( /tmp/a.jpg, /tmp/b.jpg , sc)  The output DataFrame contains a sinlge column named \"image\". The schema of \"image\" column can be\naccessed from  com.intel.analytics.zoo.pipeline.nnframes.DLImageSchema.byteSchema .\nEach record in \"image\" column represents one image record, in the format of\nRow(origin, height, width, num of channels, mode, data), where origin contains the URI for the image file,\nand  data  holds the original file bytes for the image file.  mode  represents the OpenCV-compatible\ntype: CV_8UC3, CV_8UC1 in most cases.    val byteSchema = StructType(\n    StructField( origin , StringType, true) ::\n      StructField( height , IntegerType, false) ::\n      StructField( width , IntegerType, false) ::\n      StructField( nChannels , IntegerType, false) ::\n      // OpenCV-compatible type: CV_8UC3, CV_32FC3 in most cases\n      StructField( mode , IntegerType, false) ::\n      // Bytes in OpenCV-compatible order: row-wise BGR in most cases\n      StructField( data , BinaryType, false) :: Nil)  After loading the image, user can compose the preprocess steps with the  Preprocessing  defined\nin  com.intel.analytics.zoo.feature.image .", 
            "title": "Load to Data Frame"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#load-to-imageset", 
            "text": "ImageSet  is a collection of  ImageFeature . It can be a  DistributedImageSet  for distributed image RDD or\n  LocalImageSet  for local image array.\nYou can read image data as  ImageSet  from local/distributed image path, or you can directly construct a ImageSet from RDD[ImageFeature] or Array[ImageFeature].  Scala example:  // create LocalImageSet from an image folder\nval localImageSet = ImageSet.read( /tmp/image/ )\n\n// create DistributedImageSet from an image folder\nval distributedImageSet2 = ImageSet.read( /tmp/image/ , sc, 2)  Python example:  # create LocalImageSet from an image folder\nlocal_image_frame2 = ImageSet.read( /tmp/image/ )\n\n# create DistributedImageSet from an image folder\ndistributed_image_frame = ImageSet.read( /tmp/image/ , sc, 2)", 
            "title": "Load to ImageSet"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#image-transformer", 
            "text": "Analytics Zoo has many pre-defined image processing transformers built on top of OpenCV:   ImageBrightness : Adjust the image brightness.  ImageHue : Adjust the image hue.  ImageSaturation : Adjust the image Saturation.  ImageContrast : Adjust the image Contrast.  ImageChannelOrder : Random change the channel order of an image  ImageColorJitter : Random adjust brightness, contrast, hue, saturation  ImageResize : Resize image  ImageAspectScale : Resize the image, keep the aspect ratio. scale according to the short edge  ImageRandomAspectScale : Resize the image by randomly choosing a scale  ImageChannelNormalize : Image channel normalize  ImagePixelNormalizer : Pixel level normalizer  ImageCenterCrop : Crop a  cropWidth  x  cropHeight  patch from center of image.  ImageRandomCrop : Random crop a  cropWidth  x  cropHeight  patch from an image.  ImageFixedCrop : Crop a fixed area of image  ImageDetectionCrop : Crop from object detections, each image should has a tensor detection,  ImageExpand : Expand image, fill the blank part with the meanR, meanG, meanB  ImageFiller : Fill part of image with certain pixel value  ImageHFlip : Flip the image horizontally  ImageRandomTransformer : It is a wrapper for transformers to control the transform probability  ImageBytesToMat : Transform byte array(original image file in byte) to OpenCVMat  ImageMatToFloats : Transform OpenCVMat to float array, note that in this transformer, the mat is released.  ImageMatToTensor : Transform opencv mat to tensor, note that in this transformer, the mat is released.  ImageSetToSample : Transforms tensors that map inputKeys and targetKeys to sample, note that in this transformer, the mat has been released.   More examples can be found  here  You can also define your own Transformer by extending  ImageProcessing ,\nand override the function  transformMat  to do the actual transformation to  ImageFeature .", 
            "title": "Image Transformer"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#build-image-transformation-pipeline", 
            "text": "You can easily build the image transformation pipeline by chaining transformers.  Scala example:  import com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.zoo.feature.image._\n\n\nval imgAug = ImageBytesToMat() -  ImageResize(256, 256)-  ImageCenterCrop(224, 224) - \n             ImageChannelNormalize(123, 117, 104) - \n             ImageMatToTensor[Float]() - \n             ImageSetToSample[Float]()  In the above example, the transformations will perform sequentially.  Assume you have an ImageSet containing original bytes array,    ImageBytesToMat  will transform the bytes array to  OpenCVMat .    ImageColorJitter ,  ImageExpand ,  ImageResize ,  ImageHFlip  and  ImageChannelNormalize  will transform over  OpenCVMat ,\nnote that  OpenCVMat  is overwrite by default.    ImageMatToTensor  transform  OpenCVMat  to  Tensor , and  OpenCVMat  is released in this step.    ImageSetToSample  transform the tensors that map inputKeys and targetKeys to sample,\nwhich can be used by the following prediction or training tasks.    Python example:  from zoo.feature.image.imagePreprocessing import *\n\nimg_aug = ChainedPreprocessing([ImageBytesToMat(),\n      ImageColorJitter(),\n      ImageExpand(),\n      ImageResize(300, 300, -1),\n      ImageHFlip(),\n      ImageChannelNormalize(123.0, 117.0, 104.0),\n      ImageMatToTensor(),\n      ImageSetToSample()])", 
            "title": "Build Image Transformation Pipeline"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#image-train", 
            "text": "", 
            "title": "Image Train"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#train-with-image-dataframe", 
            "text": "You can use NNEstimator/NNCLassifier to train Zoo Keras/BigDL model with Image DataFrame. You can pass in image preprocessing to NNEstimator/NNClassifier to do image preprocessing before training. Then call  fit  method to let Analytics Zoo train the model  For detail APIs, please refer to:  NNFrames  Scala example:  val batchsize = 128\nval nEpochs = 10\nval featureTransformer = RowToImageFeature() -  ImageResize(256, 256) - \n                                   ImageCenterCrop(224, 224) - \n                                   ImageChannelNormalize(123, 117, 104) - \n                                   ImageMatToTensor() - \n                                   ImageFeatureToTensor()\nval classifier = NNClassifier(model, CrossEntropyCriterion[Float](), featureTransformer)\n        .setFeaturesCol( image )\n        .setLearningRate(0.003)\n        .setBatchSize(batchsize)\n        .setMaxEpoch(nEpochs)\n        .setValidation(Trigger.everyEpoch, valDf, Array(new Top1Accuracy()), batchsize)\nval trainedModel = classifier.fit(trainDf)  Python example:  batchsize = 128\nnEpochs = 10\nfeatureTransformer = ChainedPreprocessing([RowToImageFeature(), ImageResize(256, 256),\n                                   ImageCenterCrop(224, 224),\n                                   ImageChannelNormalize(123, 117, 104),\n                                   ImageMatToTensor(),\n                                   ImageFeatureToTensor()])\nclassifier = NNClassifier(model, CrossEntropyCriterion(), featureTransformer)\\\n        .setFeaturesCol( image )\\\n        .setLearningRate(0.003)\\\n        .setBatchSize(batchsize)\\\n        .setMaxEpoch(nEpochs)\\\n        .setValidation(EveryEpoch(), valDf, [Top1Accuracy()], batch_size)\ntrainedModel = classifier.fit(trainDf)", 
            "title": "Train with Image DataFrame"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#train-with-imageset", 
            "text": "You can train Zoo Keras model with ImageSet. Just call  fit  method to let Analytics Zoo train the model.  Python example:  from zoo.common.nncontext import *\nfrom zoo.feature.common import *\nfrom zoo.feature.image.imagePreprocessing import *\nfrom zoo.pipeline.api.keras.layers import Dense, Input, Flatten\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.net import *\nfrom bigdl.optim.optimizer import *\n\nsc = init_nncontext( train keras )\nimg_path= /tmp/image \nimage_set = ImageSet.read(img_path,sc, min_partitions=1)\ntransformer = ChainedPreprocessing(\n        [ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n         ImageSetToSample()])\nimage_data = transformer(image_set)\nlabels = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\nlabel_rdd = sc.parallelize(labels, 1)\nsamples = image_data.get_image().zip(label_rdd).map(\n        lambda tuple: Sample.from_ndarray(tuple[0], tuple[1]))\n# create model\nmodel_path= /tmp/bigdl_inception-v1_imagenet_0.4.0.model \nfull_model = Net.load_bigdl(model_path)\n# create a new model by remove layers after pool5/drop_7x7_s1\nmodel = full_model.new_graph([ pool5/drop_7x7_s1 ])\n# freeze layers from input to pool4/3x3_s2 inclusive\nmodel.freeze_up_to([ pool4/3x3_s2 ])\n\ninputNode = Input(name= input , shape=(3, 224, 224))\ninception = model.to_keras()(inputNode)\nflatten = Flatten()(inception)\nlogits = Dense(2)(flatten)\nlrModel = Model(inputNode, logits)\n\nbatchsize = 4\nnEpochs = 10\nlrModel.compile(optimizer=Adam(learningrate=1e-4),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\nlrModel.fit(x = samples, batch_size=batchsize, nb_epoch=nEpochs)", 
            "title": "Train with ImageSet"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#image-predict", 
            "text": "", 
            "title": "Image Predict"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#predict-with-image-dataframe", 
            "text": "After training with  NNEstimator/NNCLassifier , you'll get a trained  NNModel/NNClassifierModel  . You can call  transform  to predict Image DataFrame with this  NNModel/NNClassifierModel  . Or you can load pre-trained  Analytics-Zoo/BigDL/Caffe/Torch/Tensorflow   model and create  NNModel/NNClassifierModel  with this model. Then call to  transform  to Image DataFrame.  After prediction, there is a new column  prediction  in the prediction image dataframe.  Scala example:   val batchsize = 128\n val nEpochs = 10\n val featureTransformer = RowToImageFeature() -  ImageResize(256, 256) - \n                                    ImageCenterCrop(224, 224) - \n                                    ImageChannelNormalize(123, 117, 104) - \n                                    ImageMatToTensor() - \n                                    ImageFeatureToTensor()\n val classifier = NNClassifier(model, CrossEntropyCriterion[Float](), featureTransformer)\n         .setFeaturesCol( image )\n         .setLearningRate(0.003)\n         .setBatchSize(batchsize)\n         .setMaxEpoch(nEpochs)\n         .setValidation(Trigger.everyEpoch, valDf, Array(new Top1Accuracy()), batchsize)\n val trainedModel = classifier.fit(trainDf)\n // predict with trained model\n val predictions = trainedModel.transform(testDf)\n predictions.select(col( image ), col( label ), col( prediction )).show(false)\n\n // predict with loaded pre-trained model\n val model = Module.loadModule[Float](modelPath)\n val dlmodel = NNClassifierModel(model, featureTransformer)\n         .setBatchSize(batchsize)\n         .setFeaturesCol( image )\n         .setPredictionCol( prediction ) \n val resultDF = dlmodel.transform(testDf)  Python example:   batchsize = 128\n nEpochs = 10\n featureTransformer = ChainedPreprocessing([RowToImageFeature(), ImageResize(256, 256),\n                                    ImageCenterCrop(224, 224),\n                                    ImageChannelNormalize(123, 117, 104),\n                                    ImageMatToTensor(),\n                                    ImageFeatureToTensor()])\n classifier = NNClassifier(model, CrossEntropyCriterion(), featureTransformer)\\\n         .setFeaturesCol( image )\\\n         .setLearningRate(0.003)\\\n         .setBatchSize(batchsize)\\\n         .setMaxEpoch(nEpochs)\\\n         .setValidation(EveryEpoch(), valDf, [Top1Accuracy()], batch_size)\ntrainedModel = classifier.fit(trainDf)\n# predict with trained model\npredictions = trainedModel.transform(testDf)\npredictions.select( image ,  label , prediction ).show(False)\n\n# predict with loaded pre-trained model\nmodel = Model.loadModel(model_path)\ndlmodel = NNClassifierModel(model, featureTransformer)\\\n         .setBatchSize(batchsize)\\\n         .setFeaturesCol( image )\\\n         .setPredictionCol( prediction ) \nresultDF = dlmodel.transform(testDf)", 
            "title": "Predict with Image DataFrame"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#predict-with-imageset", 
            "text": "After training Zoo Keras model, you can call  predict  to predict ImageSet.\nOr you can load pre-trained Analytics-Zoo/BigDL model. Then call to  predictImageSet  to predict ImageSet.", 
            "title": "Predict with ImageSet"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#predict-with-trained-zoo-keras-model", 
            "text": "Python example:  from zoo.common.nncontext import *\nfrom zoo.feature.common import *\nfrom zoo.feature.image.imagePreprocessing import *\nfrom zoo.pipeline.api.keras.layers import Dense, Input, Flatten\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.net import *\nfrom bigdl.optim.optimizer import *\n\nsc = init_nncontext( train keras )\nimg_path= /tmp/image \nimage_set = ImageSet.read(img_path,sc, min_partitions=1)\ntransformer = ChainedPreprocessing(\n        [ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n         ImageSetToSample()])\nimage_data = transformer(image_set)\nlabels = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\nlabel_rdd = sc.parallelize(labels, 1)\nsamples = image_data.get_image().zip(label_rdd).map(\n        lambda tuple: Sample.from_ndarray(tuple[0], tuple[1]))\n# create model\nmodel_path= /tmp/bigdl_inception-v1_imagenet_0.4.0.model \nfull_model = Net.load_bigdl(model_path)\n# create a new model by remove layers after pool5/drop_7x7_s1\nmodel = full_model.new_graph([ pool5/drop_7x7_s1 ])\n# freeze layers from input to pool4/3x3_s2 inclusive\nmodel.freeze_up_to([ pool4/3x3_s2 ])\n\ninputNode = Input(name= input , shape=(3, 224, 224))\ninception = model.to_keras()(inputNode)\nflatten = Flatten()(inception)\nlogits = Dense(2)(flatten)\nlrModel = Model(inputNode, logits)\n\nbatchsize = 4\nnEpochs = 10\nlrModel.compile(optimizer=Adam(learningrate=1e-4),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\nlrModel.fit(x = samples, batch_size=batchsize, nb_epoch=nEpochs)\nprediction = lrModel.predict(samples)\nresult = prediction.collect()", 
            "title": "Predict with trained Zoo Keras Model"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#predict-with-loaded-model", 
            "text": "You can load pre-trained Analytics-Zoo/BigDL model. Then call to  predictImageSet  to predict ImageSet.  For details, you can check guide of  image classificaion  or  object detection", 
            "title": "Predict with loaded Model"
        }, 
        {
            "location": "/ProgrammingGuide/workingwithimages/#3d-image-support", 
            "text": "For 3D images, we can support above operations based on ImageSet. For details, please refer to  image API guide", 
            "title": "3D Image Support"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/", 
            "text": "Analytics Zoo provides a collection of pre-trained models for Object Detection. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios, users can embed the models locally, distributedly in Apache Spark, Apache Storm or Apache Flink.\n\n\nObject Detection examples\n\n\nAnalytics Zoo provides two typical kind of pre-trained Object Detection models : \nSSD\n and \nFaster-RCNN\n on dataset \nPASCAL\n and \nCOCO\n. For the usage of these models, please check below examples.\n\n\nScala\n\n\nScala example\n\n\nIt's very easy to apply the model for inference with below code piece.\n\n\nval model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval output = model.predictImageSet(data)\n\n\n\n\nFor preprocessors for Object Detection models, please check \nObject Detection Config\n\n\nNote: We expect the loaded images has 3 channels. If the channel is not 3(eg, gray/png images), please set \nimageCodec\n when loading images \nImageSet.read\n. See https://analytics-zoo.github.io/0.1.0/#ProgrammingGuide/object-detection/#object-detection-examples\n\n\nUsers can also do the inference directly using Analytics zoo.\nSample code for SSD VGG on PASCAL as below:\n\n\nval model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval preprocessor = Resize(300, 300) -\n\n                         ChannelNormalize(123f, 117f, 104f, 1f, 1f, 1f) -\n\n                         MatToTensor() -\n ImageFrameToSample()\nval output = model.predictImageset(data)\n\n\n\n\nPython\n\n\nPython example\n\n\nIt's very easy to apply the model for inference with below code piece.\n\n\nmodel = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = model.predict_image_set(image_set)\n\n\n\n\nUser can also define his own configuration to do the inference with below code piece.\n\n\nmodel = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(preprocessing)\noutput = model.predict_image_set(image_set)\n\n\n\n\nFor preprocessors for Object Detection models, please check \nObject Detection Config\n\n\nDownload link\n\n\nPASCAL VOC models\n\n\n\n\nSSD 300x300 MobileNet\n\n\nSSD 300x300 VGG\n\n\nSSD 300x300 VGG Quantize\n\n\nSSD 512x512 VGG\n\n\nSSD 512x512 VGG Quantize\n\n\nFaster-RCNN VGG\n\n\nFaster-RCNN VGG Compress\n\n\nFaster-RCNN VGG Compress Quantize\n\n\nFaster-RCNN PvaNet\n\n\nFaster-RCNN PvaNet Compress\n\n\nFaster-RCNN PvaNet Compress Quantize\n\n\n\n\nCOCO models\n\n\n\n\nSSD 300x300 VGG\n\n\nSSD 300x300 VGG Quantize\n\n\nSSD 512x512 VGG\n\n\nSSD 512x512 VGG Quantize", 
            "title": "Object Detection API"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/#object-detection-examples", 
            "text": "Analytics Zoo provides two typical kind of pre-trained Object Detection models :  SSD  and  Faster-RCNN  on dataset  PASCAL  and  COCO . For the usage of these models, please check below examples.  Scala  Scala example  It's very easy to apply the model for inference with below code piece.  val model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval output = model.predictImageSet(data)  For preprocessors for Object Detection models, please check  Object Detection Config  Note: We expect the loaded images has 3 channels. If the channel is not 3(eg, gray/png images), please set  imageCodec  when loading images  ImageSet.read . See https://analytics-zoo.github.io/0.1.0/#ProgrammingGuide/object-detection/#object-detection-examples  Users can also do the inference directly using Analytics zoo.\nSample code for SSD VGG on PASCAL as below:  val model = ObjectDetector.load[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval preprocessor = Resize(300, 300) - \n                         ChannelNormalize(123f, 117f, 104f, 1f, 1f, 1f) - \n                         MatToTensor() -  ImageFrameToSample()\nval output = model.predictImageset(data)  Python  Python example  It's very easy to apply the model for inference with below code piece.  model = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = model.predict_image_set(image_set)  User can also define his own configuration to do the inference with below code piece.  model = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(preprocessing)\noutput = model.predict_image_set(image_set)  For preprocessors for Object Detection models, please check  Object Detection Config", 
            "title": "Object Detection examples"
        }, 
        {
            "location": "/ProgrammingGuide/object-detection/#download-link", 
            "text": "PASCAL VOC models   SSD 300x300 MobileNet  SSD 300x300 VGG  SSD 300x300 VGG Quantize  SSD 512x512 VGG  SSD 512x512 VGG Quantize  Faster-RCNN VGG  Faster-RCNN VGG Compress  Faster-RCNN VGG Compress Quantize  Faster-RCNN PvaNet  Faster-RCNN PvaNet Compress  Faster-RCNN PvaNet Compress Quantize   COCO models   SSD 300x300 VGG  SSD 300x300 VGG Quantize  SSD 512x512 VGG  SSD 512x512 VGG Quantize", 
            "title": "Download link"
        }, 
        {
            "location": "/ProgrammingGuide/image-classification/", 
            "text": "Analytics Zoo provides a collection of pre-trained models for Image Classification. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios, users can embed the models locally, distributedly in Spark such as Apache Storm and Apache Flink.\n\n\nImage Classification examples\n\n\nAnalytics Zoo provides several typical kind of pre-trained Image Classfication models : \nAlexnet\n, \nInception-V1\n, \nVGG\n, \nResnet\n, \nDensenet\n, \nMobilenet\n, \nSqueezenet\n models. To use these models, please check below examples.\n\n\nScala\n\n\nScala example\n\n\nIt's very easy to apply the model for inference with below code piece.\n\n\nval imc = ImageClassifier.loadModel[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval output = imc.predictImageSet(data)\n\n\n\n\nUser can also define his own configuration to do the inference with below code piece.\n\n\nval imc = ImageClassifier.loadModel[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval preprocessing = ImageResize(256, 256)-\n ImageCenterCrop(224, 224) -\n\n        ImageChannelNormalize(123, 117, 104) -\n\n        ImageMatToTensor[Float]() -\n\n        ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preprocessing)        \nval output = imc.predictImageSet(data, config)\n\n\n\n\nPython\n\n\nPython example\n\n\nIt's very easy to apply the model for inference with below code piece.\n\n\nimc = ImageClassifier.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = imc.predict_image_set(image_set)\n\n\n\n\nUser can also define his own configuration to do the inference with below code piece.\n\n\nimc = ImageClassifier.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(preprocessing) \noutput = imc.predict_image_set(image_set)\n\n\n\n\nFor preprocessors for Image Classification models, please check \nImage Classification Config\n\n\nDownload link\n\n\n\n\nAlexnet\n\n\nAlexnet Quantize\n\n\nInception-V1\n\n\nInception-V1 Quantize\n\n\nInception-V3\n\n\nInception-V3 Quantize\n\n\nVGG-16\n\n\nVGG-16 Quantize\n\n\nVGG-19\n\n\nVGG-19 Quantize\n\n\nResnet-50\n\n\nResnet-50 Quantize\n\n\nDensenet-161\n\n\nDensenet-161 Quantize\n\n\nMobilenet\n\n\nMobilenet-V2\n\n\nMobilenet-V2 Quantize\n\n\nSqueezenet\n\n\nSqueezenet Quantize", 
            "title": "Image Classification API"
        }, 
        {
            "location": "/ProgrammingGuide/image-classification/#image-classification-examples", 
            "text": "Analytics Zoo provides several typical kind of pre-trained Image Classfication models :  Alexnet ,  Inception-V1 ,  VGG ,  Resnet ,  Densenet ,  Mobilenet ,  Squeezenet  models. To use these models, please check below examples.  Scala  Scala example  It's very easy to apply the model for inference with below code piece.  val imc = ImageClassifier.loadModel[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval output = imc.predictImageSet(data)  User can also define his own configuration to do the inference with below code piece.  val imc = ImageClassifier.loadModel[Float](params.model)\nval data = ImageSet.read(params.image, sc, params.nPartition)\nval preprocessing = ImageResize(256, 256)-  ImageCenterCrop(224, 224) - \n        ImageChannelNormalize(123, 117, 104) - \n        ImageMatToTensor[Float]() - \n        ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preprocessing)        \nval output = imc.predictImageSet(data, config)  Python  Python example  It's very easy to apply the model for inference with below code piece.  imc = ImageClassifier.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = imc.predict_image_set(image_set)  User can also define his own configuration to do the inference with below code piece.  imc = ImageClassifier.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(preprocessing) \noutput = imc.predict_image_set(image_set)  For preprocessors for Image Classification models, please check  Image Classification Config", 
            "title": "Image Classification examples"
        }, 
        {
            "location": "/ProgrammingGuide/image-classification/#download-link", 
            "text": "Alexnet  Alexnet Quantize  Inception-V1  Inception-V1 Quantize  Inception-V3  Inception-V3 Quantize  VGG-16  VGG-16 Quantize  VGG-19  VGG-19 Quantize  Resnet-50  Resnet-50 Quantize  Densenet-161  Densenet-161 Quantize  Mobilenet  Mobilenet-V2  Mobilenet-V2 Quantize  Squeezenet  Squeezenet Quantize", 
            "title": "Download link"
        }, 
        {
            "location": "/ProgrammingGuide/text-classification/", 
            "text": "Analytics Zoo provides pre-defined models having different encoders that can be used for classifying texts.\n\n\nHighlights\n\n\n\n\nEasy-to-use models, could be fed into NNFrames or BigDL Optimizer for training.\n\n\nThe encoders we support include CNN, LSTM and GRU.\n\n\n\n\n\n\nBuild a TextClassifier model\n\n\nYou can call the following API in Scala and Python respectively to create a \nTextClassifier\n with \npre-trained GloVe word embeddings as the first layer\n.\n\n\nScala\n\n\nval textClassifier = TextClassifier(classNum, embeddingFile, wordIndex = null, sequenceLength = 500, encoder = \ncnn\n, encoderOutputDim = 256)\n\n\n\n\n\n\nclassNum\n: The number of text categories to be classified. Positive integer.\n\n\nembeddingFile\n The path to the word embedding file. Currently only \nglove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt\n are supported. You can download from \nhere\n.\n\n\nwordIndex\n Map of word (String) and its corresponding index (integer). The index is supposed to \nstart from 1\n with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is null. In this case, all the words in the embeddingFile will be taken into account and you can call \nWordEmbedding.getWordIndex(embeddingFile)\n to retrieve the map.\n\n\nsequenceLength\n: The length of a sequence. Positive integer. Default is 500.\n\n\nencoder\n: The encoder for input sequences. String. \"cnn\" or \"lstm\" or \"gru\" are supported. Default is \"cnn\".\n\n\nencoderOutputDim\n: The output dimension for the encoder. Positive integer. Default is 256.\n\n\n\n\nPython\n\n\ntext_classifier = TextClassifier(class_num, embedding_file, word_index=None, sequence_length=500, encoder=\ncnn\n, encoder_output_dim=256)\n\n\n\n\n\n\nclass_num\n: The number of text categories to be classified. Positive int.\n\n\nembedding_file\n The path to the word embedding file. Currently only \nglove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt\n are supported. You can download from \nhere\n.\n\n\nword_index\n Dictionary of word (string) and its corresponding index (int). The index is supposed to \nstart from 1\n with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is None. In this case, all the words in the embedding_file will be taken into account and you can call \nWordEmbedding.get_word_index(embedding_file)\n to retrieve the map.\n\n\nsequence_length\n: The length of a sequence. Positive int. Default is 500.\n\n\nencoder\n: The encoder for input sequences. String. 'cnn' or 'lstm' or 'gru' are supported. Default is 'cnn'.\n\n\nencoder_output_dim\n: The output dimension for the encoder. Positive int. Default is 256.\n\n\n\n\n\n\nTrain a TextClassifier model\n\n\nAfter building the model, we can use BigDL Optimizer to train it (with validation) using RDD of \nSample\n.\n\n\nNote that raw text data may need to go through tokenization and word2index before being fed into the Optimizer. You can refer to the \nexamples\n we provide for data pre-processing.\n\n\nScala\n\n\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.zoo.pipeline.api.keras.metrics.Accuracy\nimport com.intel.analytics.zoo.pipeline.api.keras.objectives.SparseCategoricalCrossEntropy\n\nval optimizer = Optimizer(\n  model = textClassifier,\n  sampleRDD = trainRDD,\n  criterion = SparseCategoricalCrossEntropy[Float](),\n  batchSize = 128)\n\noptimizer\n  .setOptimMethod(new Adagrad(learningRate = 0.01, learningRateDecay = 0.001))\n  .setValidation(Trigger.everyEpoch, valRDD, Array(new Accuracy), 128)\n  .setEndWhen(Trigger.maxEpoch(20))\n  .optimize()\n\n\n\n\nPython\n\n\nfrom bigdl.optim.optimizer import *\nfrom zoo.pipeline.api.keras.objectives import SparseCategoricalCrossEntropy\nfrom zoo.pipeline.api.keras.metrics import Accuracy\n\noptimizer = Optimizer(\n    model=text_classifier,\n    training_rdd=train_rdd,\n    criterion=SparseCategoricalCrossEntropy(),\n    end_trigger=MaxEpoch(20),\n    batch_size=128,\n    optim_method=Adagrad(learningrate=0.01, learningrate_decay=0.001))\n\noptimizer.set_validation(\n    batch_size=128,\n    val_rdd=val_rdd,\n    trigger=EveryEpoch(),\n    val_method=[Accuracy()])\n\n\n\n\n\n\nDo prediction\n\n\nAfter training the model, it can be used to predict probabilities or class labels.\n\n\nScala\n\n\n// Predict for probability distributions.\nval results = textClassifier.predict(rdd)\n// Predict for class labels. By default, label starts from 0.\nval resultClasses = textClassifier.predictClasses(rdd)\n\n\n\n\nPython\n\n\n# Predict for probability distributions.\nresults = text_classifier.predict(rdd)\n# Predict for class labels. By default, label starts from 0.\nresult_classes = text_classifier.predict_classes(rdd)\n\n\n\n\n\n\nExamples\n\n\nWe provide an example to train the TextClassifier model on 20 Newsgroup dataset and uses the model to do prediction.\n\n\nSee \nhere\n for the Scala example.\n\n\nSee \nhere\n for the Python example.", 
            "title": "Text Classification API"
        }, 
        {
            "location": "/ProgrammingGuide/text-classification/#build-a-textclassifier-model", 
            "text": "You can call the following API in Scala and Python respectively to create a  TextClassifier  with  pre-trained GloVe word embeddings as the first layer .  Scala  val textClassifier = TextClassifier(classNum, embeddingFile, wordIndex = null, sequenceLength = 500, encoder =  cnn , encoderOutputDim = 256)   classNum : The number of text categories to be classified. Positive integer.  embeddingFile  The path to the word embedding file. Currently only  glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt  are supported. You can download from  here .  wordIndex  Map of word (String) and its corresponding index (integer). The index is supposed to  start from 1  with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is null. In this case, all the words in the embeddingFile will be taken into account and you can call  WordEmbedding.getWordIndex(embeddingFile)  to retrieve the map.  sequenceLength : The length of a sequence. Positive integer. Default is 500.  encoder : The encoder for input sequences. String. \"cnn\" or \"lstm\" or \"gru\" are supported. Default is \"cnn\".  encoderOutputDim : The output dimension for the encoder. Positive integer. Default is 256.   Python  text_classifier = TextClassifier(class_num, embedding_file, word_index=None, sequence_length=500, encoder= cnn , encoder_output_dim=256)   class_num : The number of text categories to be classified. Positive int.  embedding_file  The path to the word embedding file. Currently only  glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt  are supported. You can download from  here .  word_index  Dictionary of word (string) and its corresponding index (int). The index is supposed to  start from 1  with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is None. In this case, all the words in the embedding_file will be taken into account and you can call  WordEmbedding.get_word_index(embedding_file)  to retrieve the map.  sequence_length : The length of a sequence. Positive int. Default is 500.  encoder : The encoder for input sequences. String. 'cnn' or 'lstm' or 'gru' are supported. Default is 'cnn'.  encoder_output_dim : The output dimension for the encoder. Positive int. Default is 256.", 
            "title": "Build a TextClassifier model"
        }, 
        {
            "location": "/ProgrammingGuide/text-classification/#train-a-textclassifier-model", 
            "text": "After building the model, we can use BigDL Optimizer to train it (with validation) using RDD of  Sample .  Note that raw text data may need to go through tokenization and word2index before being fed into the Optimizer. You can refer to the  examples  we provide for data pre-processing.  Scala  import com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.zoo.pipeline.api.keras.metrics.Accuracy\nimport com.intel.analytics.zoo.pipeline.api.keras.objectives.SparseCategoricalCrossEntropy\n\nval optimizer = Optimizer(\n  model = textClassifier,\n  sampleRDD = trainRDD,\n  criterion = SparseCategoricalCrossEntropy[Float](),\n  batchSize = 128)\n\noptimizer\n  .setOptimMethod(new Adagrad(learningRate = 0.01, learningRateDecay = 0.001))\n  .setValidation(Trigger.everyEpoch, valRDD, Array(new Accuracy), 128)\n  .setEndWhen(Trigger.maxEpoch(20))\n  .optimize()  Python  from bigdl.optim.optimizer import *\nfrom zoo.pipeline.api.keras.objectives import SparseCategoricalCrossEntropy\nfrom zoo.pipeline.api.keras.metrics import Accuracy\n\noptimizer = Optimizer(\n    model=text_classifier,\n    training_rdd=train_rdd,\n    criterion=SparseCategoricalCrossEntropy(),\n    end_trigger=MaxEpoch(20),\n    batch_size=128,\n    optim_method=Adagrad(learningrate=0.01, learningrate_decay=0.001))\n\noptimizer.set_validation(\n    batch_size=128,\n    val_rdd=val_rdd,\n    trigger=EveryEpoch(),\n    val_method=[Accuracy()])", 
            "title": "Train a TextClassifier model"
        }, 
        {
            "location": "/ProgrammingGuide/text-classification/#do-prediction", 
            "text": "After training the model, it can be used to predict probabilities or class labels.  Scala  // Predict for probability distributions.\nval results = textClassifier.predict(rdd)\n// Predict for class labels. By default, label starts from 0.\nval resultClasses = textClassifier.predictClasses(rdd)  Python  # Predict for probability distributions.\nresults = text_classifier.predict(rdd)\n# Predict for class labels. By default, label starts from 0.\nresult_classes = text_classifier.predict_classes(rdd)", 
            "title": "Do prediction"
        }, 
        {
            "location": "/ProgrammingGuide/text-classification/#examples", 
            "text": "We provide an example to train the TextClassifier model on 20 Newsgroup dataset and uses the model to do prediction.  See  here  for the Scala example.  See  here  for the Python example.", 
            "title": "Examples"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/", 
            "text": "Analytics Zoo provides two Recommender models, including Wide and Deep(WND) learning model and Neural network-based Collaborative Filtering (NCF) model. \n\n\nHighlights\n\n\n\n\nEasy-to-use models, could be fed into NNFrames or BigDL Optimizer for training.\n\n\nRecommenders can handle either explict or implicit feedback, given corresponding features.\n\n\nIt provides three user-friendly APIs to predict user item pairs, and recommend items (users) for users (items).\n\n\n\n\nThe examples/notebooks are included in the Analytics Zoo source code.\n\n\n\n\nWide and Deep Learning Model.\n    \nScala example\n\n    \nPython notebook\n\n\nNCF.\n    \nScala example\n\n    \nPython notebook\n\n\n\n\n\n\nWide and Deep\n\n\nScala\n\n\nBuild a WND model for recommendation. \n\n\nval wideAndDeep = WideAndDeep(modelType = \nwide_n_deep\n, numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))\n\n\n\n\nTrain a WND model using BigDL Optimizer.\n\n\nval optimizer = Optimizer(\n      model = wideAndDeep,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()\n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nval userItemPairPrediction = wideAndDeep.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = wideAndDeep.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = wideAndDeep.recommendForItem(validationpairFeatureRdds, 3)\n\n\n\n\nSee more details in our\nRecommender API\n and \nScala example\n.\n\n\nPython\n\n\nBuild a WND model for recommendation. \n\n\nwide_n_deep = WideAndDeep(class_num, column_info, model_type=\nwide_n_deep\n, hidden_layers=(40, 20, 10))\n\n\n\n\nTrain a WND model using BigDL Optimizer \n\n\noptimizer = Optimizer(\n    model=wide_n_deep,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize() \n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nuserItemPairPrediction = wide_n_deep.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = wide_n_deep.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = wide_n_deep.recommend_for_item(valPairFeatureRdds, 3)\n\n\n\n\nSee more details in our \nRecommender API\n and \nPython notebook\n.\n\n\n\n\nNeural network-based Collaborative Filtering\n\n\nScala\n\n\nBuild a NCF model for recommendation. \n\n\nval ncf = NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)\n\n\n\n\nTrain a NCF model using BigDL Optimizer \n\n\nval optimizer = Optimizer(\n      model = ncf,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()\n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nval userItemPairPrediction = ncf.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = ncf.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = ncf.recommendForItem(validationpairFeatureRdds, 3)\n\n\n\n\nSee more details in our\nRecommender API\n and \nScala example\n\n\nPython\n\n\nBuild a NCF model for recommendation. \n\n\nncf=NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)\n\n\n\n\nTrain a NCF model using BigDL Optimizer \n\n\noptimizer = Optimizer(\n    model=ncf,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize() \n\n\n\n\nPredict and recommend items(users) for users(items) with given features.\n\n\nuserItemPairPrediction = ncf.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = ncf.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = ncf.recommend_for_item(valPairFeatureRdds, 3)\n\n\n\n\nSee more details in our \nRecommender API\n and \nPython notebook\n.", 
            "title": "Recommendation API"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/#wide-and-deep", 
            "text": "Scala  Build a WND model for recommendation.   val wideAndDeep = WideAndDeep(modelType =  wide_n_deep , numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))  Train a WND model using BigDL Optimizer.  val optimizer = Optimizer(\n      model = wideAndDeep,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()  Predict and recommend items(users) for users(items) with given features.  val userItemPairPrediction = wideAndDeep.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = wideAndDeep.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = wideAndDeep.recommendForItem(validationpairFeatureRdds, 3)  See more details in our Recommender API  and  Scala example .  Python  Build a WND model for recommendation.   wide_n_deep = WideAndDeep(class_num, column_info, model_type= wide_n_deep , hidden_layers=(40, 20, 10))  Train a WND model using BigDL Optimizer   optimizer = Optimizer(\n    model=wide_n_deep,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize()   Predict and recommend items(users) for users(items) with given features.  userItemPairPrediction = wide_n_deep.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = wide_n_deep.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = wide_n_deep.recommend_for_item(valPairFeatureRdds, 3)  See more details in our  Recommender API  and  Python notebook .", 
            "title": "Wide and Deep"
        }, 
        {
            "location": "/ProgrammingGuide/recommendation/#neural-network-based-collaborative-filtering", 
            "text": "Scala  Build a NCF model for recommendation.   val ncf = NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)  Train a NCF model using BigDL Optimizer   val optimizer = Optimizer(\n      model = ncf,\n      sampleRDD = trainRdds,\n      criterion = ClassNLLCriterion[Float](),\n      batchSize = 8000)\n\noptimizer\n      .setOptimMethod(new Adam[Float](learningRate = 1e-2,learningRateDecay = 1e-5))\n      .setEndWhen(Trigger.maxEpoch(10))\n      .optimize()  Predict and recommend items(users) for users(items) with given features.  val userItemPairPrediction = ncf.predictUserItemPair(validationpairFeatureRdds)\nval userRecs = ncf.recommendForUser(validationpairFeatureRdds, 3)\nval itemRecs = ncf.recommendForItem(validationpairFeatureRdds, 3)  See more details in our Recommender API  and  Scala example  Python  Build a NCF model for recommendation.   ncf=NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)  Train a NCF model using BigDL Optimizer   optimizer = Optimizer(\n    model=ncf,\n    training_rdd=train_data,\n    criterion=ClassNLLCriterion(),\n    optim_method=Adam(learningrate = 0.001, learningrate_decay=0.00005),\n    end_trigger=MaxEpoch(10),\n    batch_size=batch_size)\noptimizer.optimize()   Predict and recommend items(users) for users(items) with given features.  userItemPairPrediction = ncf.predict_user_item_pair(valPairFeatureRdds)\nuserRecs = ncf.recommend_for_user(valPairFeatureRdds, 3)\nitemRecs = ncf.recommend_for_item(valPairFeatureRdds, 3)  See more details in our  Recommender API  and  Python notebook .", 
            "title": "Neural network-based Collaborative Filtering"
        }, 
        {
            "location": "/ProgrammingGuide/usercases-overview/", 
            "text": "Overview\n\n\nAnalytics Zoo provides a collection of reference user applications and demos, which can be modified or even used off-the-shelf in real world applications. Some are listed below. See all in \nanalytics-zoo/apps\n.\n\n\n\n\n\n\nAnomaly Detection\n demostrates using LSTM network to detect anomalies in time series data.\n\n\n\n\n\n\nFraud Detection\n demostrates using feed-forward neural network to detect frauds in credit card transactions data. \n\n\n\n\n\n\nImage Augmentation\n demostrates how to do image augmentation for vision projects. \n\n\n\n\n\n\nObject Detection\n demonstrates how to use Analytics Zoo Object Detection API (and pretrained SSD model) on videos. \n\n\n\n\n\n\nRecommendation\n demonstrates how to use Analytics Zoo Recommendation APIs (i.e.Neural Collaborative Filtering, Wide and Deep Learning) to do recommendation on data with explicit feedback. \n\n\n\n\n\n\nSentiment Analysis\n demostrates how to do sentiment analysis using neural network models (e.g. CNN, LSTM, GRU, Bi-LSTM).  \n\n\n\n\n\n\nVariational AutoEncoder\n demostrates how to use variational autoencoder to generate faces and digital numbers.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/ProgrammingGuide/usercases-overview/#overview", 
            "text": "Analytics Zoo provides a collection of reference user applications and demos, which can be modified or even used off-the-shelf in real world applications. Some are listed below. See all in  analytics-zoo/apps .    Anomaly Detection  demostrates using LSTM network to detect anomalies in time series data.    Fraud Detection  demostrates using feed-forward neural network to detect frauds in credit card transactions data.     Image Augmentation  demostrates how to do image augmentation for vision projects.     Object Detection  demonstrates how to use Analytics Zoo Object Detection API (and pretrained SSD model) on videos.     Recommendation  demonstrates how to use Analytics Zoo Recommendation APIs (i.e.Neural Collaborative Filtering, Wide and Deep Learning) to do recommendation on data with explicit feedback.     Sentiment Analysis  demostrates how to do sentiment analysis using neural network models (e.g. CNN, LSTM, GRU, Bi-LSTM).      Variational AutoEncoder  demostrates how to use variational autoencoder to generate faces and digital numbers.", 
            "title": "Overview"
        }, 
        {
            "location": "/ProgrammingGuide/anomaly-detection/", 
            "text": "Analytics Zoo shows how to detect anomalies in time series data based on RNN network. Currently, a \nPython notebook\n is provided. \nIn the example, a RNN network using Analytics Zoo Keras-Style API is built, and \nNYC taxi passengers dataset\n is used to train and test the model.\n\n\nWe split the entire curve into 2 sections - training data and testing data. The training data section is treated all as normal and an RNN model is trained to fit the training data, the model has three LSTM layers followed by one Dense layer at the end.\n\n\nmodel = Sequential()\nmodel.add(LSTM(input_shape=(input_dim1, input_dim2, output_dim=8, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(15,return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(output_dim=1))\n\n\n\n\nTrain the model using using MSE loss (as a regression problem).\n\n\nmodel.compile(loss='mse', optimizer='rmsprop')\nmodel.fit(x_train,y_train,batch_size=3028,nb_epoch=30)\n\n\n\n\nUse this RNN model to predict the testing curve.\n\n\npredictions = model.predict(x_test)\n\n\n\n\nAnomalies could be defined by comparing the predictions and actual values. The current example defines data points as anomalies if the difference of predictions and actual values are larger than a certain value(threshold).\nSee more details in the exmaple \nPython notebook", 
            "title": "Anomaly Detection"
        }, 
        {
            "location": "/wp-bigdl/", 
            "text": "BigDL: A Distributed Deep Learning Framework for Big Data\n\n\nJason (Jinquan) Dai\n1\n, Yiheng Wang\n1\n, Xin Qiu\n1\n, Ding Ding\n1\n, Yao Zhang\n2 \u01c2\n, Yanzhang Wang\n1\n, Xianyan Jia\n2 \u01c2\n, Cherry (Li) Zhang\n1\n, Yan Wan\n3 \u01c2\n, Zhichao Li\n1\n, Jiao Wang\n1\n, Shengsheng Huang\n1\n, Zhongyuan Wu\n1\n, Yang Wang\n1\n, Yuhao Yang\n1\n, Bowen She\n1\n, Dongjie Shi\n1\n, Qi Lu\n1\n, Kai Huang\n1\n, Guoqiong Song\n1\n\n\n1\nIntel Corporation,    \n2\nTencent Inc.,    \n3\nAlibaba Group\n\n\n\u01c2\nWork was done when the author worked at Intel\n\n\n\n\nAbstract\n\n\nIn this paper, we present BigDL, a distributed deep learning framework for Big Data platforms and workflows. It is implemented on top of Apache Spark, and allows users to write their deep learning applications as standard Spark programs (running directly on large-scale big data clusters in a distributed fashion). It provides an expressive, \u201cdata-analytics integrated\u201d deep learning programming model, so that users can easily build the end-to-end analytics + AI pipelines under a unified programing paradigm; by implementing an \nAllReduce\n like operation using existing primitives in Spark (e.g., shuffle, broadcast, and in-memory data persistence), it also provides a highly efficient \u201cparameter server\u201d style architecture, so as to achieve highly scalable, data-parallel distributed training. Since its initial open source release, BigDL users have built many analytics and deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark.\n\n\n1. Introduction\n\n\nRecent breakthroughs in artificial intelligence have brought deep learning to the forefront of new generations of data analytics; as the requirements and usage models expand, new systems and architecture beyond existing deep learning frameworks (e.g., Caffe [1], Torch [2], TensorFlow [3], MXNet [4], Chainer [5], etc.) have inevitably emerged. In particular, there is increasing demand from organizations to apply deep learning technologies (such as computer vision, natural language processing, generative adversary networks, etc.) to their big data platforms and pipelines. This emerging convergence of deep learning and big data analytics is driven by several important technology and industry trends:\n\n\n\n\n\n\nData scale drives deep learning process.\n Today users are building even deeper, more complex neural networks to take advantage of the massive amount of data that they have access to. In practice, big data (e.g., Apache Hadoop [6] or Apache Spark [7]) clusters are ubiquitously deployed as the global data platform, where all the production data are stored and made available to all the users. Therefore, it is usually much more efficient to run the algorithm directly on the big data cluster where the data are stored and shared (than copying data to a separate infrastructure).\n\n\n\n\n\n\nReal-world deep learning applications are complex big data pipelines,\n which require a lot of data processing (such as cleaning, transformation, augmentation, feature extraction, etc.) beyond model training/inference. Therefore, it is much simpler and more efficient (for development and workflow management) to seamlessly integrate deep learning functionalities into existing big data workflow running on the same infrastructure, especially given the recent improvements that reduce deep learning training time from weeks to hours [9] or even minutes [10].\n\n\n\n\n\n\nDeep learning is increasingly adopted by the big data and data science community.\n Unfortunately, mainstream data engineers and data scientists are usually not deep learning experts; as the usages of deep learning expand and scale to larger deployment, it will be much more easier if these users can continue the use of familiar software tools and programming models (e.g., Spark [8] or even SQL) and existing big data cluster infrastructures to build their deep learning applications.\n\n\n\n\n\n\nWe have developed \nBigDL\n [11], a distributed deep learning framework for big data platforms and workflows. It is implemented as a library on top of Apache Spark, and allows users to write their large-scale deep learning applications (including model training, fine-tuning and inference) as standard Spark programs, which can run directly on existing big data (Hadoop or Spark) clusters. BigDL provides comprehensive support of deep learning technologies (neural network operations, layers, losses and optimizers); in particular, users can directly run existing models defined in other frameworks (such as TensorFlow, Keras [12], Caffe and Torch) on Spark in a distributed fashion.\n\n\nBigDL also provides seamless integrations of deep learning technologies into the big data ecosystem. Not only can a BigDL program directly interact with different components in the Spark framework (e.g., DataFrames [13], Spark Streaming [14], ML Pipelines [15], etc.), it can also directly run in a variety of big data frameworks (such as Apache Storm [17], Apache Flink [18], Apache Kafka [19], etc.). Since its initial open source on Dec 30, 2016, BigDL has enabled many community users to build their deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark and big data platforms.\n\n\n2. Programming Model\n\n\nBigDL is implemented on Apache Spark, a widely used cluster computing engine for big data analysis. Spark provides a comprehensive set of libraries for relational processing, streaming, graph processing [16] and machine learning (in Python, Scala or Java); as a result, one can easily build the end-to-end, \u201cdata-analytics integrated\u201d deep learning and AI pipelines (under a unified programing paradigm) using Spark and BigDL, as illustrated in Figure 1.\n\n\n1    spark = SparkContext(appName=\ntext_classifier\n, \u2026)\n2    //load input data: (text, label) pairs\n3    texts_rdd = spark.textFile(\nhdfs://...\n)\n4    //convert text to list of words\n5    words_rdd = texts_rdd.map(lambda text, label: \n6                               ([w for w in to_words(text)], label))\n7    //load GloVe embedding\n8    w2v = news20.get_glove_w2v(dim=\u2026)\n9    //convert word list to list of vertors using GloVe embeddings\n10   vector_rdd = words_rdd.map(lambda word_list, label:\n11                              ([to_vec(w, w2v) for w in word_list], label))\n12   //convert (list of vertors, label) pair to Sample\n13   sample_rdd = vector_rdd.map(lambda vector_list, label: \n14                                 to_sample(vector_list, label))\n15   //construct neural network model  \n16   model = Sequential().add(Recurrent().add(LSTM(\u2026)))\n17                       .add(Linear(\u2026))\n18                       .add(LogSoftMax())\n19   //train the model \n20   loss = ClassNLLCriterion()\n21   optim_method = Adagrad()\n22   optimizer = Optimizer(model=model, training_rdd=sample_rdd, \n23                         criterion=loss, optim_method= optim_method, \u2026)\n24   optimizer.set_train_summary(summary = TrainSummary(\u2026))\n25   trained_model =optimizer.optimize()\n26   //model prediction\n27   test_rdd = \u2026\n28   prediction_rdd = trained_model.predict(test_rdd)\n\n\n\n\nFigure 1. The end-to-end text classification pipeline (including data loading, tokenization, word vectorization, training, prediction, etc.) on Spark and BigDL.\n\n\n2.1. Spark\n\n\nSpark provides the \nResilient Distributed Dataset\n (RDD) [8] in-memory storage abstraction, which is an immutable collection of Python or Scala/Java objects partitioned across a cluster, and can be transformed to derive new RDDs through data-parallel functional operators like \nmap, filter and reduce.\n Consequently, users can efficiently load very large dataset and process the loaded data in a distributed fashion using Spark, and then feed the processed data into the analytics and AI pipeline. For example, lines 1 ~ 6 in Figure 1 illustrates how to load the input data (article texts and their associated labels) from the Hadoop Distributed File System (HDFS) [20], and transforms each text string into a list of words.\n\n\n2.2. Data transformation\n\n\nSpark supports general dataflow DAGs [8] by composing multiple data-parallel operators on RDD, where each vertex represents an RDD and each edge represents the transformation by the RDD operator. By constructing the dataflow DAG in Spark, users can easily transform the input data (for, e.g., image augmentations, word vectorizations, etc.), which can then be used by the neural network models. For example, lines 7 ~ 11 in Figure 1 illustrates how to apply GloVe word embedding [21] to transform each word to a vector. \n\n\n\n\n\n\nN-dimensional array:\n In BigDL, we model the basic data elements used in neural network computations as N-dimensional numeric (int8, float32, etc.) arrays. These arrays are represented by \nnumpy.ndarry\n [22] and \nBigDL.Tensor\n (similar to \nTorch.Tensor\n [23]) for BigDL Python and Scala/Java APIs respectively.\n\n\n\n\n\n\nSample:\n Each record used in BigDL model training and prediction is modelled as a Sample, which contains an input feature and an optional label. Each input feature is one or more N-dimensional arrays, while each label is either a scalar (float32) value, or one or more \nN-dimensional arrays.\n For instance, lines 12 ~ 14 in Figure 1 shows how to turn the transformed data into an RDD of \nSamples,\n which will later be used by BigDL model training.\n\n\n\n\n\n\n2.3. Model Construction\n\n\nSimilar to Torch and Keras, BigDL uses a dataflow representation for the neural network model, where each vertex in the dataflow graph represents a neural network layer (such as \nReLu, Spatial Convolution and LSTM\n). BigDL then uses the semantics of the layers for model evaluation (\nforward\n) and gradient computation (\nbackward\n). For example, lines 15 ~ 18 in Figure 1 illustrates the model definition used in the text classification example.\n\n\n2.4. Model training\n\n\nThe transformed input data (RDD of Samples) and the constructed model can then be passed over to the \nOptimizer\n in BigDL, which automatically performs distributed model training across the cluster, as illustrated by lines 19 ~ 25 in \nFigure 1.\n\n\n\n\n\n\nOptimizer:\n In BigDL, the distributed training process is modelled by the Optimizer abstraction, which runs multiple, iterative Spark jobs to minimize the loss (as defined by the user specified Criterion) using specific optimization method (such as \nSGD, AdaGrad [24], Adam [25], etc.\n).\n\n\n\n\n\n\nVisualization:\n To make it easy for users to understand the behaviors of model training, the \noptimizer\n in BigDL can be configured to produce a \nTrainSummary\n that contains various summary data (e.g., loss, weight, etc.), as illustrated by line 24 in Figure 1; the summary data can then be visualized in, for instance, TensorBoard [26] or Jupytor Notebooks [27].\n\n\n\n\n\n\n2.5. Model Inference\n\n\nBigDL also allows users to directly use existing models (pre-trained by Caffe, Keras, TensorFlow, Torch or BigDL) in Spark, so as to directly perform model prediction in a distributed fashion (using RDD transformations), as illustrated by lines 26 ~ 28 in Figure 1. \n\n\n\n\nModelBroadcast:\n BigDL provides the \nModelBroadcast\n abstraction to manage the deployment of the pre-trained model across the cluster in a Spark job; the model prediction operation (\npredict\n) in BigDL uses \nModelBroadcast\n to cache a single copy of the model on each machine (by leveraging the \nbroadcast\n [28] mechanism in Spark), and manage the model cloning and weight sharing among different tasks in the same machine.\n\n\n\n\n2.6. Spark DataFrame and ML Pipeline\n\n\nBesides RDD, Spark provides a high level \nDataFrame\n abstraction [13], which is a distributed collection of rows with a specific schema (similar to a table in a relational database), and implements data-parallel relational operators like \nfilter\n and \njoin\n for efficient structured data analysis. On top of DataFrame, Spark introduces a high level ML \n(machine learning) pipeline\n [15] similar to SciKit-Learn [29], which allows users to construct the machine learning workflow as a graph of transformations on data (e.g., feature extraction, normalization, model training, etc.). BigDL also provides native integration with the high level Spark DataFrame and ML Pipeline APIs (using its \nDLModel\n and \nDLEstimator\n abstractions). \n\n\n3. Execution Model\n\n\nSimilar to other Big Data systems (such as MapReduce [30]), a Spark cluster consists of a single driver node and multiple worker nodes, as shown in Figure 2.  The driver node is responsible for coordinating the tasks in a Spark job (e.g., scheduling and dispatching), while the worker nodes are responsible for the actual computation and physical data storage. To automatically parallelize the large-scale data processing across the cluster in a fault-tolerant fashion, Spark provides a functional compute model where immutable RDDs are transformed through coarse-grained operators (i.e., applying the same operation to all data items). \n\n\n \n\n\nFigure 2. A Spark job contains many Spark tasks; the driver node is responsible for scheduling and dispatching the tasks to worker nodes, which runs the actual Spark tasks.\n\n\nOn the other hand, efficient and distributed training of deep neural networks would necessitate very different operations (such as fine-grained data access and in-place data mutation [3]). In this section, we describe in details how BigDL supports highly efficient and scalable distributed training, directly on top of the data parallel and functional compute model of Spark (in addition to various optimizations for model inference).\n\n\n3.1. Data-parallel training\n\n\nTo train a deep neural network model across the cluster, BigDL provides data-parallel training on Spark using synchronous mini-batch SGD, which is shown to achieve better scalability and efficiency (in terms of time-to-quality) compared to asynchronous training [31][32]. The distributed training in BigDL is implemented as an iterative process, as illustrated in Figure 3; each iteration runs a couple of Spark jobs to first compute the gradients using the current mini-batch, and then make a single update to the parameters of the neural network model.\n\n\nfor (i \n- 1 to N) {\n  //\nmodel forward-backward\n job\n  for each task in the Spark job:\n     read the latest weights\n     get a random batch of data from local Sample partition\n     compute errors (forward on local model replica)\n     compute gradients (backward on local model replica)\n  //\nparameter synchronization\n job\n  aggregate (sum) all the gradients\n  update the weights per specified optimization method\n}\n\n\n\n\nFigure 3. BigDL provides efficient, data-parallel, synchronous mini-batch SGD, where each iteration runs two Spark jobs for \u201cmodel forward-backward\u201d and \u201cparameter synchronization\u201d.\n\n\nAs described in Section 2, BigDL models the training data as an RDD of Samples, which are automatically partitioned and potentially cached in memory across the Spark cluster. In addition, to implement the data-parallel training, BigDL also constructs an RDD of models, each of which is a replica of the original neural network model. The model and Sample RDDs are co-partitioned and co-located [14] across the cluster, as shown in Figure 4; consequently, in each iteration of the model training, a single \u201cmodel forward-backward\u201d Spark job can apply the functional \nzip\n operator to the partitions of model and Sample RDDs, and compute the gradients in parallel for each model replica (using a small batch of data in the co-located Sample partition), as illustrated in Figure 4.\n\n\n \n\n\nFigure 4. The \u201cmodel forward-backward\u201d spark job, which computes the local gradients for each model replica in parallel.\n\n\n3.2. Parameter synchronization\n\n\nParameter synchronization is a performance critical operation for data-parallel training (in terms of speed and scalability). To support efficient parameter synchronization, existing deep learning frameworks usually implement the \nparameter server\n [33][34][35] architecture or \nAllReduce\n [36] operation, which unfortunately cannot be directly supported by the functional compute model provided by the Big Data systems.\n\n\nIn BigDL, we have adapted the primitives available in Spark (e.g., \nshuffle, broadcast, in-memory cache\n, etc.) to implement an efficient AllReduce-like operation, so as to mimic the functionality of a parameter server architecture (as illustrated in Figure 5).\n\n\n \n\n\nFigure 5. Parameter synchronization in BigDL. Each local gradient (computed by a task in the \u201cmodel forward-backward\u201d job) is evenly divided into N partitions; then each task n in the \u201cparameter synchronization\u201d job aggregates these local gradients and update the weights for the nth partition.\n\n\n\n\n\n\nA Spark job has \nN\n tasks, each of which is assigned a unique Id ranging from \n1\n to \nN\n in BigDL. After each task in the \u201c\nmodel forward-backward\n\u201d job computes the local gradients (as described in section 3.1), it evenly divides the local gradients into \nN\n partitions, as shown in Figure 5.\n\n\n\n\n\n\nNext, another \u201c\nparameter synchronization\n\u201d job is launched; each task \nn\n in the \u201c\nparameter synchronization\n\u201d job is responsible for managing the n\nth\n partition of the parameters, just like a parameter server (as shown in Figure 6). Specifically, the n\nth\n partition of the gradients (from all the tasks of the previous \u201c\nmodel forward-backward\n\u201d job) are first \nshuffled\n to task \nn\n, which then aggregates (sums) these gradients, and applies the updates to the n\nth\n partition of the weights (using the specific \noptimization method\n), as illustrated in Figure 5.\n\n\n\n\n\n\n \n\n\nFigure 6. The \u201cparameter synchronization\u201d Spark job, manages the n\nth\n partition of the parameters (similar to a parameter server).\n\n\n\n\n\n\nAfter that, each task \nn\n in the \u201c\nparameter synchronization\n\u201d job \nbroadcasts\n the n\nth\n partition of the updated weights; consequently, tasks in the \u201c\nmodel forward-backward\n\u201d job of the next iteration can read the latest value of all the weights before the next training step begins.\n\n\n\n\n\n\nThe \nshuffle\n and \ntask-side broadcast\n operations described above are implemented on top of the distributed \nin-memory\n storage in Spark: both the shuffled \ngradients\n and broadcasted \nweights\n are materialized in memory, which can be read remotely by the Spark tasks with extremely low latency.\n\n\n\n\n\n\nBy implementing the AllReduce operation using primitives in Spark, BigDL provides a highly efficient \u201cparameter server\u201d style architecture directly on top of Big Data frameworks. As a result, it is demonstrated to support highly scalable distributed training on up to 256-node, as reported by Cray [37] and shown in Figure 7. \n\n\n \n\n\nFigure 7. Throughput of ImageNet Inception v1 training reported by Cary [37] (using BigDL 0.3.0 and dual-socket Intel Broadwell 2.1 GHz); the training throughput scales almost linear up to 128 nodes (and continue to scale reasonably up to 256 nodes).\n\n\n3.3. Task scheduling\n\n\nWhile BigDL provides a highly efficient \u201cparameter server\u201d style architecture, it has a fundamentally different implementation than existing deep learning frameworks. In particular, existing deep learning frameworks are typically deployed as multiple long-running, potentially stateful tasks [3], which interact with each other (in a blocking fashion to support synchronous mini-batch SGD) for model computation and parameter synchronization.\n\n\nIn contrast, BigDL runs a series of short-lived Spark jobs (e.g., two jobs per mini-batch as described in earlier sections), and each task in the job is stateless and non-blocking. As a result, BigDL programs can automatically adapt to the dynamic resource changes (e.g., preemption, failures, incremental scaling, resource sharing, etc.) in a timely fashion. On the other hand, task scheduling in Spark can become a potential bottleneck of the distributed training on a large cluster. For instance, Figure 8 shows that, for ImageNet Inception v1 training, the overhead of launching tasks (as a fraction of average compute time) in BigDL, while low for 100~200 tasks, can grows to over 10% when there are close to 500 tasks [39]. To address this issue, BigDL will launch a single, multi-threaded task on each worker, so as to achieve high scalability on large clusters (e.g., up to 256 servers as shown in Figure 7 above). \n\n\nTo scale to an even larger number (e.g., 500) of workers, one can potentially leverages the iterative nature of the model training (in which the same operations are executed repeatedly). For instance, group scheduling introduced by \nDrizzle\n [38] (a low latency execution engine for Spark) can help schedule multiple iterations (or a group) of computations at once, so as to greatly reduce scheduling overheads even if there are a large number of tasks, as benchmarked by RISELab [39] and shown in Figure 8.\n\n\n \n\n\nFigure 8. Overheads of task scheduling and dispatch (as a fraction of average compute time) for ImageNet Inception v1 training in BigDL [39].\n\n\n3.4. Model quantization\n\n\nQuantization refers to using technologies that store numbers and perform calculations on them in more compact and lower precision form (than their original format such as 32-bit floating point). BigDL takes advantage of this type of low precision computing to quantize existing models (which can be pre-trained by various frameworks such as Caffe, Keras, TensorFlow, Torch or BigDL) for optimized inference.\n\n\nBigDL first loads existing models and then quantizes the parameters of some selected layers (e.g., Spatial Convolution) into 8-bit integer (using the equation shown in Figure 9) to produce a quantized model. During model inference, each quantized layer quantizes the input (float32) data into 8-bit integer on the fly, applies the 8-bit calculations (such as GEMM) using the quantized parameters and data, and dequantizes the results to 32-bit floating point. Many of these operations can be fused in the implementation, and consequently the quantization and dequantization overheads are very low at inference time.\n\n\nMath.round(1.0 * value \n           / Math.max(Math.abs(max), Math.abs(min)) \n           * Byte.MaxValue).toByte\n\n\n\n\nFigure 9. Equation for quantizing 32-bit floating point to 8-bit integer.\n\n\nUnlike many existing quantization implementations, BigDL adopts a new local quantization scheme. That is, it performs the quantization and dequantization operations (as described above) in each small local quantization window, a small sub-block (such as a patch or kernel in convolution) of the parameters or input data. As a result, BigDL can use very low bit integers, such as 8-bit, in model quantization with extremely low model accuracy drop (less than 0.1%), 4x model size reduction, and up to 2x inference speedup, as benchmarked on AWS EC2 [40] and shown in Figure 10.\n\n\n\n\nFigure 10. Model quantization results (accuracy, inference speed and model size) for SSD, VGG16 and VGG19 (using BigDL 0.3.0 and AWS EC2 C5.18xlarge instances) [40].\n\n\n3.5. Local execution\n\n\nIn addition to being a standard Spark program, BigDL also provide support to run the model training and inference on a local JVM (without Spark). This helps improve the efficiency when running BigDL on a single node, as there are no overheads such as parameter synchronizations or task scheduling. More importantly, it makes it easy to directly integrate BigDL models (for either inference or fine-tuning) with various big data frameworks, such as Apache Storm, Apache Flink or Apache Kafka, which are usually JVM based.\n\n\n4. Applications\n\n\nSince its initial open source release (on Dec 30, 2016), BigDL users have built many deep learning applications on Spark and Big Data platforms. In this section, we describes three typical use cases (namely, model inference, distributed training and transfer learning) using Spark and BigDL.\n\n\n4.1. Model Inference: image feature extraction\n\n\nJD.com [41] is one of the largest online retailers in the world. It has built an end-to-end \nobject detection and image feature extraction\n pipeline on top of Spark and BigDL[42], as illustrated in Figure 11.\n\n\n \n\n\nFigure 11. End-to-end object detection and image feature extraction pipeline (using SSD and DeepBit models) on top of Spark and BigDL [42].\n\n\n\n\n\n\nThe pipeline first reads hundreds of millions of pictures from a distributed database into Spark (as an RDD of pictures), and then pre-processes the RDD of pictures (including \nresizing\n, \nnormalization\n, and \nbatching\n) in a distributed fashion using Spark.\n\n\n\n\n\n\nAfter that, it uses BigDL to load a \nSSD\n [43] model (pre-trained in Caffe) for large scale, distributed object detection on Spark, which generates the coordinates and scores for the detected objects in each of the pictures.\n\n\n\n\n\n\nIt then generates the target images (by keeping the object with highest score as the target, and cropping the original picture based on the coordinates of the target), and further pre-processes the RDD of target images (including \nresizing\n and \nbatching\n).\n\n\n\n\n\n\nFinally it uses BigDL to load a \nDeepBit\n [44] model (again pre-trained in Caffe) for distributed feature extraction of the target images to generate the corresponding features, and stores the results (RDD of extracted object features) in the Hadoop Distributed File System (HDFS).\n\n\n\n\n\n\nThe entire data analytics and deep learning pipeline, including data loading, partitioning, preprocessing, model inference, and storing the results, can be easily implemented under a unified programming paradigm (using Spark and BigDL). In addition, the end-to-end pipeline also delivers ~3.83x speedup compared to running the same solution on a GPU cluster, as reported by JD [42] and shown in Figure 12.\n\n\n \n\n\nFigure 12. Throughput of GPU clusters and Xeon clusters for the image feature extraction pipeline benchmarked by JD [42]; the GPU throughput is tested on 20 NVIDIA Tesla K40 cards, and the Xeon throughput is tested on 1200 logical cores (where each dual-socket Intel Xeon E5-2650 v4 server runs 50 logical cores).\n\n\n4.2. Distributed training: precipitation nowcasting\n\n\nCray has integrated BigDL to their Urika-XC analytics software suite, and built an end-to-end precipitation nowcasting (\npredicting short-term precipitation\n) workflow on spark and BigDL[37], including data preparation, model training and inference (as illustrated in Figure 13). \n\n\n \n\n\nFigure 13. End-to-end precipitation nowcasting workflow (using sequence-to-sequence model) [37] on Spark and BigDL.\n\n\n\n\n\n\nThe application first reads over a terabyte of raw radar scan data into Spark (as an RDD of radar images), and then converts it into an RDD of \nNumPy ndarrays\n.\n\n\n\n\n\n\nIt then trains a \nsequence-to-sequence\n model [45][46] (as illustrated in Figure 13), using a sequence of images leading up to the current time as the input, and a sequence of predicted images in the future as the output.\n\n\n\n\n\n\nAfter the model is trained, it can be used to predict, say, precipitation patterns for the next hour, as illustrated in Figure 14.\n\n\n\n\n\n\n \n\n\nFigure 14. Predicting precipitation patterns for the next hour (i.e., a sequence of images for the future time steps of the next hour) on Spark and BigDL [37]\n\n\n4.3. Transfer learning: image-similarity based house recommendations\n\n\nMLSListings Inc. is a large \nMultiple Listing Service\n (MLS) for real estate listings, who has been building an image-similarity based house recommendation system on Spark and BigDL [47]. The end-to-end workflow is implemented by leveraging transfer learning (including feature extractions and fine-tuning) technologies, so as to compute both the semantic and visual similarity of the house photos, as illustrated in Figure 15.\n\n\n \n\n\nFigure 15. End-to-end workflow for image-similarity based house recommendations on Spark and BigDL [47]\n\n\nTo compute the \nsemantic similarity\n for the photos, the system fine-tunes the Inception v1 [48] model pre-trained on the Places dataset [49], so as to train three new classifiers (namely, whether the photo shows the house front exterior, the house style and the house stories). In particular, it first loads three pre-trained Inception v1 models, and then appends two new layers (a fully-connected layer followed by a Softmax layer) to each model, so as to train the new classifiers (using photos for which MLSListings have been assigned copyrights). After the training, it can use these classifiers to produce the tags (or labels) for each house listing photo.\n\n\nTo compute the visual similarity, the system use the VGG-16 [50] model pre-trained on the Places dataset to extract the image feature for each house listing photo, which is then combined with the tags generated by the classifiers and stored into a distributed table storage.\n\n\nAt \nmodel serving\n time, the user can select a house listing photo, and have the system to recommend house listings of similar visual characteristics (by computing the cosine similarity score using the image features, while taking into considerations other properties of the houses such as photo tags, house prices, locations, etc.), as illustrated in the \n\u201cSimilar Houses\u201d\n section of the webpage in Figure 16.\n\n\n \n\n\nFigure 16. Automatically recommending \u201cSimilar Houses\u201d with similar visual characteristics [47]\n\n\n5.Related Work\n\n\nExisting big data systems, such as MapReduce [30], Dryad [51] and Spark [8], provide a data-parallel, functional compute model (with potentially dataflow DAG support), so as to efficiently support data partitioning, parallel and distributed computing, fault tolerance, incremental scale-out, etc., in an automatic and transparent fashion. BigDL is built on top of this data-parallel, functional compute model, and adds new support of deep learning technologies to Apache Spark, so as to provide the \u201cdata-analytics integrated\u201d deep learning programming model.\n\n\nExisting deep learning frameworks, such as Caffe [1], Torch [2], TensorFlow [3], Keras [12], MXNet [4] and DL4J [52], usually use a dataflow graph (of either primitive operators or more complex layers) to represent neural network models. For distributed training, they typically implement the parameter server architecture or AllReduce operation (with fine-grained data access and in-place data mutation [3]), which are however not supported by existing big data systems. In contrast, BigDL adopts the similar dataflow representation of neural network models, but provides efficient distributed training directly on top of Apache Spark.\n\n\nRecently there are also a lot of efforts to bring existing deep learning frameworks to Apache Spark. For instance, TensorFrames [53] and Deep Learning Pipelines [54] allow users to directly run TensorFlow or Keras models on each individual partition of Spark Dataframes, for both model inference and single-node model tuning; however, they do not support distributed model training or fine-tuning across multiple machines in a cluster. CaffeOnSpark [55] and TensorFlowOnSpark [56] frameworks use Spark as the orchestration layer to allocate resources from the cluster, and then launch the distributed Caffe or TensorFlow job on the allocated machines; however, the Caffe or TensorFlow job still runs outside of the big data framework, and has very limited interactions with the analytics pipelines. SparkNet [57] uses asynchronous SGD for distributed training on Spark; the master first broadcasts weights to the workers, and each work then trains its own Caffe model for a certain period of time, after which the weights on each worker are sent to the master and averaged to form the new weights; however, the broadcast and weight averaging is very inefficient in SparkNet (e.g., ~20 seconds with just 5 workers [57]). In contrast, BigDL provides highly efficient and scalable distributed training, directly on top of big data framework (using the primitives available in Spark). \n\n\n6. Summary\n\n\nWe have described BigDL, including its programming model, execution model and typical use cases. It combines the benefits of big data and HPC (high performance computing) architecture, so as to provide both an expressive, \u201cdata-analytics integrated\u201d deep learning programing model for users to build their analytics + AI pipelines, and a highly efficient \u201cparameter server\u201d style architecture directly on top of Big Data platforms for scalable data-parallel training. \n\n\nBigDL is a work in progress, but our initial experience is encouraging. Since its initial open source release (on Dec 30, 2016), it has received over 2400 stars on Github; and it have enabled many users to build new analytics and deep learning applications, which can directly run on top of existing Hadoop and/or Spark clusters.\n\n\n7. Acknowledgement\n\n\nWe gratefully acknowledge contributions from our (current and former) colleagues at Intel (including Jun Wang, Liangying Lv, Andy Chen, Yan Dai, Sergey Ermolin, Zewei Chen, Ning Wang, Yulia Tell, Pengfei Yue, Wesley Du, Erjin Ren, Xiao Dong Wang, Radhika Rangarajan, Jack Chen, Milind Damle and Dave Nielsen), and numerous users and collaborators from the open source community (including Shivaram Venkataraman, Xiao Xu, Zhenhua Wang, Alex Heye, Omid Khanmohamadi, Mike Ringenburg, Joseph Spisak, Gopi Kumar, Suqiang Song, Karthik Palaniappan, Rong Gu, etc.) to the BigDL project.\n\n\n8. Reference\n\n\n[1] Caffe. \nhttp://caffe.berkeleyvision.org\n\n\n[2] Torch. \nhttp://torch.ch\n\n\n[3] Mart\u00edn Abadi, et al. \u201cTensorFlow: A System for Large-Scale Machine Learning\u201d, OSDI 2016.\n\n\n[4] Tianqi Chen, et al. \u201cMXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems\u201d, LearningSys 2015.\n\n\n[5] Seiya Tokui, et al. \u201cChainer: a Next-Generation Open Source Framework for Deep Learning\u201d. LearningSys 2015.\n\n\n[6] Apache Hadoop. \nhttp://hadoop.apache.org\n\n\n[7] Apache Spark. \nhttps://spark.apache.org\n\n\n[8] Matei Zaharia , et al. \u201cResilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing\u201d, NSDI 2012.\n\n\n[9] Priya Goyal, et al. \u201cAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour\u201d, arXiv: 1706.02677 [cs.CV]\n\n\n[10] Yang You, et al. \u201cImageNet Training in Minutes\u201d, arXiv:1709.05011 [cs.CV]\n\n\n[11] BigDL. \nhttps://github.com/intel-analytics/BigDL/\n\n\n[12] Keras. \nhttps://keras.io\n\n\n[13] Michael Armbrust, et al. \u201cSpark SQL: Relational Data Processing in Spark\u201d, SIGMOD 2015.\n\n\n[14] Matei Zaharia, et al. \u201cDiscretized Streams: Fault-Tolerant Streaming Computation at Scale\u201d, SOSP 2013.\n\n\n[15] Xiangrui Meng, et al. \u201cMLlib: Machine Learning in Apache Spark\u201d, Journal of Machine Learning Research (JMLR) 2016.\n\n\n[16] Reynold S. Xin, et al. \u201cGraphX: Unifying Data-Parallel and Graph-Parallel Analytics\u201d, OSDI 2014.\n\n\n[17] Apache Storm. \nhttp://storm.apache.org\n\n\n[18] Apache Flink. \nhttps://flink.apache.org\n\n\n[19] Apache Kafka. \nhttps://kafka.apache.org\n\n\n[20] Konstantin Shvachko, et al. \u201cThe Hadoop Distributed File System\u201d, MSST 2010.\n\n\n[21] Jeffrey Pennington, et al. \u201cGloVe: Global Vectors for Word Representation\u201d, EMNLP 2014.\n\n\n[22] Numpy. \nhttp://www.numpy.org\n\n\n[23] Torch7. \nhttps://github.com/torch/torch7\n\n\n[24] J. Duchi, et al. \u201cAdaptive subgradient methods for online learning and stochastic optimization.\u201d Journal of Machine Learning Research (JMLR) 2011.\n\n\n[25] Diederik P. Kingma, et al. \u201cAdam: A Method for Stochastic Optimization\u201d, ICLR 2015.\n\n\n[26] M. Abadi, et al.\n\u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems.\n, 2016.\n\n\n[27] Project Jupyter. \nhttp://jupyter.org\n\n\n[28] Reynold Xin, et al. \u201cShark: SQL and Rich Analytics at Scale\u201d, SIGMOD 2013.\n\n\n[29] SciKit-Learn. \nhttp://scikit-learn.org/stable/\n\n\n[30] Jeffrey Dean, et al. \u201cMapReduce: simplified data processing on large clusters\u201d, OSDI 2014.\n\n\n[31] J. Chen, et al. \u201cRevisiting distributed synchronous SGD\u201d, ICLR Workshop 2016.\n\n\n[32] H. Cui, et al. \u201cGeePS: Scalable deep learning on distributed GPUs with a GPU specialized parameter server\u201d, EuroSys 2016.\n\n\n[33] J. Dean, et al. \u201cLarge scale distributed deep networks\u201d, NIPS 2012.\n\n\n[34] T. Chilimbi, et al. \u201cProject Adam: Building an efficient and scalable deep learning training system\u201d, OSDI 2014.\n\n\n[35] M. Li, et al. \u201cScaling distributed machine learning with the Parameter Server\u201d, OSDI 2014.\n\n\n[36] Andrew Gibiansky. \nBringing HPC Techniques to Deep Learning\n\n\n[37] Alex Heye, et al. \nScalable Deep Learning with BigDL on the Urika-XC Software Suite\n\n\n[38] Shivaram Venkataraman, et al. \u201cDrizzle: Fast and Adaptable Stream Processing at Scale\u201d, SOSP 2017.\n\n\n[39] Shivaram Venkataraman, et al. \nAccelerating Deep Learning Training with BigDL and Drizzle on Apache Spark\n\n\n[40] Jason (Jinquan) Dai, et al. \nLeveraging Low Precision and Quantization for Deep Learning Using the Amazon EC2 C5 Instance and BigDL\n\n\n[41] JD. \nhttps://en.wikipedia.org/wiki/JD.com\n\n\n[42] Jason (Jinquan) Dai, et al. \nBuilding Large-Scale Image Feature Extraction with BigDL at JD.com\n\n\n[43] Wei Liu, et al. \u201cSSD: Single Shot MultiBox Detector\u201d, ECCV 2016.\n\n\n[44] Kevin Lin, et al. \u201cLearning Compact Binary Descriptors with Unsupervised Deep Neural Networks\u201d, CVPR 2016.\n\n\n[45] I. Sutskever, et al. \u201cSequence to sequence learning with neural networks\u201d, NIPS 2014. \n\n\n[46] Xingjian Shi, et al. \u201cConvolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting\u201d, NIPS 2015.\n\n\n[47] Jason (Jinquan) Dai, et al. \n\u201cUsing BigDL to Build Image Similarity-Based House Recommendations\u201d\n\n\n[48] Christian Szegedy, et al. \u201cGoing deeper with convolutions\u201d, CVPR 2015.\n\n\n[49] B. Zhou, et al. \u201cPlaces: A 10 million Image Database for Scene Recognition\u201d, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.\n\n\n[50] Karen Simonyan, at al. \n\u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d\n, 2014.\n\n\n[51]    Michael Isard, et al. \u201cDryad: distributed data-parallel programs from sequential building blocks\u201d, EuroSys 2017.\n\n\n[52]    DJ4J. https://deeplearning4j.org/\n\n\n[53]    TensorFrames. https://github.com/databricks/tensorframes\n\n\n[54]    Deep Learning Pipelines. https://github.com/databricks/spark-deep-learning\n\n\n[55]    CaffeOnSpark. https://github.com/yahoo/CaffeOnSpark\n\n\n[56]    TensorFlowOnSpark. https://github.com/yahoo/TensorFlowOnSpark\n\n\n[57]    Philipp Moritz, et al.  \u201cSparkNet: Training Deep Networks in Spark\u201d, ICLR 2016.", 
            "title": "BigDL"
        }, 
        {
            "location": "/wp-bigdl/#bigdl-a-distributed-deep-learning-framework-for-big-data", 
            "text": "Jason (Jinquan) Dai 1 , Yiheng Wang 1 , Xin Qiu 1 , Ding Ding 1 , Yao Zhang 2 \u01c2 , Yanzhang Wang 1 , Xianyan Jia 2 \u01c2 , Cherry (Li) Zhang 1 , Yan Wan 3 \u01c2 , Zhichao Li 1 , Jiao Wang 1 , Shengsheng Huang 1 , Zhongyuan Wu 1 , Yang Wang 1 , Yuhao Yang 1 , Bowen She 1 , Dongjie Shi 1 , Qi Lu 1 , Kai Huang 1 , Guoqiong Song 1  1 Intel Corporation,     2 Tencent Inc.,     3 Alibaba Group  \u01c2 Work was done when the author worked at Intel", 
            "title": "BigDL: A Distributed Deep Learning Framework for Big Data"
        }, 
        {
            "location": "/wp-bigdl/#abstract", 
            "text": "In this paper, we present BigDL, a distributed deep learning framework for Big Data platforms and workflows. It is implemented on top of Apache Spark, and allows users to write their deep learning applications as standard Spark programs (running directly on large-scale big data clusters in a distributed fashion). It provides an expressive, \u201cdata-analytics integrated\u201d deep learning programming model, so that users can easily build the end-to-end analytics + AI pipelines under a unified programing paradigm; by implementing an  AllReduce  like operation using existing primitives in Spark (e.g., shuffle, broadcast, and in-memory data persistence), it also provides a highly efficient \u201cparameter server\u201d style architecture, so as to achieve highly scalable, data-parallel distributed training. Since its initial open source release, BigDL users have built many analytics and deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark.", 
            "title": "Abstract"
        }, 
        {
            "location": "/wp-bigdl/#1-introduction", 
            "text": "Recent breakthroughs in artificial intelligence have brought deep learning to the forefront of new generations of data analytics; as the requirements and usage models expand, new systems and architecture beyond existing deep learning frameworks (e.g., Caffe [1], Torch [2], TensorFlow [3], MXNet [4], Chainer [5], etc.) have inevitably emerged. In particular, there is increasing demand from organizations to apply deep learning technologies (such as computer vision, natural language processing, generative adversary networks, etc.) to their big data platforms and pipelines. This emerging convergence of deep learning and big data analytics is driven by several important technology and industry trends:    Data scale drives deep learning process.  Today users are building even deeper, more complex neural networks to take advantage of the massive amount of data that they have access to. In practice, big data (e.g., Apache Hadoop [6] or Apache Spark [7]) clusters are ubiquitously deployed as the global data platform, where all the production data are stored and made available to all the users. Therefore, it is usually much more efficient to run the algorithm directly on the big data cluster where the data are stored and shared (than copying data to a separate infrastructure).    Real-world deep learning applications are complex big data pipelines,  which require a lot of data processing (such as cleaning, transformation, augmentation, feature extraction, etc.) beyond model training/inference. Therefore, it is much simpler and more efficient (for development and workflow management) to seamlessly integrate deep learning functionalities into existing big data workflow running on the same infrastructure, especially given the recent improvements that reduce deep learning training time from weeks to hours [9] or even minutes [10].    Deep learning is increasingly adopted by the big data and data science community.  Unfortunately, mainstream data engineers and data scientists are usually not deep learning experts; as the usages of deep learning expand and scale to larger deployment, it will be much more easier if these users can continue the use of familiar software tools and programming models (e.g., Spark [8] or even SQL) and existing big data cluster infrastructures to build their deep learning applications.    We have developed  BigDL  [11], a distributed deep learning framework for big data platforms and workflows. It is implemented as a library on top of Apache Spark, and allows users to write their large-scale deep learning applications (including model training, fine-tuning and inference) as standard Spark programs, which can run directly on existing big data (Hadoop or Spark) clusters. BigDL provides comprehensive support of deep learning technologies (neural network operations, layers, losses and optimizers); in particular, users can directly run existing models defined in other frameworks (such as TensorFlow, Keras [12], Caffe and Torch) on Spark in a distributed fashion.  BigDL also provides seamless integrations of deep learning technologies into the big data ecosystem. Not only can a BigDL program directly interact with different components in the Spark framework (e.g., DataFrames [13], Spark Streaming [14], ML Pipelines [15], etc.), it can also directly run in a variety of big data frameworks (such as Apache Storm [17], Apache Flink [18], Apache Kafka [19], etc.). Since its initial open source on Dec 30, 2016, BigDL has enabled many community users to build their deep learning applications (e.g., object detection, sequence-to-sequence generation, visual similarity, neural recommendations, fraud detection, etc.) on Spark and big data platforms.", 
            "title": "1. Introduction"
        }, 
        {
            "location": "/wp-bigdl/#2-programming-model", 
            "text": "BigDL is implemented on Apache Spark, a widely used cluster computing engine for big data analysis. Spark provides a comprehensive set of libraries for relational processing, streaming, graph processing [16] and machine learning (in Python, Scala or Java); as a result, one can easily build the end-to-end, \u201cdata-analytics integrated\u201d deep learning and AI pipelines (under a unified programing paradigm) using Spark and BigDL, as illustrated in Figure 1.  1    spark = SparkContext(appName= text_classifier , \u2026)\n2    //load input data: (text, label) pairs\n3    texts_rdd = spark.textFile( hdfs://... )\n4    //convert text to list of words\n5    words_rdd = texts_rdd.map(lambda text, label: \n6                               ([w for w in to_words(text)], label))\n7    //load GloVe embedding\n8    w2v = news20.get_glove_w2v(dim=\u2026)\n9    //convert word list to list of vertors using GloVe embeddings\n10   vector_rdd = words_rdd.map(lambda word_list, label:\n11                              ([to_vec(w, w2v) for w in word_list], label))\n12   //convert (list of vertors, label) pair to Sample\n13   sample_rdd = vector_rdd.map(lambda vector_list, label: \n14                                 to_sample(vector_list, label))\n15   //construct neural network model  \n16   model = Sequential().add(Recurrent().add(LSTM(\u2026)))\n17                       .add(Linear(\u2026))\n18                       .add(LogSoftMax())\n19   //train the model \n20   loss = ClassNLLCriterion()\n21   optim_method = Adagrad()\n22   optimizer = Optimizer(model=model, training_rdd=sample_rdd, \n23                         criterion=loss, optim_method= optim_method, \u2026)\n24   optimizer.set_train_summary(summary = TrainSummary(\u2026))\n25   trained_model =optimizer.optimize()\n26   //model prediction\n27   test_rdd = \u2026\n28   prediction_rdd = trained_model.predict(test_rdd)  Figure 1. The end-to-end text classification pipeline (including data loading, tokenization, word vectorization, training, prediction, etc.) on Spark and BigDL.", 
            "title": "2. Programming Model"
        }, 
        {
            "location": "/wp-bigdl/#21-spark", 
            "text": "Spark provides the  Resilient Distributed Dataset  (RDD) [8] in-memory storage abstraction, which is an immutable collection of Python or Scala/Java objects partitioned across a cluster, and can be transformed to derive new RDDs through data-parallel functional operators like  map, filter and reduce.  Consequently, users can efficiently load very large dataset and process the loaded data in a distributed fashion using Spark, and then feed the processed data into the analytics and AI pipeline. For example, lines 1 ~ 6 in Figure 1 illustrates how to load the input data (article texts and their associated labels) from the Hadoop Distributed File System (HDFS) [20], and transforms each text string into a list of words.", 
            "title": "2.1. Spark"
        }, 
        {
            "location": "/wp-bigdl/#22-data-transformation", 
            "text": "Spark supports general dataflow DAGs [8] by composing multiple data-parallel operators on RDD, where each vertex represents an RDD and each edge represents the transformation by the RDD operator. By constructing the dataflow DAG in Spark, users can easily transform the input data (for, e.g., image augmentations, word vectorizations, etc.), which can then be used by the neural network models. For example, lines 7 ~ 11 in Figure 1 illustrates how to apply GloVe word embedding [21] to transform each word to a vector.     N-dimensional array:  In BigDL, we model the basic data elements used in neural network computations as N-dimensional numeric (int8, float32, etc.) arrays. These arrays are represented by  numpy.ndarry  [22] and  BigDL.Tensor  (similar to  Torch.Tensor  [23]) for BigDL Python and Scala/Java APIs respectively.    Sample:  Each record used in BigDL model training and prediction is modelled as a Sample, which contains an input feature and an optional label. Each input feature is one or more N-dimensional arrays, while each label is either a scalar (float32) value, or one or more  N-dimensional arrays.  For instance, lines 12 ~ 14 in Figure 1 shows how to turn the transformed data into an RDD of  Samples,  which will later be used by BigDL model training.", 
            "title": "2.2. Data transformation"
        }, 
        {
            "location": "/wp-bigdl/#23-model-construction", 
            "text": "Similar to Torch and Keras, BigDL uses a dataflow representation for the neural network model, where each vertex in the dataflow graph represents a neural network layer (such as  ReLu, Spatial Convolution and LSTM ). BigDL then uses the semantics of the layers for model evaluation ( forward ) and gradient computation ( backward ). For example, lines 15 ~ 18 in Figure 1 illustrates the model definition used in the text classification example.", 
            "title": "2.3. Model Construction"
        }, 
        {
            "location": "/wp-bigdl/#24-model-training", 
            "text": "The transformed input data (RDD of Samples) and the constructed model can then be passed over to the  Optimizer  in BigDL, which automatically performs distributed model training across the cluster, as illustrated by lines 19 ~ 25 in  Figure 1.    Optimizer:  In BigDL, the distributed training process is modelled by the Optimizer abstraction, which runs multiple, iterative Spark jobs to minimize the loss (as defined by the user specified Criterion) using specific optimization method (such as  SGD, AdaGrad [24], Adam [25], etc. ).    Visualization:  To make it easy for users to understand the behaviors of model training, the  optimizer  in BigDL can be configured to produce a  TrainSummary  that contains various summary data (e.g., loss, weight, etc.), as illustrated by line 24 in Figure 1; the summary data can then be visualized in, for instance, TensorBoard [26] or Jupytor Notebooks [27].", 
            "title": "2.4. Model training"
        }, 
        {
            "location": "/wp-bigdl/#25-model-inference", 
            "text": "BigDL also allows users to directly use existing models (pre-trained by Caffe, Keras, TensorFlow, Torch or BigDL) in Spark, so as to directly perform model prediction in a distributed fashion (using RDD transformations), as illustrated by lines 26 ~ 28 in Figure 1.    ModelBroadcast:  BigDL provides the  ModelBroadcast  abstraction to manage the deployment of the pre-trained model across the cluster in a Spark job; the model prediction operation ( predict ) in BigDL uses  ModelBroadcast  to cache a single copy of the model on each machine (by leveraging the  broadcast  [28] mechanism in Spark), and manage the model cloning and weight sharing among different tasks in the same machine.", 
            "title": "2.5. Model Inference"
        }, 
        {
            "location": "/wp-bigdl/#26-spark-dataframe-and-ml-pipeline", 
            "text": "Besides RDD, Spark provides a high level  DataFrame  abstraction [13], which is a distributed collection of rows with a specific schema (similar to a table in a relational database), and implements data-parallel relational operators like  filter  and  join  for efficient structured data analysis. On top of DataFrame, Spark introduces a high level ML  (machine learning) pipeline  [15] similar to SciKit-Learn [29], which allows users to construct the machine learning workflow as a graph of transformations on data (e.g., feature extraction, normalization, model training, etc.). BigDL also provides native integration with the high level Spark DataFrame and ML Pipeline APIs (using its  DLModel  and  DLEstimator  abstractions).", 
            "title": "2.6. Spark DataFrame and ML Pipeline"
        }, 
        {
            "location": "/wp-bigdl/#3-execution-model", 
            "text": "Similar to other Big Data systems (such as MapReduce [30]), a Spark cluster consists of a single driver node and multiple worker nodes, as shown in Figure 2.  The driver node is responsible for coordinating the tasks in a Spark job (e.g., scheduling and dispatching), while the worker nodes are responsible for the actual computation and physical data storage. To automatically parallelize the large-scale data processing across the cluster in a fault-tolerant fashion, Spark provides a functional compute model where immutable RDDs are transformed through coarse-grained operators (i.e., applying the same operation to all data items).      Figure 2. A Spark job contains many Spark tasks; the driver node is responsible for scheduling and dispatching the tasks to worker nodes, which runs the actual Spark tasks.  On the other hand, efficient and distributed training of deep neural networks would necessitate very different operations (such as fine-grained data access and in-place data mutation [3]). In this section, we describe in details how BigDL supports highly efficient and scalable distributed training, directly on top of the data parallel and functional compute model of Spark (in addition to various optimizations for model inference).", 
            "title": "3. Execution Model"
        }, 
        {
            "location": "/wp-bigdl/#31-data-parallel-training", 
            "text": "To train a deep neural network model across the cluster, BigDL provides data-parallel training on Spark using synchronous mini-batch SGD, which is shown to achieve better scalability and efficiency (in terms of time-to-quality) compared to asynchronous training [31][32]. The distributed training in BigDL is implemented as an iterative process, as illustrated in Figure 3; each iteration runs a couple of Spark jobs to first compute the gradients using the current mini-batch, and then make a single update to the parameters of the neural network model.  for (i  - 1 to N) {\n  // model forward-backward  job\n  for each task in the Spark job:\n     read the latest weights\n     get a random batch of data from local Sample partition\n     compute errors (forward on local model replica)\n     compute gradients (backward on local model replica)\n  // parameter synchronization  job\n  aggregate (sum) all the gradients\n  update the weights per specified optimization method\n}  Figure 3. BigDL provides efficient, data-parallel, synchronous mini-batch SGD, where each iteration runs two Spark jobs for \u201cmodel forward-backward\u201d and \u201cparameter synchronization\u201d.  As described in Section 2, BigDL models the training data as an RDD of Samples, which are automatically partitioned and potentially cached in memory across the Spark cluster. In addition, to implement the data-parallel training, BigDL also constructs an RDD of models, each of which is a replica of the original neural network model. The model and Sample RDDs are co-partitioned and co-located [14] across the cluster, as shown in Figure 4; consequently, in each iteration of the model training, a single \u201cmodel forward-backward\u201d Spark job can apply the functional  zip  operator to the partitions of model and Sample RDDs, and compute the gradients in parallel for each model replica (using a small batch of data in the co-located Sample partition), as illustrated in Figure 4.     Figure 4. The \u201cmodel forward-backward\u201d spark job, which computes the local gradients for each model replica in parallel.", 
            "title": "3.1. Data-parallel training"
        }, 
        {
            "location": "/wp-bigdl/#32-parameter-synchronization", 
            "text": "Parameter synchronization is a performance critical operation for data-parallel training (in terms of speed and scalability). To support efficient parameter synchronization, existing deep learning frameworks usually implement the  parameter server  [33][34][35] architecture or  AllReduce  [36] operation, which unfortunately cannot be directly supported by the functional compute model provided by the Big Data systems.  In BigDL, we have adapted the primitives available in Spark (e.g.,  shuffle, broadcast, in-memory cache , etc.) to implement an efficient AllReduce-like operation, so as to mimic the functionality of a parameter server architecture (as illustrated in Figure 5).     Figure 5. Parameter synchronization in BigDL. Each local gradient (computed by a task in the \u201cmodel forward-backward\u201d job) is evenly divided into N partitions; then each task n in the \u201cparameter synchronization\u201d job aggregates these local gradients and update the weights for the nth partition.    A Spark job has  N  tasks, each of which is assigned a unique Id ranging from  1  to  N  in BigDL. After each task in the \u201c model forward-backward \u201d job computes the local gradients (as described in section 3.1), it evenly divides the local gradients into  N  partitions, as shown in Figure 5.    Next, another \u201c parameter synchronization \u201d job is launched; each task  n  in the \u201c parameter synchronization \u201d job is responsible for managing the n th  partition of the parameters, just like a parameter server (as shown in Figure 6). Specifically, the n th  partition of the gradients (from all the tasks of the previous \u201c model forward-backward \u201d job) are first  shuffled  to task  n , which then aggregates (sums) these gradients, and applies the updates to the n th  partition of the weights (using the specific  optimization method ), as illustrated in Figure 5.       Figure 6. The \u201cparameter synchronization\u201d Spark job, manages the n th  partition of the parameters (similar to a parameter server).    After that, each task  n  in the \u201c parameter synchronization \u201d job  broadcasts  the n th  partition of the updated weights; consequently, tasks in the \u201c model forward-backward \u201d job of the next iteration can read the latest value of all the weights before the next training step begins.    The  shuffle  and  task-side broadcast  operations described above are implemented on top of the distributed  in-memory  storage in Spark: both the shuffled  gradients  and broadcasted  weights  are materialized in memory, which can be read remotely by the Spark tasks with extremely low latency.    By implementing the AllReduce operation using primitives in Spark, BigDL provides a highly efficient \u201cparameter server\u201d style architecture directly on top of Big Data frameworks. As a result, it is demonstrated to support highly scalable distributed training on up to 256-node, as reported by Cray [37] and shown in Figure 7.      Figure 7. Throughput of ImageNet Inception v1 training reported by Cary [37] (using BigDL 0.3.0 and dual-socket Intel Broadwell 2.1 GHz); the training throughput scales almost linear up to 128 nodes (and continue to scale reasonably up to 256 nodes).", 
            "title": "3.2. Parameter synchronization"
        }, 
        {
            "location": "/wp-bigdl/#33-task-scheduling", 
            "text": "While BigDL provides a highly efficient \u201cparameter server\u201d style architecture, it has a fundamentally different implementation than existing deep learning frameworks. In particular, existing deep learning frameworks are typically deployed as multiple long-running, potentially stateful tasks [3], which interact with each other (in a blocking fashion to support synchronous mini-batch SGD) for model computation and parameter synchronization.  In contrast, BigDL runs a series of short-lived Spark jobs (e.g., two jobs per mini-batch as described in earlier sections), and each task in the job is stateless and non-blocking. As a result, BigDL programs can automatically adapt to the dynamic resource changes (e.g., preemption, failures, incremental scaling, resource sharing, etc.) in a timely fashion. On the other hand, task scheduling in Spark can become a potential bottleneck of the distributed training on a large cluster. For instance, Figure 8 shows that, for ImageNet Inception v1 training, the overhead of launching tasks (as a fraction of average compute time) in BigDL, while low for 100~200 tasks, can grows to over 10% when there are close to 500 tasks [39]. To address this issue, BigDL will launch a single, multi-threaded task on each worker, so as to achieve high scalability on large clusters (e.g., up to 256 servers as shown in Figure 7 above).   To scale to an even larger number (e.g., 500) of workers, one can potentially leverages the iterative nature of the model training (in which the same operations are executed repeatedly). For instance, group scheduling introduced by  Drizzle  [38] (a low latency execution engine for Spark) can help schedule multiple iterations (or a group) of computations at once, so as to greatly reduce scheduling overheads even if there are a large number of tasks, as benchmarked by RISELab [39] and shown in Figure 8.     Figure 8. Overheads of task scheduling and dispatch (as a fraction of average compute time) for ImageNet Inception v1 training in BigDL [39].", 
            "title": "3.3. Task scheduling"
        }, 
        {
            "location": "/wp-bigdl/#34-model-quantization", 
            "text": "Quantization refers to using technologies that store numbers and perform calculations on them in more compact and lower precision form (than their original format such as 32-bit floating point). BigDL takes advantage of this type of low precision computing to quantize existing models (which can be pre-trained by various frameworks such as Caffe, Keras, TensorFlow, Torch or BigDL) for optimized inference.  BigDL first loads existing models and then quantizes the parameters of some selected layers (e.g., Spatial Convolution) into 8-bit integer (using the equation shown in Figure 9) to produce a quantized model. During model inference, each quantized layer quantizes the input (float32) data into 8-bit integer on the fly, applies the 8-bit calculations (such as GEMM) using the quantized parameters and data, and dequantizes the results to 32-bit floating point. Many of these operations can be fused in the implementation, and consequently the quantization and dequantization overheads are very low at inference time.  Math.round(1.0 * value \n           / Math.max(Math.abs(max), Math.abs(min)) \n           * Byte.MaxValue).toByte  Figure 9. Equation for quantizing 32-bit floating point to 8-bit integer.  Unlike many existing quantization implementations, BigDL adopts a new local quantization scheme. That is, it performs the quantization and dequantization operations (as described above) in each small local quantization window, a small sub-block (such as a patch or kernel in convolution) of the parameters or input data. As a result, BigDL can use very low bit integers, such as 8-bit, in model quantization with extremely low model accuracy drop (less than 0.1%), 4x model size reduction, and up to 2x inference speedup, as benchmarked on AWS EC2 [40] and shown in Figure 10.   Figure 10. Model quantization results (accuracy, inference speed and model size) for SSD, VGG16 and VGG19 (using BigDL 0.3.0 and AWS EC2 C5.18xlarge instances) [40].", 
            "title": "3.4. Model quantization"
        }, 
        {
            "location": "/wp-bigdl/#35-local-execution", 
            "text": "In addition to being a standard Spark program, BigDL also provide support to run the model training and inference on a local JVM (without Spark). This helps improve the efficiency when running BigDL on a single node, as there are no overheads such as parameter synchronizations or task scheduling. More importantly, it makes it easy to directly integrate BigDL models (for either inference or fine-tuning) with various big data frameworks, such as Apache Storm, Apache Flink or Apache Kafka, which are usually JVM based.", 
            "title": "3.5. Local execution"
        }, 
        {
            "location": "/wp-bigdl/#4-applications", 
            "text": "Since its initial open source release (on Dec 30, 2016), BigDL users have built many deep learning applications on Spark and Big Data platforms. In this section, we describes three typical use cases (namely, model inference, distributed training and transfer learning) using Spark and BigDL.", 
            "title": "4. Applications"
        }, 
        {
            "location": "/wp-bigdl/#41-model-inference-image-feature-extraction", 
            "text": "JD.com [41] is one of the largest online retailers in the world. It has built an end-to-end  object detection and image feature extraction  pipeline on top of Spark and BigDL[42], as illustrated in Figure 11.     Figure 11. End-to-end object detection and image feature extraction pipeline (using SSD and DeepBit models) on top of Spark and BigDL [42].    The pipeline first reads hundreds of millions of pictures from a distributed database into Spark (as an RDD of pictures), and then pre-processes the RDD of pictures (including  resizing ,  normalization , and  batching ) in a distributed fashion using Spark.    After that, it uses BigDL to load a  SSD  [43] model (pre-trained in Caffe) for large scale, distributed object detection on Spark, which generates the coordinates and scores for the detected objects in each of the pictures.    It then generates the target images (by keeping the object with highest score as the target, and cropping the original picture based on the coordinates of the target), and further pre-processes the RDD of target images (including  resizing  and  batching ).    Finally it uses BigDL to load a  DeepBit  [44] model (again pre-trained in Caffe) for distributed feature extraction of the target images to generate the corresponding features, and stores the results (RDD of extracted object features) in the Hadoop Distributed File System (HDFS).    The entire data analytics and deep learning pipeline, including data loading, partitioning, preprocessing, model inference, and storing the results, can be easily implemented under a unified programming paradigm (using Spark and BigDL). In addition, the end-to-end pipeline also delivers ~3.83x speedup compared to running the same solution on a GPU cluster, as reported by JD [42] and shown in Figure 12.     Figure 12. Throughput of GPU clusters and Xeon clusters for the image feature extraction pipeline benchmarked by JD [42]; the GPU throughput is tested on 20 NVIDIA Tesla K40 cards, and the Xeon throughput is tested on 1200 logical cores (where each dual-socket Intel Xeon E5-2650 v4 server runs 50 logical cores).", 
            "title": "4.1. Model Inference: image feature extraction"
        }, 
        {
            "location": "/wp-bigdl/#42-distributed-training-precipitation-nowcasting", 
            "text": "Cray has integrated BigDL to their Urika-XC analytics software suite, and built an end-to-end precipitation nowcasting ( predicting short-term precipitation ) workflow on spark and BigDL[37], including data preparation, model training and inference (as illustrated in Figure 13).      Figure 13. End-to-end precipitation nowcasting workflow (using sequence-to-sequence model) [37] on Spark and BigDL.    The application first reads over a terabyte of raw radar scan data into Spark (as an RDD of radar images), and then converts it into an RDD of  NumPy ndarrays .    It then trains a  sequence-to-sequence  model [45][46] (as illustrated in Figure 13), using a sequence of images leading up to the current time as the input, and a sequence of predicted images in the future as the output.    After the model is trained, it can be used to predict, say, precipitation patterns for the next hour, as illustrated in Figure 14.       Figure 14. Predicting precipitation patterns for the next hour (i.e., a sequence of images for the future time steps of the next hour) on Spark and BigDL [37]", 
            "title": "4.2. Distributed training: precipitation nowcasting"
        }, 
        {
            "location": "/wp-bigdl/#43-transfer-learning-image-similarity-based-house-recommendations", 
            "text": "MLSListings Inc. is a large  Multiple Listing Service  (MLS) for real estate listings, who has been building an image-similarity based house recommendation system on Spark and BigDL [47]. The end-to-end workflow is implemented by leveraging transfer learning (including feature extractions and fine-tuning) technologies, so as to compute both the semantic and visual similarity of the house photos, as illustrated in Figure 15.     Figure 15. End-to-end workflow for image-similarity based house recommendations on Spark and BigDL [47]  To compute the  semantic similarity  for the photos, the system fine-tunes the Inception v1 [48] model pre-trained on the Places dataset [49], so as to train three new classifiers (namely, whether the photo shows the house front exterior, the house style and the house stories). In particular, it first loads three pre-trained Inception v1 models, and then appends two new layers (a fully-connected layer followed by a Softmax layer) to each model, so as to train the new classifiers (using photos for which MLSListings have been assigned copyrights). After the training, it can use these classifiers to produce the tags (or labels) for each house listing photo.  To compute the visual similarity, the system use the VGG-16 [50] model pre-trained on the Places dataset to extract the image feature for each house listing photo, which is then combined with the tags generated by the classifiers and stored into a distributed table storage.  At  model serving  time, the user can select a house listing photo, and have the system to recommend house listings of similar visual characteristics (by computing the cosine similarity score using the image features, while taking into considerations other properties of the houses such as photo tags, house prices, locations, etc.), as illustrated in the  \u201cSimilar Houses\u201d  section of the webpage in Figure 16.     Figure 16. Automatically recommending \u201cSimilar Houses\u201d with similar visual characteristics [47]", 
            "title": "4.3. Transfer learning: image-similarity based house recommendations"
        }, 
        {
            "location": "/wp-bigdl/#5related-work", 
            "text": "Existing big data systems, such as MapReduce [30], Dryad [51] and Spark [8], provide a data-parallel, functional compute model (with potentially dataflow DAG support), so as to efficiently support data partitioning, parallel and distributed computing, fault tolerance, incremental scale-out, etc., in an automatic and transparent fashion. BigDL is built on top of this data-parallel, functional compute model, and adds new support of deep learning technologies to Apache Spark, so as to provide the \u201cdata-analytics integrated\u201d deep learning programming model.  Existing deep learning frameworks, such as Caffe [1], Torch [2], TensorFlow [3], Keras [12], MXNet [4] and DL4J [52], usually use a dataflow graph (of either primitive operators or more complex layers) to represent neural network models. For distributed training, they typically implement the parameter server architecture or AllReduce operation (with fine-grained data access and in-place data mutation [3]), which are however not supported by existing big data systems. In contrast, BigDL adopts the similar dataflow representation of neural network models, but provides efficient distributed training directly on top of Apache Spark.  Recently there are also a lot of efforts to bring existing deep learning frameworks to Apache Spark. For instance, TensorFrames [53] and Deep Learning Pipelines [54] allow users to directly run TensorFlow or Keras models on each individual partition of Spark Dataframes, for both model inference and single-node model tuning; however, they do not support distributed model training or fine-tuning across multiple machines in a cluster. CaffeOnSpark [55] and TensorFlowOnSpark [56] frameworks use Spark as the orchestration layer to allocate resources from the cluster, and then launch the distributed Caffe or TensorFlow job on the allocated machines; however, the Caffe or TensorFlow job still runs outside of the big data framework, and has very limited interactions with the analytics pipelines. SparkNet [57] uses asynchronous SGD for distributed training on Spark; the master first broadcasts weights to the workers, and each work then trains its own Caffe model for a certain period of time, after which the weights on each worker are sent to the master and averaged to form the new weights; however, the broadcast and weight averaging is very inefficient in SparkNet (e.g., ~20 seconds with just 5 workers [57]). In contrast, BigDL provides highly efficient and scalable distributed training, directly on top of big data framework (using the primitives available in Spark).", 
            "title": "5.Related Work"
        }, 
        {
            "location": "/wp-bigdl/#6-summary", 
            "text": "We have described BigDL, including its programming model, execution model and typical use cases. It combines the benefits of big data and HPC (high performance computing) architecture, so as to provide both an expressive, \u201cdata-analytics integrated\u201d deep learning programing model for users to build their analytics + AI pipelines, and a highly efficient \u201cparameter server\u201d style architecture directly on top of Big Data platforms for scalable data-parallel training.   BigDL is a work in progress, but our initial experience is encouraging. Since its initial open source release (on Dec 30, 2016), it has received over 2400 stars on Github; and it have enabled many users to build new analytics and deep learning applications, which can directly run on top of existing Hadoop and/or Spark clusters.", 
            "title": "6. Summary"
        }, 
        {
            "location": "/wp-bigdl/#7-acknowledgement", 
            "text": "We gratefully acknowledge contributions from our (current and former) colleagues at Intel (including Jun Wang, Liangying Lv, Andy Chen, Yan Dai, Sergey Ermolin, Zewei Chen, Ning Wang, Yulia Tell, Pengfei Yue, Wesley Du, Erjin Ren, Xiao Dong Wang, Radhika Rangarajan, Jack Chen, Milind Damle and Dave Nielsen), and numerous users and collaborators from the open source community (including Shivaram Venkataraman, Xiao Xu, Zhenhua Wang, Alex Heye, Omid Khanmohamadi, Mike Ringenburg, Joseph Spisak, Gopi Kumar, Suqiang Song, Karthik Palaniappan, Rong Gu, etc.) to the BigDL project.", 
            "title": "7. Acknowledgement"
        }, 
        {
            "location": "/wp-bigdl/#8-reference", 
            "text": "[1] Caffe.  http://caffe.berkeleyvision.org  [2] Torch.  http://torch.ch  [3] Mart\u00edn Abadi, et al. \u201cTensorFlow: A System for Large-Scale Machine Learning\u201d, OSDI 2016.  [4] Tianqi Chen, et al. \u201cMXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems\u201d, LearningSys 2015.  [5] Seiya Tokui, et al. \u201cChainer: a Next-Generation Open Source Framework for Deep Learning\u201d. LearningSys 2015.  [6] Apache Hadoop.  http://hadoop.apache.org  [7] Apache Spark.  https://spark.apache.org  [8] Matei Zaharia , et al. \u201cResilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing\u201d, NSDI 2012.  [9] Priya Goyal, et al. \u201cAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour\u201d, arXiv: 1706.02677 [cs.CV]  [10] Yang You, et al. \u201cImageNet Training in Minutes\u201d, arXiv:1709.05011 [cs.CV]  [11] BigDL.  https://github.com/intel-analytics/BigDL/  [12] Keras.  https://keras.io  [13] Michael Armbrust, et al. \u201cSpark SQL: Relational Data Processing in Spark\u201d, SIGMOD 2015.  [14] Matei Zaharia, et al. \u201cDiscretized Streams: Fault-Tolerant Streaming Computation at Scale\u201d, SOSP 2013.  [15] Xiangrui Meng, et al. \u201cMLlib: Machine Learning in Apache Spark\u201d, Journal of Machine Learning Research (JMLR) 2016.  [16] Reynold S. Xin, et al. \u201cGraphX: Unifying Data-Parallel and Graph-Parallel Analytics\u201d, OSDI 2014.  [17] Apache Storm.  http://storm.apache.org  [18] Apache Flink.  https://flink.apache.org  [19] Apache Kafka.  https://kafka.apache.org  [20] Konstantin Shvachko, et al. \u201cThe Hadoop Distributed File System\u201d, MSST 2010.  [21] Jeffrey Pennington, et al. \u201cGloVe: Global Vectors for Word Representation\u201d, EMNLP 2014.  [22] Numpy.  http://www.numpy.org  [23] Torch7.  https://github.com/torch/torch7  [24] J. Duchi, et al. \u201cAdaptive subgradient methods for online learning and stochastic optimization.\u201d Journal of Machine Learning Research (JMLR) 2011.  [25] Diederik P. Kingma, et al. \u201cAdam: A Method for Stochastic Optimization\u201d, ICLR 2015.  [26] M. Abadi, et al. \u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems. , 2016.  [27] Project Jupyter.  http://jupyter.org  [28] Reynold Xin, et al. \u201cShark: SQL and Rich Analytics at Scale\u201d, SIGMOD 2013.  [29] SciKit-Learn.  http://scikit-learn.org/stable/  [30] Jeffrey Dean, et al. \u201cMapReduce: simplified data processing on large clusters\u201d, OSDI 2014.  [31] J. Chen, et al. \u201cRevisiting distributed synchronous SGD\u201d, ICLR Workshop 2016.  [32] H. Cui, et al. \u201cGeePS: Scalable deep learning on distributed GPUs with a GPU specialized parameter server\u201d, EuroSys 2016.  [33] J. Dean, et al. \u201cLarge scale distributed deep networks\u201d, NIPS 2012.  [34] T. Chilimbi, et al. \u201cProject Adam: Building an efficient and scalable deep learning training system\u201d, OSDI 2014.  [35] M. Li, et al. \u201cScaling distributed machine learning with the Parameter Server\u201d, OSDI 2014.  [36] Andrew Gibiansky.  Bringing HPC Techniques to Deep Learning  [37] Alex Heye, et al.  Scalable Deep Learning with BigDL on the Urika-XC Software Suite  [38] Shivaram Venkataraman, et al. \u201cDrizzle: Fast and Adaptable Stream Processing at Scale\u201d, SOSP 2017.  [39] Shivaram Venkataraman, et al.  Accelerating Deep Learning Training with BigDL and Drizzle on Apache Spark  [40] Jason (Jinquan) Dai, et al.  Leveraging Low Precision and Quantization for Deep Learning Using the Amazon EC2 C5 Instance and BigDL  [41] JD.  https://en.wikipedia.org/wiki/JD.com  [42] Jason (Jinquan) Dai, et al.  Building Large-Scale Image Feature Extraction with BigDL at JD.com  [43] Wei Liu, et al. \u201cSSD: Single Shot MultiBox Detector\u201d, ECCV 2016.  [44] Kevin Lin, et al. \u201cLearning Compact Binary Descriptors with Unsupervised Deep Neural Networks\u201d, CVPR 2016.  [45] I. Sutskever, et al. \u201cSequence to sequence learning with neural networks\u201d, NIPS 2014.   [46] Xingjian Shi, et al. \u201cConvolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting\u201d, NIPS 2015.  [47] Jason (Jinquan) Dai, et al.  \u201cUsing BigDL to Build Image Similarity-Based House Recommendations\u201d  [48] Christian Szegedy, et al. \u201cGoing deeper with convolutions\u201d, CVPR 2015.  [49] B. Zhou, et al. \u201cPlaces: A 10 million Image Database for Scene Recognition\u201d, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.  [50] Karen Simonyan, at al.  \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d , 2014.  [51]    Michael Isard, et al. \u201cDryad: distributed data-parallel programs from sequential building blocks\u201d, EuroSys 2017.  [52]    DJ4J. https://deeplearning4j.org/  [53]    TensorFrames. https://github.com/databricks/tensorframes  [54]    Deep Learning Pipelines. https://github.com/databricks/spark-deep-learning  [55]    CaffeOnSpark. https://github.com/yahoo/CaffeOnSpark  [56]    TensorFlowOnSpark. https://github.com/yahoo/TensorFlowOnSpark  [57]    Philipp Moritz, et al.  \u201cSparkNet: Training Deep Networks in Spark\u201d, ICLR 2016.", 
            "title": "8. Reference"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/", 
            "text": "NNEstimator\n\n\nScala:\n\n\nval estimator = NNEstimator(model, criterion)\n\n\n\n\nPython:\n\n\nestimator = NNEstimator(model, criterion)\n\n\n\n\nNNEstimator\n extends \norg.apache.spark.ml.Estimator\n and supports training a BigDL\nmodel with Spark DataFrame data. It can be integrated into a standard Spark ML Pipeline\nto allow users to combine the components of BigDL and Spark MLlib.\n\n\nNNEstimator\n supports different feature and label data types through \nPreprocessing\n.\nDuring fit (training), NNEstimator will extract feature and label data from input DataFrame and use\nthe \nPreprocessing\n to convert data for the model, typically converts the feature and label\nto Tensors or converts the (feature, option[Label]) tuple to a BigDL \nSample\n. \n\n\nEach\nPreprocessing\n conducts a data conversion step in the preprocessing phase, multiple\n\nPreprocessing\n can be combined into a \nChainedPreprocessing\n. Some pre-defined \n\nPreprocessing\n for popular data types like Image, Array or Vector are provided in package\n\ncom.intel.analytics.zoo.feature\n, while user can also develop customized \nPreprocessing\n.\n\n\nBy default, \nSeqToTensor\n is used to convert an array or Vector to a 1-dimension Tensor.\nUsing the \nPreprocessing\n allows \nNNEstimator\n to cache only the raw data and decrease the \nmemory consumption during feature conversion and training, it also enables the model to digest\nextra data types that DataFrame does not support currently.\n\n\nMore concrete examples are available in package \ncom.intel.analytics.zoo.examples.nnframes\n\n\nNNEstimator\n can be created with various parameters for different scenarios.\n\n\n1.\n \nNNEstimator(model, criterion)\n\n\nTakes only model and criterion and use \nSeqToTensor\n as feature and label\n   \nPreprocessing\n. \nNNEstimator\n will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL \nSample\n and send to model for\n   training.\n\n\n2.\n \nNNEstimator(model, criterion, featureSize: Array[Int], labelSize: Array[Int])\n\n\nTakes model, criterion, featureSize(Array of Int) and labelSize(Array of Int). \nNNEstimator\n\n   will extract the data from feature and label columns (only Scalar, Array[_] or Vector data\n   type are supported) and convert each feature/label to Tensor according to the specified Tensor\n   size.\n\n\n3.\n \nNNEstimator(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]],\nlabelPreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model, criterion, featurePreprocessing and labelPreprocessing.  \nNNEstimator\n\n   will extract the data from feature and label columns and convert each feature/label to Tensor\n   with the featurePreprocessing and labelPreprocessing. This constructor provides more flexibility\n   in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNEstimator\n supports:\n\nsetSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])\n to directly compose\nSample according to user-specified Preprocessing.\n\n\nScala Example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNEstimator\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNEstimator(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0)),\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0))))\nval df = sqlContext.createDataFrame(data).toDF(\nfeatures\n, \nlabel\n)\nval nnModel = estimator.fit(df)\nnnModel.transform(df).show(false)\n\n\n\n\nPython Example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom zoo.pipeline.nnframes.nn_classifier import *\nfrom zoo.feature.common import *\n\ndata = self.sc.parallelize([\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0)),\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0))])\n\nschema = StructType([\n    StructField(\nfeatures\n, ArrayType(DoubleType(), False), False),\n    StructField(\nlabel\n, ArrayType(DoubleType(), False), False)])\ndf = self.sqlContext.createDataFrame(data, schema)\nmodel = Sequential().add(Linear(2, 2))\ncriterion = MSECriterion()\nestimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2]))\\\n    .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(40)\nnnModel = estimator.fit(df)\nres = nnModel.transform(df)\n\n\n\n\n\n\nNNModel\n\n\nScala:\n\n\nval nnModel = NNModel(bigDLModel)\n\n\n\n\nPython:\n\n\nnn_model = NNModel(bigDLModel)\n\n\n\n\nNNModel\n extends Spark's ML\n\nTransformer\n. User can invoke\n\nfit\n in \nNNEstimator\n to get a \nNNModel\n, or directly compose a \nNNModel\n from BigDLModel.\nIt enables users to wrap a pre-trained BigDL Model into a NNModel,\nand use it as a transformer in your Spark ML pipeline to predict the results for \nDataFrame\n(DataSet)\n. \n\n\nNNModel\n can be created with various parameters for different scenarios.\n\n\n1.\n \nNNModel(model)\n\n\nTakes only model and use \nSeqToTensor\n as feature Preprocessing. \nNNModel\n will extract the\n   data from feature column (only Scalar, Array[_] or Vector data type are supported) and\n   convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.\n\n\n2.\n \nNNModel(model, featureSize: Array[Int])\n\n\nTakes model and featureSize(Array of Int). \nNNModel\n will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.\n\n\n3.\n \nNNModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model and featurePreprocessing. \nNNModel\n will extract the data from feature column\n   and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNModel\n supports:\n\nsetSamplePreprocessing(value: Preprocessing[Any, Sample[T]])\nto directly compose\nSample according to user-specified Preprocessing.\n\n\n\n\nNNClassifier\n\n\nScala:\n\n\nval classifer =  NNClassifer(model, criterion)\n\n\n\n\nPython:\n\n\nclassifier = NNClassifer(model, criterion)\n\n\n\n\nNNClassifier\n is a specialized \nNNEstimator\n that simplifies the data format for\nclassification tasks where the label space is discrete. It only supports label column of\nDoubleType, and the fitted \nNNClassifierModel\n will have the prediction column of \nDoubleType.\n\n\n\n\nmodel\n BigDL module to be optimized in the fit() method\n\n\ncriterion\n the criterion used to compute the loss and the gradient\n\n\n\n\nNNClassifier\n can be created with various parameters for different scenarios.\n\n\n1.\n \nNNClassifier(model, criterion)\n\n\nTakes only model and criterion and use \nSeqToTensor\n as feature and label\n   Preprocessing. \nNNClassifier\n will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL samples and send to model for\n   training.\n\n\n2.\n \nNNClassifier(model, criterion, featureSize: Array[Int])\n\n\nTakes model, criterion, featureSize(Array of Int). \nNNClassifier\n\n   will extract the data from feature and label columns and convert each feature to Tensor\n   according to the specified Tensor size. \nScalarToTensor\n is used to convert the label column.\n\n\n3.\n \nNNClassifier(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model, criterion and featurePreprocessing.  \nNNClassifier\n\n   will extract the data from feature and label columns and convert each feature to Tensor\n   with the featurePreprocessing. This constructor provides more flexibility\n   in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNClassifier\n supports:\n\nsetSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])\n to directly compose\nSample with user-specified Preprocessing.\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNClassifier\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNClassifier(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0),\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0)))\nval df = sqlContext.createDataFrame(data).toDF(\nfeatures\n, \nlabel\n)\nval dlModel = estimator.fit(df)\ndlModel.transform(df).show(false)\n\n\n\n\nPython Example:\n\n\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom bigdl.dlframes.dl_classifier import *\nfrom pyspark.sql.types import *\n\n#Logistic Regression with BigDL layers and Analytics zoo NNClassifier\nmodel = Sequential().add(Linear(2, 2)).add(LogSoftMax())\ncriterion = ClassNLLCriterion()\nestimator = NNClassifier(model, criterion, [2]).setBatchSize(4).setMaxEpoch(10)\ndata = sc.parallelize([\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0]),\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0])])\n\nschema = StructType([\n    StructField(\nfeatures\n, ArrayType(DoubleType(), False), False),\n    StructField(\nlabel\n, ArrayType(DoubleType(), False), False)])\ndf = sqlContext.createDataFrame(data, schema)\ndlModel = estimator.fit(df)\ndlModel.transform(df).show(False)\n\n\n\n\nNNClassifierModel\n\n\nScala:\n\n\nval nnClassifierModel = NNClassifierModel(model, featureSize)\n\n\n\n\nPython:\n\n\nnn_classifier_model = NNClassifierModel(model)\n\n\n\n\nNNClassifierModel is a specialized \nNNModel\n for classification tasks.\nBoth label and prediction column will have the datatype of Double.\n\n\nNNClassifierModel\n can be created with various parameters for different scenarios.\n\n\n1.\n \nNNClassifierModel(model)\n\n\nTakes only model and use \nSeqToTensor\n as feature Preprocessing. \nNNClassifierModel\n will\n   extract the data from feature column (only Scalar, Array[_] or Vector data type are supported)\n   and convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.\n\n\n2.\n \nNNClassifierModel(model, featureSize: Array[Int])\n\n\nTakes model and featureSize(Array of Int). \nNNClassifierModel\n will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.\n\n\n3.\n \nNNClassifierModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])\n\n\nTakes model and featurePreprocessing. \nNNClassifierModel\n will extract the data from feature\n   column and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.\n\n\nMeanwhile, for advanced use cases (e.g. model with multiple input tensor), \nNNClassifierModel\n\nsupports: \nsetSamplePreprocessing(value: Preprocessing[Any, Sample[T]])\nto directly compose\nSample according to user-specified Preprocessing.\n\n\n\n\nHyperparameter setting\n\n\nPrior to the commencement of the training process, you can modify the optimization algorithm, batch \nsize, the epoch number of your training, and learning rate to meet your goal or\n\nNNEstimator\n/\nNNClassifier\n will use the default value.\n\n\nContinue the codes above, NNEstimator and NNClassifier can be set in the same way.\n\n\nScala:\n\n\n//for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())\n//for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())\n\n\n\n\nPython:\n\n\n# for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())\n# for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())\n\n\n\n\n\nPrepare the data and start the training process\n\n\nNNEstimator/NNCLassifer supports training with Spark's\n\nDataFrame/DataSet\n\n\nSuppose \ndf\n is the training data, simple call \nfit\n method and let Analytics Zoo train the model\nfor you.\n\n\nScala:\n\n\n//get a NNClassifierModel\nval nnClassifierModel = classifier.fit(df)\n\n\n\n\nPython:\n\n\n# get a NNClassifierModel\nnnClassifierModel = classifier.fit(df)\n\n\n\n\nUser may also set validation DataFrame and validation frequency through \nsetValidation\n method.\nTrain summay and validation summary can also be configured to log the training process for\nvisualization in Tensorboard.\n\n\nMake prediction on chosen data\n\n\nSince \nNNModel\n/\nNNClassifierModel\n inherits from Spark's \nTransformer\n abstract class, simply call \n\ntransform\n method on \nNNModel\n/\nNNClassifierModel\n to make prediction.\n\n\nScala:\n\n\nnnModel.transform(df).show(false)\n\n\n\n\nPython:\n\n\nnnModel.transform(df).show(false)\n\n\n\n\nFor the complete examples of NNFrames, please refer to:\n\nScala examples\n\n\nPython examples\n\n\nNNImageReader\n\n\nNNImageReader\n is the primary DataFrame-based image loading interface, defining API to read images\ninto DataFrame.\n\n\nScala:\n\n\n    val imageDF = NNImageReader.readImages(imageDirectory, sc)\n\n\n\n\nPython:\n\n\n    image_frame = NNImageReader.readImages(image_path, self.sc)\n\n\n\n\nThe output DataFrame contains a sinlge column named \"image\". The schema of \"image\" column can be\naccessed from \ncom.intel.analytics.zoo.pipeline.nnframes.DLImageSchema.byteSchema\n.\nEach record in \"image\" column represents one image record, in the format of\nRow(origin, height, width, num of channels, mode, data), where origin contains the URI for the image file,\nand \ndata\n holds the original file bytes for the image file. \nmode\n represents the OpenCV-compatible\ntype: CV_8UC3, CV_8UC1 in most cases.\n\n\n  val byteSchema = StructType(\n    StructField(\norigin\n, StringType, true) ::\n      StructField(\nheight\n, IntegerType, false) ::\n      StructField(\nwidth\n, IntegerType, false) ::\n      StructField(\nnChannels\n, IntegerType, false) ::\n      // OpenCV-compatible type: CV_8UC3, CV_32FC3 in most cases\n      StructField(\nmode\n, IntegerType, false) ::\n      // Bytes in OpenCV-compatible order: row-wise BGR in most cases\n      StructField(\ndata\n, BinaryType, false) :: Nil)\n\n\n\n\nAfter loading the image, user can compose the preprocess steps with the \nPreprocessing\n defined\nin \ncom.intel.analytics.zoo.feature.image\n.", 
            "title": "NNFrames"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnestimator", 
            "text": "Scala:  val estimator = NNEstimator(model, criterion)  Python:  estimator = NNEstimator(model, criterion)  NNEstimator  extends  org.apache.spark.ml.Estimator  and supports training a BigDL\nmodel with Spark DataFrame data. It can be integrated into a standard Spark ML Pipeline\nto allow users to combine the components of BigDL and Spark MLlib.  NNEstimator  supports different feature and label data types through  Preprocessing .\nDuring fit (training), NNEstimator will extract feature and label data from input DataFrame and use\nthe  Preprocessing  to convert data for the model, typically converts the feature and label\nto Tensors or converts the (feature, option[Label]) tuple to a BigDL  Sample .   Each Preprocessing  conducts a data conversion step in the preprocessing phase, multiple Preprocessing  can be combined into a  ChainedPreprocessing . Some pre-defined  Preprocessing  for popular data types like Image, Array or Vector are provided in package com.intel.analytics.zoo.feature , while user can also develop customized  Preprocessing .  By default,  SeqToTensor  is used to convert an array or Vector to a 1-dimension Tensor.\nUsing the  Preprocessing  allows  NNEstimator  to cache only the raw data and decrease the \nmemory consumption during feature conversion and training, it also enables the model to digest\nextra data types that DataFrame does not support currently.  More concrete examples are available in package  com.intel.analytics.zoo.examples.nnframes  NNEstimator  can be created with various parameters for different scenarios.  1.   NNEstimator(model, criterion)  Takes only model and criterion and use  SeqToTensor  as feature and label\n    Preprocessing .  NNEstimator  will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL  Sample  and send to model for\n   training.  2.   NNEstimator(model, criterion, featureSize: Array[Int], labelSize: Array[Int])  Takes model, criterion, featureSize(Array of Int) and labelSize(Array of Int).  NNEstimator \n   will extract the data from feature and label columns (only Scalar, Array[_] or Vector data\n   type are supported) and convert each feature/label to Tensor according to the specified Tensor\n   size.  3.   NNEstimator(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]],\nlabelPreprocessing: Preprocessing[F, Tensor[T]])  Takes model, criterion, featurePreprocessing and labelPreprocessing.   NNEstimator \n   will extract the data from feature and label columns and convert each feature/label to Tensor\n   with the featurePreprocessing and labelPreprocessing. This constructor provides more flexibility\n   in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNEstimator  supports: setSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])  to directly compose\nSample according to user-specified Preprocessing.  Scala Example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNEstimator\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNEstimator(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0)),\n  (Array(2.0, 1.0), Array(1.0, 2.0)),\n  (Array(1.0, 2.0), Array(2.0, 1.0))))\nval df = sqlContext.createDataFrame(data).toDF( features ,  label )\nval nnModel = estimator.fit(df)\nnnModel.transform(df).show(false)  Python Example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom zoo.pipeline.nnframes.nn_classifier import *\nfrom zoo.feature.common import *\n\ndata = self.sc.parallelize([\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0)),\n    ((2.0, 1.0), (1.0, 2.0)),\n    ((1.0, 2.0), (2.0, 1.0))])\n\nschema = StructType([\n    StructField( features , ArrayType(DoubleType(), False), False),\n    StructField( label , ArrayType(DoubleType(), False), False)])\ndf = self.sqlContext.createDataFrame(data, schema)\nmodel = Sequential().add(Linear(2, 2))\ncriterion = MSECriterion()\nestimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2]))\\\n    .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(40)\nnnModel = estimator.fit(df)\nres = nnModel.transform(df)", 
            "title": "NNEstimator"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnmodel", 
            "text": "Scala:  val nnModel = NNModel(bigDLModel)  Python:  nn_model = NNModel(bigDLModel)  NNModel  extends Spark's ML Transformer . User can invoke fit  in  NNEstimator  to get a  NNModel , or directly compose a  NNModel  from BigDLModel.\nIt enables users to wrap a pre-trained BigDL Model into a NNModel,\nand use it as a transformer in your Spark ML pipeline to predict the results for  DataFrame\n(DataSet) .   NNModel  can be created with various parameters for different scenarios.  1.   NNModel(model)  Takes only model and use  SeqToTensor  as feature Preprocessing.  NNModel  will extract the\n   data from feature column (only Scalar, Array[_] or Vector data type are supported) and\n   convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.  2.   NNModel(model, featureSize: Array[Int])  Takes model and featureSize(Array of Int).  NNModel  will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.  3.   NNModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])  Takes model and featurePreprocessing.  NNModel  will extract the data from feature column\n   and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNModel  supports: setSamplePreprocessing(value: Preprocessing[Any, Sample[T]]) to directly compose\nSample according to user-specified Preprocessing.", 
            "title": "NNModel"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnclassifier", 
            "text": "Scala:  val classifer =  NNClassifer(model, criterion)  Python:  classifier = NNClassifer(model, criterion)  NNClassifier  is a specialized  NNEstimator  that simplifies the data format for\nclassification tasks where the label space is discrete. It only supports label column of\nDoubleType, and the fitted  NNClassifierModel  will have the prediction column of \nDoubleType.   model  BigDL module to be optimized in the fit() method  criterion  the criterion used to compute the loss and the gradient   NNClassifier  can be created with various parameters for different scenarios.  1.   NNClassifier(model, criterion)  Takes only model and criterion and use  SeqToTensor  as feature and label\n   Preprocessing.  NNClassifier  will extract the data from feature and label columns (\n   only Scalar, Array[_] or Vector data type are supported) and convert each feature/label to\n   1-dimension Tensor. The tensors will be combined into BigDL samples and send to model for\n   training.  2.   NNClassifier(model, criterion, featureSize: Array[Int])  Takes model, criterion, featureSize(Array of Int).  NNClassifier \n   will extract the data from feature and label columns and convert each feature to Tensor\n   according to the specified Tensor size.  ScalarToTensor  is used to convert the label column.  3.   NNClassifier(model, criterion, featurePreprocessing: Preprocessing[F, Tensor[T]])  Takes model, criterion and featurePreprocessing.   NNClassifier \n   will extract the data from feature and label columns and convert each feature to Tensor\n   with the featurePreprocessing. This constructor provides more flexibility\n   in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNClassifier  supports: setSamplePreprocessing(value: Preprocessing[(Any, Option[Any]), Sample[T]])  to directly compose\nSample with user-specified Preprocessing.  Scala example:  import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.zoo.pipeline.nnframes.NNClassifier\nimport com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat\n\nval model = Sequential().add(Linear(2, 2))\nval criterion = MSECriterion()\nval estimator = NNClassifier(model, criterion)\n  .setLearningRate(0.2)\n  .setMaxEpoch(40)\nval data = sc.parallelize(Seq(\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0),\n  (Array(0.0, 1.0), 1.0),\n  (Array(1.0, 0.0), 2.0)))\nval df = sqlContext.createDataFrame(data).toDF( features ,  label )\nval dlModel = estimator.fit(df)\ndlModel.transform(df).show(false)  Python Example:  from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.util.common import *\nfrom bigdl.dlframes.dl_classifier import *\nfrom pyspark.sql.types import *\n\n#Logistic Regression with BigDL layers and Analytics zoo NNClassifier\nmodel = Sequential().add(Linear(2, 2)).add(LogSoftMax())\ncriterion = ClassNLLCriterion()\nestimator = NNClassifier(model, criterion, [2]).setBatchSize(4).setMaxEpoch(10)\ndata = sc.parallelize([\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0]),\n    ((0.0, 1.0), [1.0]),\n    ((1.0, 0.0), [2.0])])\n\nschema = StructType([\n    StructField( features , ArrayType(DoubleType(), False), False),\n    StructField( label , ArrayType(DoubleType(), False), False)])\ndf = sqlContext.createDataFrame(data, schema)\ndlModel = estimator.fit(df)\ndlModel.transform(df).show(False)", 
            "title": "NNClassifier"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnclassifiermodel", 
            "text": "Scala:  val nnClassifierModel = NNClassifierModel(model, featureSize)  Python:  nn_classifier_model = NNClassifierModel(model)  NNClassifierModel is a specialized  NNModel  for classification tasks.\nBoth label and prediction column will have the datatype of Double.  NNClassifierModel  can be created with various parameters for different scenarios.  1.   NNClassifierModel(model)  Takes only model and use  SeqToTensor  as feature Preprocessing.  NNClassifierModel  will\n   extract the data from feature column (only Scalar, Array[_] or Vector data type are supported)\n   and convert each feature to 1-dimension Tensor. The tensors will be sent to model for inference.  2.   NNClassifierModel(model, featureSize: Array[Int])  Takes model and featureSize(Array of Int).  NNClassifierModel  will extract the data from feature\n   column (only Scalar, Array[_] or Vector data type are supported) and convert each feature\n   to Tensor according to the specified Tensor size.  3.   NNClassifierModel(model, featurePreprocessing: Preprocessing[F, Tensor[T]])  Takes model and featurePreprocessing.  NNClassifierModel  will extract the data from feature\n   column and convert each feature to Tensor with the featurePreprocessing. This constructor provides\n   more flexibility in supporting extra data types.  Meanwhile, for advanced use cases (e.g. model with multiple input tensor),  NNClassifierModel \nsupports:  setSamplePreprocessing(value: Preprocessing[Any, Sample[T]]) to directly compose\nSample according to user-specified Preprocessing.", 
            "title": "NNClassifierModel"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#hyperparameter-setting", 
            "text": "Prior to the commencement of the training process, you can modify the optimization algorithm, batch \nsize, the epoch number of your training, and learning rate to meet your goal or NNEstimator / NNClassifier  will use the default value.  Continue the codes above, NNEstimator and NNClassifier can be set in the same way.  Scala:  //for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())\n//for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(new Adam())  Python:  # for esitmator\nestimator.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())\n# for classifier\nclassifier.setBatchSize(4).setMaxEpoch(10).setLearningRate(0.01).setOptimMethod(Adam())", 
            "title": "Hyperparameter setting"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#prepare-the-data-and-start-the-training-process", 
            "text": "NNEstimator/NNCLassifer supports training with Spark's DataFrame/DataSet  Suppose  df  is the training data, simple call  fit  method and let Analytics Zoo train the model\nfor you.  Scala:  //get a NNClassifierModel\nval nnClassifierModel = classifier.fit(df)  Python:  # get a NNClassifierModel\nnnClassifierModel = classifier.fit(df)  User may also set validation DataFrame and validation frequency through  setValidation  method.\nTrain summay and validation summary can also be configured to log the training process for\nvisualization in Tensorboard.", 
            "title": "Prepare the data and start the training process"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#make-prediction-on-chosen-data", 
            "text": "Since  NNModel / NNClassifierModel  inherits from Spark's  Transformer  abstract class, simply call  transform  method on  NNModel / NNClassifierModel  to make prediction.  Scala:  nnModel.transform(df).show(false)  Python:  nnModel.transform(df).show(false)  For the complete examples of NNFrames, please refer to: Scala examples  Python examples", 
            "title": "Make prediction on chosen data"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/nnframes/#nnimagereader", 
            "text": "NNImageReader  is the primary DataFrame-based image loading interface, defining API to read images\ninto DataFrame.  Scala:      val imageDF = NNImageReader.readImages(imageDirectory, sc)  Python:      image_frame = NNImageReader.readImages(image_path, self.sc)  The output DataFrame contains a sinlge column named \"image\". The schema of \"image\" column can be\naccessed from  com.intel.analytics.zoo.pipeline.nnframes.DLImageSchema.byteSchema .\nEach record in \"image\" column represents one image record, in the format of\nRow(origin, height, width, num of channels, mode, data), where origin contains the URI for the image file,\nand  data  holds the original file bytes for the image file.  mode  represents the OpenCV-compatible\ntype: CV_8UC3, CV_8UC1 in most cases.    val byteSchema = StructType(\n    StructField( origin , StringType, true) ::\n      StructField( height , IntegerType, false) ::\n      StructField( width , IntegerType, false) ::\n      StructField( nChannels , IntegerType, false) ::\n      // OpenCV-compatible type: CV_8UC3, CV_32FC3 in most cases\n      StructField( mode , IntegerType, false) ::\n      // Bytes in OpenCV-compatible order: row-wise BGR in most cases\n      StructField( data , BinaryType, false) :: Nil)  After loading the image, user can compose the preprocess steps with the  Preprocessing  defined\nin  com.intel.analytics.zoo.feature.image .", 
            "title": "NNImageReader"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/", 
            "text": "mean\n\n\nMean of a \nVariable\n, alongside the specified axis.\n- \naxis\n axis to compute the mean. 0-based indexed.\n- \nkeepDims\n A boolean, whether to keep the dimensions or not.\n   If \nkeepDims\n is \nFalse\n, the rank of the \nVariable\n is reduced\n   by 1. If \nkeepDims\n is \nTrue\n,\n   the reduced dimensions are retained with length 1.\n\n\nScala example\n\n\nmean(x: Variable[T], axis: Int = 0, keepDims: Boolean = false)\n\n\n\n\nPython example\n\n\nmean(x, axis=0, keepDims=False):\n\n\n\n\nabs\n\n\nElement-wise absolute value.\n- \nx\n A \nVariable\n.\n\n\nScala example\n\n\nabs(x: Variable[T])\n\n\n\n\nPython example\n\n\nabs(x):\n\n\n\n\nsum\n\n\nSum of the values in a \nVariable\n, alongside the specified axis.\n- \naxis\n axis to compute the mean. 0-based indexed.\n- \nkeepDims\n A boolean, whether to keep the dimensions or not.\n   If \nkeepDims\n is \nFalse\n, the rank of the \nVariable\n is reduced\n   by 1. If \nkeepDims\n is \nTrue\n,\n   the reduced dimensions are retained with length 1.\n\n\nScala example\n\n\nsum(x: Variable[T], axis: Int = 0, keepDims: Boolean = false)\n\n\n\n\nPython example\n\n\nsum(x, axis=0, keepDims=False):\n\n\n\n\nclip\n\n\nElement-wise value clipping.\n- \nx\n A \nVariable\n.\n- \nmin\n Double\n- \nmax\n Double\n\n\nScala example\n\n\nclip(x: Variable[T], min: Double, max: Double)\n\n\n\n\nPython example\n\n\nclip(x, min, max)\n\n\n\n\nsquare\n\n\nElement-wise square.\n- \nx\n A \nVariable\n.\n\n\nScala example\n\n\nsquare(x: Variable[T])\n\n\n\n\nPython example\n\n\nsquare(x):\n\n\n\n\nsqrt\n\n\nElement-wise square root.\n- \nx\n A \nVariable\n.\n\n\nScala example\n\n\nsqrt(x: Variable[T])\n\n\n\n\nPython example\n\n\nsqrt(x):\n\n\n\n\nmaximum\n\n\nElement-wise maximum of two \nVariables\n.\n- \nx\n A \nVariable\n.\n- \ny\n A \nVariable\n or Double.\n\n\nScala example\n\n\nmaximum(x: Variable[T], y: Variable[T])\n\n\n\n\nPython example\n\n\nmaximum(x, y):\n\n\n\n\nlog\n\n\nElement-wise log.\n- \nx\n A \nVariable\n.\n\n\nScala example\n\n\nlog(x: Variable[T])\n\n\n\n\nPython example\n\n\nlog(x):\n\n\n\n\nexp\n\n\nElement-wise exponential.\n- \nx\n A \nVariable\n.\n\n\nScala example\n\n\nexp(x: Variable[T])\n\n\n\n\nPython example\n\n\nexp(x):\n\n\n\n\npow\n\n\nElement-wise exponentiation.\n- \nx\n A \nVariable\n.\n- \na\n Double.   \n\n\nScala example\n\n\npow(x: Variable[T])\n\n\n\n\nPython example\n\n\npow(x):\n\n\n\n\nsoftsign\n\n\nSoftsign of a \nVariable\n.\n\n\nScala example\n\n\nsoftsign(x: Variable[T])\n\n\n\n\nPython example\n\n\nsoftsign(x):\n\n\n\n\nsoftplus\n\n\nSoftplus of a \nVariable\n.\n\n\nScala example\n\n\nsoftplus(x: Variable[T])\n\n\n\n\nPython example\n\n\nsoftplus(x):\n\n\n\n\nstack\n\n\nStacks a list of rank \nR\n tensors into a rank \nR+1\n tensor.\n   You should start from 1 as dim 0 is for batch.\n   - inputs: List of variables (tensors)\n   - axis: xis along which to perform stacking.\n\n\nScala example\n\n\ndef stack[T: ClassTag](inputs: List[Variable[T]], axis: Int = 1)\n\n\n\n\nPython example\n\n\ndef stack(inputs, axis=1)\n\n\n\n\nexpand_dims\n\n\nAdds a 1-sized dimension at index \"axis\".\n\n\nScala example\n\n\ndef expandDims[T: ClassTag](x: Variable[T], axis: Int)\n\n\n\n\nPython example\n\n\nexpand_dims(x, axis)\n\n\n\n\ncontiguous\n\n\nTurn the output and grad to be contiguous for the input Variable\n\n\nScala example\n\n\ndef contiguous[T: ClassTag](input: Variable[T])\n\n\n\n\nPython example\n\n\ndef contiguous(x)\n\n\n\n\nmm\n\n\nModule to perform matrix multiplication on two mini-batch inputs, producing a mini-batch.\n- \nx\n A variable.\n- \ny\n A variable.\n- \naxes\n Axes along which to perform multiplication.\n\n\nScala example\n\n\ndef mm[T: ClassTag](x: Variable[T], y: Variable[T], axes: List[Int])\n\n\n\n\nPython example\n\n\ndef mm(x, y, axes)\n\n\n\n\nbatch_dot\n\n\nOperator that computes a dot product between samples in two tensors.\n- \nx\n Shape should only be [batch, xx]\n- \ny\n Shape should only be [batch, xx]\n- \naxes\n Integer or tuple of integers, axis or axes along which to take the dot product.\n- \nnormalize\n Whether to L2-normalize samples along the\n              dot product axis before taking the dot product.\n              If set to True, then the output of the dot product\n              is the cosine proximity between the two samples.\n\n\nScala example\n\n\ndef batchDot[T: ClassTag](x: Variable[T], y: Variable[T], axes: List[Int], normalize: Boolean = false)\n\n\n\n\nPython example\n\n\ndef batch_dot(x, y, axes=1, normalize=False)\n\n\n\n\nl2_normalize\n\n\nNormalizes a tensor wrt the L2 norm alongside the specified axis.\n- \nx\n A variable.\n- \naxis\n Axis along which to perform normalization.\n\n\nScala example\n\n\ndef l2Normalize[T: ClassTag](x: Variable[T], axis: Int)\n\n\n\n\nPython example\n\n\ndef l2_normalize(x, axis)", 
            "title": "Autograd-Math"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#mean", 
            "text": "Mean of a  Variable , alongside the specified axis.\n-  axis  axis to compute the mean. 0-based indexed.\n-  keepDims  A boolean, whether to keep the dimensions or not.\n   If  keepDims  is  False , the rank of the  Variable  is reduced\n   by 1. If  keepDims  is  True ,\n   the reduced dimensions are retained with length 1.  Scala example  mean(x: Variable[T], axis: Int = 0, keepDims: Boolean = false)  Python example  mean(x, axis=0, keepDims=False):", 
            "title": "mean"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#abs", 
            "text": "Element-wise absolute value.\n-  x  A  Variable .  Scala example  abs(x: Variable[T])  Python example  abs(x):", 
            "title": "abs"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#sum", 
            "text": "Sum of the values in a  Variable , alongside the specified axis.\n-  axis  axis to compute the mean. 0-based indexed.\n-  keepDims  A boolean, whether to keep the dimensions or not.\n   If  keepDims  is  False , the rank of the  Variable  is reduced\n   by 1. If  keepDims  is  True ,\n   the reduced dimensions are retained with length 1.  Scala example  sum(x: Variable[T], axis: Int = 0, keepDims: Boolean = false)  Python example  sum(x, axis=0, keepDims=False):", 
            "title": "sum"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#clip", 
            "text": "Element-wise value clipping.\n-  x  A  Variable .\n-  min  Double\n-  max  Double  Scala example  clip(x: Variable[T], min: Double, max: Double)  Python example  clip(x, min, max)", 
            "title": "clip"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#square", 
            "text": "Element-wise square.\n-  x  A  Variable .  Scala example  square(x: Variable[T])  Python example  square(x):", 
            "title": "square"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#sqrt", 
            "text": "Element-wise square root.\n-  x  A  Variable .  Scala example  sqrt(x: Variable[T])  Python example  sqrt(x):", 
            "title": "sqrt"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#maximum", 
            "text": "Element-wise maximum of two  Variables .\n-  x  A  Variable .\n-  y  A  Variable  or Double.  Scala example  maximum(x: Variable[T], y: Variable[T])  Python example  maximum(x, y):", 
            "title": "maximum"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#log", 
            "text": "Element-wise log.\n-  x  A  Variable .  Scala example  log(x: Variable[T])  Python example  log(x):", 
            "title": "log"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#exp", 
            "text": "Element-wise exponential.\n-  x  A  Variable .  Scala example  exp(x: Variable[T])  Python example  exp(x):", 
            "title": "exp"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#pow", 
            "text": "Element-wise exponentiation.\n-  x  A  Variable .\n-  a  Double.     Scala example  pow(x: Variable[T])  Python example  pow(x):", 
            "title": "pow"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#softsign", 
            "text": "Softsign of a  Variable .  Scala example  softsign(x: Variable[T])  Python example  softsign(x):", 
            "title": "softsign"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#softplus", 
            "text": "Softplus of a  Variable .  Scala example  softplus(x: Variable[T])  Python example  softplus(x):", 
            "title": "softplus"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#stack", 
            "text": "Stacks a list of rank  R  tensors into a rank  R+1  tensor.\n   You should start from 1 as dim 0 is for batch.\n   - inputs: List of variables (tensors)\n   - axis: xis along which to perform stacking.  Scala example  def stack[T: ClassTag](inputs: List[Variable[T]], axis: Int = 1)  Python example  def stack(inputs, axis=1)", 
            "title": "stack"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#expand_dims", 
            "text": "Adds a 1-sized dimension at index \"axis\".  Scala example  def expandDims[T: ClassTag](x: Variable[T], axis: Int)  Python example  expand_dims(x, axis)", 
            "title": "expand_dims"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#contiguous", 
            "text": "Turn the output and grad to be contiguous for the input Variable  Scala example  def contiguous[T: ClassTag](input: Variable[T])  Python example  def contiguous(x)", 
            "title": "contiguous"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#mm", 
            "text": "Module to perform matrix multiplication on two mini-batch inputs, producing a mini-batch.\n-  x  A variable.\n-  y  A variable.\n-  axes  Axes along which to perform multiplication.  Scala example  def mm[T: ClassTag](x: Variable[T], y: Variable[T], axes: List[Int])  Python example  def mm(x, y, axes)", 
            "title": "mm"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#batch_dot", 
            "text": "Operator that computes a dot product between samples in two tensors.\n-  x  Shape should only be [batch, xx]\n-  y  Shape should only be [batch, xx]\n-  axes  Integer or tuple of integers, axis or axes along which to take the dot product.\n-  normalize  Whether to L2-normalize samples along the\n              dot product axis before taking the dot product.\n              If set to True, then the output of the dot product\n              is the cosine proximity between the two samples.  Scala example  def batchDot[T: ClassTag](x: Variable[T], y: Variable[T], axes: List[Int], normalize: Boolean = false)  Python example  def batch_dot(x, y, axes=1, normalize=False)", 
            "title": "batch_dot"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/math/#l2_normalize", 
            "text": "Normalizes a tensor wrt the L2 norm alongside the specified axis.\n-  x  A variable.\n-  axis  Axis along which to perform normalization.  Scala example  def l2Normalize[T: ClassTag](x: Variable[T], axis: Int)  Python example  def l2_normalize(x, axis)", 
            "title": "l2_normalize"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/variable/", 
            "text": "Basic operators: \n+ - * /\n\n\nThose are supported as element-wise operation.\n\n\nScala example\n\n\nx + 1.0\nx + y\n\n\n\n\nPython example\n\n\nx + 1.0\nx + y\n\n\n\n\nsqueeze\n\n\nDelete the singleton dimension(s).\n   The batch dimension needs to be unchanged.\n   For example, if input has size (2, 1, 3, 4, 1):\n   - squeeze(dim = 1) will give output size (2, 3, 4, 1)\n   - squeeze(dims = null) will give output size (2, 3, 4)\n\n\nScala example\n\n\nx.squeeze(1)\n\n\n\n\nPython example\n\n\nx.squeeze(1)\n\n\n\n\nslice\n\n\nSlice the input with the number of dimensions not being reduced.\nThe batch dimension needs to be unchanged.\n- dim The dimension to narrow. 0-based index. Cannot narrow the batch dimension.\n     -1 means the last dimension of the input.\n- startIndex Non-negative integer. The start index on the given dimension. 0-based index.\n- length The length to be sliced. Default is 1.\n\n\nFor example, \nif input is:\n1 2 3\n4 5 6\n- slice(1, 1, 2) will give output\n2 3\n5 6\n- slice(1, 2, -1) will give output\n3\n6\n\n\nScala example\n\n\nx.slice(1, 1, 2)\n\n\n\n\nPython example\n\n\nx.slice(1, 1, 2)\n\n\n\n\nindex_select\n\n\nSelect an index of the input in the given dim and return the subset part.\n The batch dimension needs to be unchanged.\n The selected dim would be remove after this operation.\n - dim: The dimension to select. 0-based index. Cannot select the batch dimension.\n                 -1 means the last dimension of the input.\n - index: The index of the dimension to be selected. 0-based index.\n                -1 means the last dimension of the input.\n\n\nFor example, if input is:\n           1 2 3\n           4 5 6\n - Select(1, 1) will give output [2 5]\n - Select(1, -1) will give output [3 6]\n\n\nScala example\n\n\nx.select(1, 1)\n\n\n\n\nPython example\n\n\nx.select(1, 1)", 
            "title": "Autograd-Variable"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/variable/#basic-operators-", 
            "text": "Those are supported as element-wise operation.  Scala example  x + 1.0\nx + y  Python example  x + 1.0\nx + y", 
            "title": "Basic operators: + - * /"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/variable/#squeeze", 
            "text": "Delete the singleton dimension(s).\n   The batch dimension needs to be unchanged.\n   For example, if input has size (2, 1, 3, 4, 1):\n   - squeeze(dim = 1) will give output size (2, 3, 4, 1)\n   - squeeze(dims = null) will give output size (2, 3, 4)  Scala example  x.squeeze(1)  Python example  x.squeeze(1)", 
            "title": "squeeze"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/variable/#slice", 
            "text": "Slice the input with the number of dimensions not being reduced.\nThe batch dimension needs to be unchanged.\n- dim The dimension to narrow. 0-based index. Cannot narrow the batch dimension.\n     -1 means the last dimension of the input.\n- startIndex Non-negative integer. The start index on the given dimension. 0-based index.\n- length The length to be sliced. Default is 1.  For example, \nif input is:\n1 2 3\n4 5 6\n- slice(1, 1, 2) will give output\n2 3\n5 6\n- slice(1, 2, -1) will give output\n3\n6  Scala example  x.slice(1, 1, 2)  Python example  x.slice(1, 1, 2)", 
            "title": "slice"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/variable/#index_select", 
            "text": "Select an index of the input in the given dim and return the subset part.\n The batch dimension needs to be unchanged.\n The selected dim would be remove after this operation.\n - dim: The dimension to select. 0-based index. Cannot select the batch dimension.\n                 -1 means the last dimension of the input.\n - index: The index of the dimension to be selected. 0-based index.\n                -1 means the last dimension of the input.  For example, if input is:\n           1 2 3\n           4 5 6\n - Select(1, 1) will give output [2 5]\n - Select(1, -1) will give output [3 6]  Scala example  x.select(1, 1)  Python example  x.select(1, 1)", 
            "title": "index_select"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/", 
            "text": "Net\n\n\nLoad Analytics Zoo Model\n\n\nUse \nNet.load\n(in Scala) or \nNet.load\n (in Python) to load an existing model defined using the Analytics Zoo Keras-style API.  \nNet\n (Scala) or \nNet\n(Python) is a utility class provided in Analytics Zoo. We just need to specify the model path and optionally weight path if exists where we previously saved the model to load it to memory for resume training or prediction purpose.\n\n\nScala example\n\n\nval model = Net.load(\n/tmp/model.def\n, \n/tmp/model.weights\n) //load from local fs\nval model = Net.load(\nhdfs://...\n) //load from hdfs\nval model = Net.load(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nmodel = Net.load(\n/tmp/model.def\n, \n/tmp/model.weights\n) //load from local fs\nmodel = Net.load(\nhdfs://...\n) //load from hdfs\nmodel = Net.load(\ns3://...\n) //load from s3\n\n\n\n\nLoad BigDL Model\n\n\nScala example\n\n\nval model = Net.loadBigDL(\n/tmp/model.def\n, \n/tmp/model.weights\n) //load from local fs\nval model = Net.loadBigDL(\nhdfs://...\n) //load from hdfs\nval model = Net.loadBigDL(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nmodel = Net.loadBigDL(\n/tmp/model.def\n, \n/tmp/model.weights\n) //load from local fs\nmodel = Net.loadBigDL(\nhdfs://...\n) //load from hdfs\nmodel = Net.loadBigDL(\ns3://...\n) //load from s3\n\n\n\n\nLoad Torch Model\n\n\nScala example\n\n\nval model = Net.loadTorch(\n/tmp/torch_model\n) //load from local fs\nval model = Net.loadTorch(\nhdfs://...\n) //load from hdfs\nval model = Net.loadTorch(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nmodel = Net.loadTorch(\n/tmp/torch_model\n) //load from local fs\nmodel = Net.loadTorch(\nhdfs://...\n) //load from hdfs\nmodel = Net.loadTorch(\ns3://...\n) //load from s3\n\n\n\n\nLoad Caffe Model\n\n\nScala example\n\n\nval model = Net.loadCaffe(\n/tmp/def/path\n, \n/tmp/model/path\n) //load from local fs\nval model = Net.loadCaffe(\nhdfs://def/path\n, \nhdfs://model/path\n) //load from hdfs\nval model = Net.loadCaffe(\ns3://def/path\n, \ns3://model/path\n) //load from s3\n\n\n\n\nPython example\n\n\nmodel = Net.loadCaffe(\n/tmp/def/path\n, \n/tmp/model/path\n) //load from local fs\nmodel = Net.loadCaffe(\nhdfs://def/path\n, \nhdfs://model/path\n) //load from hdfs\nmodel = Net.loadCaffe(\ns3://def/path\n, \ns3://model/path\n) //load from s3\n\n\n\n\nLoad Tensorflow model\n\n\nWe also provides utilities to load tensorflow model.\n\n\nIf we already have a frozen graph protobuf file, we can use the \nloadTF\n api directly to\nload the tensorflow model. \n\n\nOtherwise, we should first use the \nexport_tf_checkpoint.py\n script provided by BigDL's distribution\npackage, or the \ndump_model\n function defined in \nhere\n to\ngenerate the model definition file (\nmodel.pb\n) and variable binary file (\nmodel.bin\n). \n\n\nUse Script\n\n\nGRAPH_META_FILE=/tmp/tensorflow/model.ckpt.meta\nCKPT_FILE_PREFIX=/tmp/tensorflow/model.ckpt\nSAVE_PATH=/tmp/model/\npython export_tf_checkpoint.py $GRAPH_META_FILE $CKPT_FILE_PREFIX $SAVE_PATH\n\n\n\n\nUse python function\n\n\nimport tensorflow as tf\n\n# This is your model definition.\nxs = tf.placeholder(tf.float32, [None, 1])\n\nW1 = tf.Variable(tf.zeros([1,10])+0.2)\nb1 = tf.Variable(tf.zeros([10])+0.1)\nWx_plus_b1 = tf.nn.bias_add(tf.matmul(xs,W1), b1)\noutput = tf.nn.tanh(Wx_plus_b1, name=\noutput\n)\n\n# Adding the following lines right after your model definition \nfrom bigdl.util.tf_utils import dump_model\ndump_model_path = \n/tmp/model\n\n# This line of code will create a Session and initialized all the Variable and\n# save the model definition and variable to dump_model_path as BigDL readable format.\ndump_model(path=dump_model_path)\n\n\n\n\nThen we can use the \nloadTF\n api to load the tensorflow model into BigDL.\n\n\nScala example\n\n\nval modelPath = \n/tmp/model/model.pb\n\nval binPath = \n/tmp/model/model.bin\n\nval inputs = Seq(\nPlaceholder\n)\nval outputs = Seq(\noutput\n)\n\n// For tensorflow frozen graph or graph without Variables\nval model = Net.loadTF(modelPath, inputs, outputs, ByteOrder.LITTLE_ENDIAN)\n\n// For tensorflow graph with Variables\nval model = Net.loadTF(modelPath, inputs, outputs, ByteOrder.LITTLE_ENDIAN, Some(binPath))\n\n\n\n\nPython example\n\n\nmodel_def = \n/tmp/model/model.pb\n\nmodel_variable = \n/tmp/model/model.bin\n\ninputs = [\nPlaceholder\n]\noutputs = [\noutput\n]\n# For tensorflow frozen graph or graph without Variables\nmodel = Net.load_tensorflow(model_def, inputs, outputs, byte_order = \nlittle_endian\n, bigdl_type=\nfloat\n)\n\n# For tensorflow graph with Variables\nmodel = Net.load_tensorflow(model_def, inputs, outputs, byte_order = \nlittle_endian\n, bigdl_type=\nfloat\n, bin_file=model_variable)\n\n\n\n\nTFNet\n\n\nTFNet is a analytics-zoo layer that wraps a tensorflow frozen graph and can easily run in parallel.\n\n\nThe difference between Net.loadTF() is that TFNet will call tensorflow's java api to do the computation.\n\n\nTFNet cannot be trained, so it can only be used for inference or as a feature extractor for fine tuning a model.\nWhen used as feature extractor, there should not be any trainable layers before TFNet, as all the gradient\nfrom TFNet is set to zero.\n\n\nExport tensorflow model to frozen inference graph\n\n\nAnalytics-zoo provides a useful utility function, \nexport_tf\n, to export a tensorflow model\nto frozen inference graph.\n\n\nFor example:\n\n\nPython:\n\n\nimport tensorflow as tf\nfrom nets import inception\nslim = tf.contrib.slim\n\nimages = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3))\n\nwith slim.arg_scope(inception.inception_v1_arg_scope()):\n    logits, end_points = inception.inception_v1(images, num_classes=1001, is_training=False)\n\nsess = tf.Session()\nsaver = tf.train.Saver()\nsaver.restore(sess, \n/tmp/models/inception_v1.ckpt\n)\n\nfrom zoo.util.tf import export_tf\nexport_tf(sess, \n/tmp/models/tfnet\n, inputs=[images], outputs=[logits])\n\n\n\n\nIn the above code, the \nexport_tf\n utility function will frozen the tensorflow graph, strip unused operation according to the inputs and outputs and save it to the specified directory along with the input/output tensor names. \n\n\nCreating a TFNet\n\n\nAfter we have export the tensorflow model, we can easily create a TFNet.\n\n\nScala:\n\n\nval m = TFNet(\n/tmp/models/tfnet\n)\n\n\n\n\nPython:\n\n\nm = TFNet.from_export_folder(\n/tmp/models/tfnet\n)\n\n\n\n\nPlease refer to \nTFNet Object Detection Example (Scala)\n\nor \nTFNet Object Detection Example (Python)\n and\nthe \nImage Classification Using TFNet Notebook\n for more information.\n\n\nTFDataset\n\n\nTFDatset represents a distributed collection of elements to be feed into Tensorflow graph.\nTFDatasets can be created using a RDD and each of its records is a list of numpy.ndarray representing\nthe tensors to be feed into tensorflow graph on each iteration. TFDatasets must be used with the\nTFOptimizer or TFPredictor.\n\n\nPython\n\n\n   dataset = TFDataset.from_rdd(train_rdd,\n                                 names=[\nfeatures\n, \nlabels\n],\n                                 shapes=[[28, 28, 1], [1]],\n                                 types=[tf.float32, tf.int32],\n                                 batch_size=BATCH_SIZE)\n\n\n\n\nTFOptimizer\n\n\nTFOptimizer is the class that does all the hard work in distributed training, such as model\ndistribution and parameter synchronization. It takes the \nloss\n (a scalar tensor) as input and runs\nstochastic gradient descent using the given \noptimMethod\n on all the \nVariables\n that contributing\nto this loss.\n\n\nPython\n\n\noptimizer = TFOptimizer(loss, Adam(1e-3))\noptimizer.set_train_summary(TrainSummary(\n/tmp/az_lenet\n, \nlenet\n))\noptimizer.optimize(end_trigger=MaxEpoch(5))\n\n\n\n\nTFPredictor\n\n\nTFPredictor takes a list of tensorflow tensors as the model outputs and feed all the elements in\n TFDatasets to produce those outputs and returns a Spark RDD with each of its elements representing the\n model prediction for the corresponding input elements.\n\n\nPython\n\n\npredictor = TFPredictor(sess, [logits])\npredictions_rdd = predictor.predict()", 
            "title": "Net"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#net", 
            "text": "", 
            "title": "Net"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#load-analytics-zoo-model", 
            "text": "Use  Net.load (in Scala) or  Net.load  (in Python) to load an existing model defined using the Analytics Zoo Keras-style API.   Net  (Scala) or  Net (Python) is a utility class provided in Analytics Zoo. We just need to specify the model path and optionally weight path if exists where we previously saved the model to load it to memory for resume training or prediction purpose.  Scala example  val model = Net.load( /tmp/model.def ,  /tmp/model.weights ) //load from local fs\nval model = Net.load( hdfs://... ) //load from hdfs\nval model = Net.load( s3://... ) //load from s3  Python example  model = Net.load( /tmp/model.def ,  /tmp/model.weights ) //load from local fs\nmodel = Net.load( hdfs://... ) //load from hdfs\nmodel = Net.load( s3://... ) //load from s3", 
            "title": "Load Analytics Zoo Model"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#load-bigdl-model", 
            "text": "Scala example  val model = Net.loadBigDL( /tmp/model.def ,  /tmp/model.weights ) //load from local fs\nval model = Net.loadBigDL( hdfs://... ) //load from hdfs\nval model = Net.loadBigDL( s3://... ) //load from s3  Python example  model = Net.loadBigDL( /tmp/model.def ,  /tmp/model.weights ) //load from local fs\nmodel = Net.loadBigDL( hdfs://... ) //load from hdfs\nmodel = Net.loadBigDL( s3://... ) //load from s3", 
            "title": "Load BigDL Model"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#load-torch-model", 
            "text": "Scala example  val model = Net.loadTorch( /tmp/torch_model ) //load from local fs\nval model = Net.loadTorch( hdfs://... ) //load from hdfs\nval model = Net.loadTorch( s3://... ) //load from s3  Python example  model = Net.loadTorch( /tmp/torch_model ) //load from local fs\nmodel = Net.loadTorch( hdfs://... ) //load from hdfs\nmodel = Net.loadTorch( s3://... ) //load from s3", 
            "title": "Load Torch Model"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#load-caffe-model", 
            "text": "Scala example  val model = Net.loadCaffe( /tmp/def/path ,  /tmp/model/path ) //load from local fs\nval model = Net.loadCaffe( hdfs://def/path ,  hdfs://model/path ) //load from hdfs\nval model = Net.loadCaffe( s3://def/path ,  s3://model/path ) //load from s3  Python example  model = Net.loadCaffe( /tmp/def/path ,  /tmp/model/path ) //load from local fs\nmodel = Net.loadCaffe( hdfs://def/path ,  hdfs://model/path ) //load from hdfs\nmodel = Net.loadCaffe( s3://def/path ,  s3://model/path ) //load from s3", 
            "title": "Load Caffe Model"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#load-tensorflow-model", 
            "text": "We also provides utilities to load tensorflow model.  If we already have a frozen graph protobuf file, we can use the  loadTF  api directly to\nload the tensorflow model.   Otherwise, we should first use the  export_tf_checkpoint.py  script provided by BigDL's distribution\npackage, or the  dump_model  function defined in  here  to\ngenerate the model definition file ( model.pb ) and variable binary file ( model.bin ).   Use Script  GRAPH_META_FILE=/tmp/tensorflow/model.ckpt.meta\nCKPT_FILE_PREFIX=/tmp/tensorflow/model.ckpt\nSAVE_PATH=/tmp/model/\npython export_tf_checkpoint.py $GRAPH_META_FILE $CKPT_FILE_PREFIX $SAVE_PATH  Use python function  import tensorflow as tf\n\n# This is your model definition.\nxs = tf.placeholder(tf.float32, [None, 1])\n\nW1 = tf.Variable(tf.zeros([1,10])+0.2)\nb1 = tf.Variable(tf.zeros([10])+0.1)\nWx_plus_b1 = tf.nn.bias_add(tf.matmul(xs,W1), b1)\noutput = tf.nn.tanh(Wx_plus_b1, name= output )\n\n# Adding the following lines right after your model definition \nfrom bigdl.util.tf_utils import dump_model\ndump_model_path =  /tmp/model \n# This line of code will create a Session and initialized all the Variable and\n# save the model definition and variable to dump_model_path as BigDL readable format.\ndump_model(path=dump_model_path)  Then we can use the  loadTF  api to load the tensorflow model into BigDL.  Scala example  val modelPath =  /tmp/model/model.pb \nval binPath =  /tmp/model/model.bin \nval inputs = Seq( Placeholder )\nval outputs = Seq( output )\n\n// For tensorflow frozen graph or graph without Variables\nval model = Net.loadTF(modelPath, inputs, outputs, ByteOrder.LITTLE_ENDIAN)\n\n// For tensorflow graph with Variables\nval model = Net.loadTF(modelPath, inputs, outputs, ByteOrder.LITTLE_ENDIAN, Some(binPath))  Python example  model_def =  /tmp/model/model.pb \nmodel_variable =  /tmp/model/model.bin \ninputs = [ Placeholder ]\noutputs = [ output ]\n# For tensorflow frozen graph or graph without Variables\nmodel = Net.load_tensorflow(model_def, inputs, outputs, byte_order =  little_endian , bigdl_type= float )\n\n# For tensorflow graph with Variables\nmodel = Net.load_tensorflow(model_def, inputs, outputs, byte_order =  little_endian , bigdl_type= float , bin_file=model_variable)", 
            "title": "Load Tensorflow model"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#tfnet", 
            "text": "TFNet is a analytics-zoo layer that wraps a tensorflow frozen graph and can easily run in parallel.  The difference between Net.loadTF() is that TFNet will call tensorflow's java api to do the computation.  TFNet cannot be trained, so it can only be used for inference or as a feature extractor for fine tuning a model.\nWhen used as feature extractor, there should not be any trainable layers before TFNet, as all the gradient\nfrom TFNet is set to zero.", 
            "title": "TFNet"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#export-tensorflow-model-to-frozen-inference-graph", 
            "text": "Analytics-zoo provides a useful utility function,  export_tf , to export a tensorflow model\nto frozen inference graph.  For example:  Python:  import tensorflow as tf\nfrom nets import inception\nslim = tf.contrib.slim\n\nimages = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3))\n\nwith slim.arg_scope(inception.inception_v1_arg_scope()):\n    logits, end_points = inception.inception_v1(images, num_classes=1001, is_training=False)\n\nsess = tf.Session()\nsaver = tf.train.Saver()\nsaver.restore(sess,  /tmp/models/inception_v1.ckpt )\n\nfrom zoo.util.tf import export_tf\nexport_tf(sess,  /tmp/models/tfnet , inputs=[images], outputs=[logits])  In the above code, the  export_tf  utility function will frozen the tensorflow graph, strip unused operation according to the inputs and outputs and save it to the specified directory along with the input/output tensor names.", 
            "title": "Export tensorflow model to frozen inference graph"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#creating-a-tfnet", 
            "text": "After we have export the tensorflow model, we can easily create a TFNet.  Scala:  val m = TFNet( /tmp/models/tfnet )  Python:  m = TFNet.from_export_folder( /tmp/models/tfnet )  Please refer to  TFNet Object Detection Example (Scala) \nor  TFNet Object Detection Example (Python)  and\nthe  Image Classification Using TFNet Notebook  for more information.", 
            "title": "Creating a TFNet"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#tfdataset", 
            "text": "TFDatset represents a distributed collection of elements to be feed into Tensorflow graph.\nTFDatasets can be created using a RDD and each of its records is a list of numpy.ndarray representing\nthe tensors to be feed into tensorflow graph on each iteration. TFDatasets must be used with the\nTFOptimizer or TFPredictor.  Python     dataset = TFDataset.from_rdd(train_rdd,\n                                 names=[ features ,  labels ],\n                                 shapes=[[28, 28, 1], [1]],\n                                 types=[tf.float32, tf.int32],\n                                 batch_size=BATCH_SIZE)", 
            "title": "TFDataset"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#tfoptimizer", 
            "text": "TFOptimizer is the class that does all the hard work in distributed training, such as model\ndistribution and parameter synchronization. It takes the  loss  (a scalar tensor) as input and runs\nstochastic gradient descent using the given  optimMethod  on all the  Variables  that contributing\nto this loss.  Python  optimizer = TFOptimizer(loss, Adam(1e-3))\noptimizer.set_train_summary(TrainSummary( /tmp/az_lenet ,  lenet ))\noptimizer.optimize(end_trigger=MaxEpoch(5))", 
            "title": "TFOptimizer"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/net/#tfpredictor", 
            "text": "TFPredictor takes a list of tensorflow tensors as the model outputs and feed all the elements in\n TFDatasets to produce those outputs and returns a Spark RDD with each of its elements representing the\n model prediction for the corresponding input elements.  Python  predictor = TFPredictor(sess, [logits])\npredictions_rdd = predictor.predict()", 
            "title": "TFPredictor"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/inference/", 
            "text": "Inference Model\n\n\nOverview\n\n\nInference is a package in Analytics Zoo aiming to provide high level APIs to speed-up development. It \nallows user to conveniently use pre-trained models from Analytics Zoo, Tensorflow and Caffe.\nInference provides multiple Java/Scala interfaces.\n\n\nHighlights\n\n\n\n\n\n\nEasy-to-use java/scala API for loading and prediction with deep learning models.\n\n\n\n\n\n\nSupport transformation of various input data type, thus supporting future prediction tasks\n\n\n\n\n\n\nPrimary APIs for Java\n\n\nload\n\n\nAbstractInferenceModel provides \nload\n API for loading a pre-trained model,\nthus we can conveniently load various kinds of pre-trained models in java applications. The load result of\n\nAbstractInferenceModel\n is a \nFloatInferenceModel\n. \nWe just need to specify the model path and optionally weight path if exists where we previously saved the model.\n\n\npredict\n\n\nAbstractInferenceModel provides \npredict\n API for prediction with loaded model.\nThe predict result of\nAbstractInferenceModel\n is a \nList\nList\nJTensor\n by default.\n\n\nJava example\n\n\nIt's very easy to apply abstract inference model for inference with below code piece.\nYou will need to write a subclass that extends AbstractinferenceModel.\n\n\nimport com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;\nimport com.intel.analytics.zoo.pipeline.inference.JTensor;\n\npublic class TextClassificationModel extends AbstractInferenceModel {\n    public TextClassificationModel() {\n        super();\n    }\n }\nTextClassificationModel model = new TextClassificationModel();\nmodel.load(modelPath, weightPath);\nList\nList\nJTensor\n result = model.predict(inputList);\n\n\n\n\nPrimary APIs for Scala\n\n\nInferenceSupportive\n\n\nInferenceSupportive\n is a trait containing several methods for type transformation, which transfer a model input \nto a valid data type, thus supporting future inference model prediction tasks.\n\n\nFor example, method \ntransferTensorToJTensor\n convert a model input of data type \nTensor\n \nto \nJTensor\n\n, which will be the input for a FloatInferenceModel.\n\n\nFloatInferenceModel\n\n\nFloatInferenceModel\n is an extending class of \nInferenceSupportive\n and additionally provides \npredict\n API for prediction tasks.\n\n\nInferenceModelFactory\n\n\nInferenceModelFactory\n is an object with APIs for loading pre-trained Analytics Zoo models, Caffe models and Tensorflow models.\nWe just need to specify the model path and optionally weight path if exists where we previously saved the model.\nThe load result of is a \nFloatInferenceModel\n.\n\n\nModelLoader\n\n\nModelLoader\n is an extending object of  \nInferenceSupportive\n and focus on the implementation of loading pre-trained models", 
            "title": "Inference"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/inference/#inference-model", 
            "text": "", 
            "title": "Inference Model"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/inference/#overview", 
            "text": "Inference is a package in Analytics Zoo aiming to provide high level APIs to speed-up development. It \nallows user to conveniently use pre-trained models from Analytics Zoo, Tensorflow and Caffe.\nInference provides multiple Java/Scala interfaces.", 
            "title": "Overview"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/inference/#highlights", 
            "text": "Easy-to-use java/scala API for loading and prediction with deep learning models.    Support transformation of various input data type, thus supporting future prediction tasks", 
            "title": "Highlights"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/inference/#primary-apis-for-java", 
            "text": "load  AbstractInferenceModel provides  load  API for loading a pre-trained model,\nthus we can conveniently load various kinds of pre-trained models in java applications. The load result of AbstractInferenceModel  is a  FloatInferenceModel . \nWe just need to specify the model path and optionally weight path if exists where we previously saved the model.  predict  AbstractInferenceModel provides  predict  API for prediction with loaded model.\nThe predict result of AbstractInferenceModel  is a  List List JTensor  by default.  Java example  It's very easy to apply abstract inference model for inference with below code piece.\nYou will need to write a subclass that extends AbstractinferenceModel.  import com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;\nimport com.intel.analytics.zoo.pipeline.inference.JTensor;\n\npublic class TextClassificationModel extends AbstractInferenceModel {\n    public TextClassificationModel() {\n        super();\n    }\n }\nTextClassificationModel model = new TextClassificationModel();\nmodel.load(modelPath, weightPath);\nList List JTensor  result = model.predict(inputList);", 
            "title": "Primary APIs for Java"
        }, 
        {
            "location": "/APIGuide/PipelineAPI/inference/#primary-apis-for-scala", 
            "text": "InferenceSupportive  InferenceSupportive  is a trait containing several methods for type transformation, which transfer a model input \nto a valid data type, thus supporting future inference model prediction tasks.  For example, method  transferTensorToJTensor  convert a model input of data type  Tensor  \nto  JTensor \n, which will be the input for a FloatInferenceModel.  FloatInferenceModel  FloatInferenceModel  is an extending class of  InferenceSupportive  and additionally provides  predict  API for prediction tasks.  InferenceModelFactory  InferenceModelFactory  is an object with APIs for loading pre-trained Analytics Zoo models, Caffe models and Tensorflow models.\nWe just need to specify the model path and optionally weight path if exists where we previously saved the model.\nThe load result of is a  FloatInferenceModel .  ModelLoader  ModelLoader  is an extending object of   InferenceSupportive  and focus on the implementation of loading pre-trained models", 
            "title": "Primary APIs for Scala"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/", 
            "text": "Analytics Zoo provides a series of Image APIs for end-to-end image processing pipeline, including image loading, pre-processing, inference/training and some utilities on different formats.\n\n\nLoad Image\n\n\nAnalytics Zoo provides APIs to read image to different formats:\n\n\nLoad to Data Frame\n\n\nScala:\n\n\npackage com.intel.analytics.zoo.pipeline.nnframes\n\nobject NNImageReader {\n  def readImages(path: String, sc: SparkContext, minPartitions: Int = 1, resizeH: Int = -1, resizeW: Int = -1): DataFrame\n}\n\n\n\n\nRead the directory of images from the local or remote source, return DataFrame with a single column \"image\" of images.\n\n\n\n\npath: Directory to the input data files, the path can be comma separated paths as the list of inputs. Wildcards path are supported similarly to sc.binaryFiles(path).\n\n\nsc: SparkContext to be used.\n\n\nminPartitions: Number of the DataFrame partitions, if omitted uses defaultParallelism instead\n\n\nresizeH: height after resize, by default is -1 which will not resize the image\n\n\nresizeW: width after resize, by default is -1 which will not resize the image\n\n\n\n\nPython:\n\n\nclass zoo.pipeline.nnframes.NNImageReader\n    static readImages(path, sc=None, minPartitions=1, resizeH=-1, resizeW=-1, bigdl_type=\nfloat\n)\n\n\n\n\nImageSet\n\n\nImageSet\n is a collection of \nImageFeature\n. It can be a \nDistributedImageSet\n for distributed image RDD or\n \nLocalImageSet\n for local image array.\nYou can read image data as \nImageSet\n from local/distributed image path, or you can directly construct a ImageSet from RDD[ImageFeature] or Array[ImageFeature].\n\n\nScala APIs:\n\n\nobject com.intel.analytics.zoo.feature.image.ImageSet\n\n\n\n\ndef array(data: Array[ImageFeature]): LocalImageSet\n\n\n\n\nCreate LocalImageSet from array of ImeageFeature\n\n\n\n\ndata: array of ImageFeature\n\n\n\n\ndef rdd(data: RDD[ImageFeature]): DistributedImageSet\n\n\n\n\nCreate DistributedImageSet from rdd of ImageFeature\n\n\n\n\ndata: array of ImageFeature\n\n\n\n\n  def read(path: String, sc: SparkContext = null, minPartitions: Int = 1, resizeH: Int = -1, resizeW: Int = -1): ImageSet\n\n\n\n\nRead images as Image Set.\nIf sc is defined, read image as DistributedImageSet from local file system or HDFS.\nIf sc is null, Read image as LocalImageSet from local file system\n\n\n\n\npath: path to read images. If sc is defined, path can be local or HDFS. Wildcard character are supported. If sc is null, path is local directory/image file/image file with wildcard character\n\n\nsc: SparkContext\n\n\nminPartitions: A suggestion value of the minimal splitting number for input data.\n\n\nresizeH: height after resize, by default is -1 which will not resize the image\n\n\nresizeW: width after resize, by default is -1 which will not resize the image\n\n\n\n\nExample:\n\n\n// create LocalImageSet from an image folder\nval localImageSet = ImageSet.read(\n/tmp/image/\n)\n\n// create DistributedImageSet from an image folder\nval distributedImageSet2 = ImageSet.read(\n/tmp/image/\n, sc, 2)\n\n\n\n\nPython APIs:\n\n\nclass zoo.feature.image.ImageSet\n\n\n\n\nread(path, sc=None, min_partitions=1, resize_height=-1, resize_width=-1, bigdl_type=\nfloat\n)\n\n\n\n\nRead images as Image Set.\nIf sc is defined, read image as DistributedImageSet from local file system or HDFS.\nIf sc is null, Read image as LocalImageSet from local file system\n\n\n\n\npath: path to read images. If sc is defined, path can be local or HDFS. Wildcard character are supported. If sc is null, path is local directory/image file/image file with wildcard character\n\n\nsc: SparkContext\n\n\nmin_partitions: A suggestion value of the minimal splitting number for input data.\n\n\nresize_height height after resize, by default is -1 which will not resize the image\n\n\nresize_width width after resize, by default is -1 which will not resize the image\n\n\n\n\nPython example:\n\n\n# create LocalImageSet from an image folder\nlocal_image_set2 = ImageSet.read(\n/tmp/image/\n)\n\n# create DistributedImageSet from an image folder\ndistributed_image_set = ImageSet.read(\n/tmp/image/\n, sc, 2)\n\n\n\n\nImage Transformer\n\n\nAnalytics Zoo provides many pre-defined image processing transformers built on top of OpenCV. After create these transformers, call \ntransform\n with ImageSet to get transformed ImageSet. Or pass the transformer to NNEstimator/NNClassifier to preprocess before training. \n\n\nScala APIs:\n\n\npackage com.intel.analytics.zoo.feature.image\n\nobject ImageBrightness\n\ndef apply(deltaLow: Double, deltaHigh: Double): ImageBrightness\n\n\n\n\nAdjust the image brightness.\n\n\n\n\ndeltaLow: low bound of brightness parameter\n\n\ndeltaHigh: high bound of brightness parameter\n\n\n\n\nExample:\n\n\nval transformer = ImageBrightness(0.0, 32.0)\nval transformed = imageSet.transform(transformer)\n\n\n\n\nPython APIs:\n\n\nclass zoo.feature.image.imagePreprocessing.ImageBrightness\n\ndef __init__(delta_low, delta_high, bigdl_type=\nfloat\n)\n\n\n\n\nAdjust the image brightness.\n\n\n\n\ndelta_low: low bound of brightness parameter\n\n\ndelta_high: high bound of brightness parameter\n\n\n\n\nExample:\n\n\ntransformer = ImageBrightness(0.0, 32.0)\ntransformed = imageSet.transform(transformer)\n\n\n\n\nScala APIs:\n\n\npackage com.intel.analytics.zoo.feature.image\n\nobject ImageBytesToMat\n\ndef apply(byteKey: String = ImageFeature.bytes,\n          imageCodec: Int = Imgcodecs.CV_LOAD_IMAGE_UNCHANGED): ImageBytesToMat\n\n\n\n\nTransform byte array(original image file in byte) to OpenCVMat\n\n\n\n\nbyteKey: key that maps byte array. Default value is ImageFeature.bytes\n\n\nimageCodec: specifying the color type of a loaded image, same as in OpenCV.imread.\n              1. CV_LOAD_IMAGE_ANYDEPTH - If set, return 16-bit/32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit.\n              2. CV_LOAD_IMAGE_COLOR - If set, always convert image to the color one\n              3. CV_LOAD_IMAGE_GRAYSCALE - If set, always convert image to the grayscale one\n              4. \n0 Return a 3-channel color image.\n              Note The alpha channel is stripped from the output image. Use negative value if you need the alpha channel.\n              5. =0 Return a grayscale image.\n              6. \n0 Return the loaded image as is (with alpha channel).\n              Default value is Imgcodecs.CV_LOAD_IMAGE_UNCHANGED.\n\n\n\n\nExample:\n\n\nval imageSet = ImageSet.read(path, sc)\nimageSet -\n ImageBytesToMat()\n\n\n\n\n3D Image Support\n\n\nCreate ImageSet for 3D Images\n\n\nFor 3D images, you can still use ImageSet as the collection of ImageFeature3D. You can create ImageSet for 3D images in the similar way as for 2D images. Since we do not provide 3D image reader in analytics zoo, before create ImageSet, we suppose you already read 3D images to tensor(scala) or numpy array(python).\n\n\nScala example:\n\n\nval image = ImageFeature3D(tensor)\n\n// create local imageset for 3D images\nval arr = Array[ImageFeature](image)\nval localImageSet = ImageSet.array(arr)\n\n// create distributed imageset for 3D images\nval rdd = sc.parallelize(Seq[ImageFeature](image))\nval imageSet = ImageSet.rdd(rdd)\n\n\n\n\nPython example:\n\n\n\n# get image numpy array\nimg_np =\n\n# create local imageset for 3D images\nlocal_imageset = LocalImageSet(image_list=[img_np])\n\n# create distributed imageset for 3D images\nrdd = sc.parallelize([img_np])\ndist_imageSet = DistributedImageSet(image_rdd=rdd)\n\n\n\n\n3D Image Transformers\n\n\nAnalytics zoo also provides several image transformers for 3D Images.\nThe usage is similar as 2D image transformers. After create these transformers, call \ntransform\n with ImageSet to get transformed ImageSet.\n\n\nCurrently we support three kinds of 3D image transformers: Crop, Rotation and Affine Transformation.\n\n\nCrop transformers\n\n\nCrop3D\n\n\nScala:\n\n\nimport com.intel.analytics.zoo.feature.image3d.Crop3D\n\n// create Crop3D transformer\nval cropper = Crop3D(start, patchSize)\nval outputImageSet = imageset.transform(cropper)\n\n\n\n\nCrop a patch from a 3D image from 'start' of patch size. The patch size should be less than the image size.\n   * start: start point array(depth, height, width) for cropping\n   * patchSize: patch size array(depth, height, width)\n\n\nPython:\n\n\nfrom zoo.feature.image3d.transformation import Crop3D\n\ncrop = Crop3D(start, patch_size)\ntransformed_image = crop(image_set)\n\n\n\n\n\n\nstart: start point list[]depth, height, width] for cropping\n\n\npatch_size: patch size list[]depth, height, width]\n\n\n\n\nRandomCrop3D\n\n\nScala:\n\n\nimport com.intel.analytics.zoo.feature.image3d.RandomCrop3D\n\n// create Crop3D transformer\nval cropper = RandomCrop3D(cropDepth, cropHeight, cropWidth)\nval outputImageSet = imageset.transform(cropper)\n\n\n\n\nCrop a random patch from an 3D image with specified patch size. The patch size should be less than the image size.\n\n cropDepth: depth after crop\n\n cropHeight: height after crop\n* cropWidth: width after crop\n\n\nPython:\n\n\nfrom zoo.feature.image3d.transformation import RandomCrop3D\n\ncrop = RandomCrop3D(crop_depth, crop_height, crop_width)\ntransformed_image = crop(image_set)\n\n\n\n\n\n\ncrop_depth: depth after crop\n\n\ncrop_height: height after crop\n\n\ncrop_width: width after crop\n\n\n\n\nCenterCrop3D\n\n\nScala:\n\n\nimport com.intel.analytics.zoo.feature.image3d.CenterCrop3D\n\n// create Crop3D transformer\nval cropper = CenterCrop3D(cropDepth, cropHeight, cropWidth)\nval outputImageSet = imageset.transform(cropper)\n\n\n\n\nCrop a \ncropDepth\n x \ncropWidth\n x \ncropHeight\n patch from center of image. The patch size should be less than the image size.\n\n cropDepth: depth after crop\n\n cropHeight: height after crop\n* cropWidth: width after crop\n\n\nPython:\n\n\nfrom zoo.feature.image3d.transformation import CenterCrop3D\n\ncrop = CenterCrop3D(crop_depth, crop_height, crop_width)\ntransformed_image = crop(image_set)\n\n\n\n\n\n\ncrop_depth: depth after crop\n\n\ncrop_height: height after crop\n\n\ncrop_width: width after crop\n\n\n\n\nRotation\n\n\nScala:\n\n\nimport com.intel.analytics.zoo.feature.image3d.Rotate3D\n\n// create Crop3D transformer\nval rotAngles = Array[Double](yaw, pitch, roll)\nval rot = Rotate3D(rotAngles)\nval outputImageSet = imageset.transform(rot)\n\n\n\n\nRotate a 3D image with specified angles.\n* rotationAngles: the angles for rotation.\n   Which are the yaw(a counterclockwise rotation angle about the z-axis),\n   pitch(a counterclockwise rotation angle about the y-axis),\n   and roll(a counterclockwise rotation angle about the x-axis).\n\n\nPython:\n\n\nfrom zoo.feature.image3d.transformation import Rotate3D\n\nrot = Rotate3D(rotation_angles)\ntransformed_image = rot(image_set)\n\n\n\n\nAffine Transformation\n\n\nScala:\n\n\nimport com.intel.analytics.zoo.feature.image3d.AffineTransform3D\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n// create Crop3D transformer\nval matArray = Array[Double](1, 0, 0, 0, 1.5, 1.2, 0, 1.3, 1.4)\nval matTensor = Tensor[Double](matArray, Array[Int](3, 3))\nval trans = Tensor[Double](3)\ntrans(1) = 0\ntrans(2) = 1.8\ntrans(3) = 1.1\nval aff = AffineTransform3D(mat=matTensor, translation = trans, clampMode = \nclamp\n, padVal = 0)\nval outputImageSet = imageset.transform(aff)\n\n\n\n\nAffine transformer implements affine transformation on a given tensor.\nTo avoid defects in resampling, the mapping is from destination to source.\ndst(z,y,x) = src(f(z),f(y),f(x)) where f: dst -\n src\n\n\n\n\nmat: [Tensor[Double], dim: DxHxW] defines affine transformation from dst to src.\n\n\ntranslation: [Tensor[Double], dim: 3, default: (0,0,0)] defines translation in each axis.\n\n\nclampMode: [String, (default: \"clamp\",'padding')] defines how to handle interpolation off the input image.\n\n\npadVal: [Double, default: 0] defines padding value when clampMode=\"padding\". Setting this value when clampMode=\"clamp\" will cause an error.\n\n\n\n\nPython:\n\n\nfrom zoo.feature.image3d.transformation import AffineTransform3D\n\naffine = AffineTransform3D(affine_mat, translation, clamp_mode, pad_val)\ntransformed_image = affine(image_set)\n\n\n\n\n\n\naffine_mat: numpy array in 3x3 shape.Define affine transformation from dst to src.\n\n\ntranslation: numpy array in 3 dimension.Default value is np.zero(3). Define translation in each axis.\n\n\nclamp_mode: str, default value is \"clamp\". Define how to handle interpolation off the input image.\n\n\npad_val: float, default is 0.0. Define padding value when clampMode=\"padding\". Setting this value when clampMode=\"clamp\" will cause an error.", 
            "title": "Image"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#load-image", 
            "text": "Analytics Zoo provides APIs to read image to different formats:", 
            "title": "Load Image"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#load-to-data-frame", 
            "text": "Scala:  package com.intel.analytics.zoo.pipeline.nnframes\n\nobject NNImageReader {\n  def readImages(path: String, sc: SparkContext, minPartitions: Int = 1, resizeH: Int = -1, resizeW: Int = -1): DataFrame\n}  Read the directory of images from the local or remote source, return DataFrame with a single column \"image\" of images.   path: Directory to the input data files, the path can be comma separated paths as the list of inputs. Wildcards path are supported similarly to sc.binaryFiles(path).  sc: SparkContext to be used.  minPartitions: Number of the DataFrame partitions, if omitted uses defaultParallelism instead  resizeH: height after resize, by default is -1 which will not resize the image  resizeW: width after resize, by default is -1 which will not resize the image   Python:  class zoo.pipeline.nnframes.NNImageReader\n    static readImages(path, sc=None, minPartitions=1, resizeH=-1, resizeW=-1, bigdl_type= float )", 
            "title": "Load to Data Frame"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#imageset", 
            "text": "ImageSet  is a collection of  ImageFeature . It can be a  DistributedImageSet  for distributed image RDD or\n  LocalImageSet  for local image array.\nYou can read image data as  ImageSet  from local/distributed image path, or you can directly construct a ImageSet from RDD[ImageFeature] or Array[ImageFeature].  Scala APIs:  object com.intel.analytics.zoo.feature.image.ImageSet  def array(data: Array[ImageFeature]): LocalImageSet  Create LocalImageSet from array of ImeageFeature   data: array of ImageFeature   def rdd(data: RDD[ImageFeature]): DistributedImageSet  Create DistributedImageSet from rdd of ImageFeature   data: array of ImageFeature     def read(path: String, sc: SparkContext = null, minPartitions: Int = 1, resizeH: Int = -1, resizeW: Int = -1): ImageSet  Read images as Image Set.\nIf sc is defined, read image as DistributedImageSet from local file system or HDFS.\nIf sc is null, Read image as LocalImageSet from local file system   path: path to read images. If sc is defined, path can be local or HDFS. Wildcard character are supported. If sc is null, path is local directory/image file/image file with wildcard character  sc: SparkContext  minPartitions: A suggestion value of the minimal splitting number for input data.  resizeH: height after resize, by default is -1 which will not resize the image  resizeW: width after resize, by default is -1 which will not resize the image   Example:  // create LocalImageSet from an image folder\nval localImageSet = ImageSet.read( /tmp/image/ )\n\n// create DistributedImageSet from an image folder\nval distributedImageSet2 = ImageSet.read( /tmp/image/ , sc, 2)  Python APIs:  class zoo.feature.image.ImageSet  read(path, sc=None, min_partitions=1, resize_height=-1, resize_width=-1, bigdl_type= float )  Read images as Image Set.\nIf sc is defined, read image as DistributedImageSet from local file system or HDFS.\nIf sc is null, Read image as LocalImageSet from local file system   path: path to read images. If sc is defined, path can be local or HDFS. Wildcard character are supported. If sc is null, path is local directory/image file/image file with wildcard character  sc: SparkContext  min_partitions: A suggestion value of the minimal splitting number for input data.  resize_height height after resize, by default is -1 which will not resize the image  resize_width width after resize, by default is -1 which will not resize the image   Python example:  # create LocalImageSet from an image folder\nlocal_image_set2 = ImageSet.read( /tmp/image/ )\n\n# create DistributedImageSet from an image folder\ndistributed_image_set = ImageSet.read( /tmp/image/ , sc, 2)", 
            "title": "ImageSet"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#image-transformer", 
            "text": "Analytics Zoo provides many pre-defined image processing transformers built on top of OpenCV. After create these transformers, call  transform  with ImageSet to get transformed ImageSet. Or pass the transformer to NNEstimator/NNClassifier to preprocess before training.   Scala APIs:  package com.intel.analytics.zoo.feature.image\n\nobject ImageBrightness\n\ndef apply(deltaLow: Double, deltaHigh: Double): ImageBrightness  Adjust the image brightness.   deltaLow: low bound of brightness parameter  deltaHigh: high bound of brightness parameter   Example:  val transformer = ImageBrightness(0.0, 32.0)\nval transformed = imageSet.transform(transformer)  Python APIs:  class zoo.feature.image.imagePreprocessing.ImageBrightness\n\ndef __init__(delta_low, delta_high, bigdl_type= float )  Adjust the image brightness.   delta_low: low bound of brightness parameter  delta_high: high bound of brightness parameter   Example:  transformer = ImageBrightness(0.0, 32.0)\ntransformed = imageSet.transform(transformer)  Scala APIs:  package com.intel.analytics.zoo.feature.image\n\nobject ImageBytesToMat\n\ndef apply(byteKey: String = ImageFeature.bytes,\n          imageCodec: Int = Imgcodecs.CV_LOAD_IMAGE_UNCHANGED): ImageBytesToMat  Transform byte array(original image file in byte) to OpenCVMat   byteKey: key that maps byte array. Default value is ImageFeature.bytes  imageCodec: specifying the color type of a loaded image, same as in OpenCV.imread.\n              1. CV_LOAD_IMAGE_ANYDEPTH - If set, return 16-bit/32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit.\n              2. CV_LOAD_IMAGE_COLOR - If set, always convert image to the color one\n              3. CV_LOAD_IMAGE_GRAYSCALE - If set, always convert image to the grayscale one\n              4.  0 Return a 3-channel color image.\n              Note The alpha channel is stripped from the output image. Use negative value if you need the alpha channel.\n              5. =0 Return a grayscale image.\n              6.  0 Return the loaded image as is (with alpha channel).\n              Default value is Imgcodecs.CV_LOAD_IMAGE_UNCHANGED.   Example:  val imageSet = ImageSet.read(path, sc)\nimageSet -  ImageBytesToMat()", 
            "title": "Image Transformer"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#3d-image-support", 
            "text": "", 
            "title": "3D Image Support"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#create-imageset-for-3d-images", 
            "text": "For 3D images, you can still use ImageSet as the collection of ImageFeature3D. You can create ImageSet for 3D images in the similar way as for 2D images. Since we do not provide 3D image reader in analytics zoo, before create ImageSet, we suppose you already read 3D images to tensor(scala) or numpy array(python).  Scala example:  val image = ImageFeature3D(tensor)\n\n// create local imageset for 3D images\nval arr = Array[ImageFeature](image)\nval localImageSet = ImageSet.array(arr)\n\n// create distributed imageset for 3D images\nval rdd = sc.parallelize(Seq[ImageFeature](image))\nval imageSet = ImageSet.rdd(rdd)  Python example:  \n# get image numpy array\nimg_np =\n\n# create local imageset for 3D images\nlocal_imageset = LocalImageSet(image_list=[img_np])\n\n# create distributed imageset for 3D images\nrdd = sc.parallelize([img_np])\ndist_imageSet = DistributedImageSet(image_rdd=rdd)", 
            "title": "Create ImageSet for 3D Images"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#3d-image-transformers", 
            "text": "Analytics zoo also provides several image transformers for 3D Images.\nThe usage is similar as 2D image transformers. After create these transformers, call  transform  with ImageSet to get transformed ImageSet.  Currently we support three kinds of 3D image transformers: Crop, Rotation and Affine Transformation.", 
            "title": "3D Image Transformers"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#crop-transformers", 
            "text": "", 
            "title": "Crop transformers"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#crop3d", 
            "text": "Scala:  import com.intel.analytics.zoo.feature.image3d.Crop3D\n\n// create Crop3D transformer\nval cropper = Crop3D(start, patchSize)\nval outputImageSet = imageset.transform(cropper)  Crop a patch from a 3D image from 'start' of patch size. The patch size should be less than the image size.\n   * start: start point array(depth, height, width) for cropping\n   * patchSize: patch size array(depth, height, width)  Python:  from zoo.feature.image3d.transformation import Crop3D\n\ncrop = Crop3D(start, patch_size)\ntransformed_image = crop(image_set)   start: start point list[]depth, height, width] for cropping  patch_size: patch size list[]depth, height, width]", 
            "title": "Crop3D"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#randomcrop3d", 
            "text": "Scala:  import com.intel.analytics.zoo.feature.image3d.RandomCrop3D\n\n// create Crop3D transformer\nval cropper = RandomCrop3D(cropDepth, cropHeight, cropWidth)\nval outputImageSet = imageset.transform(cropper)  Crop a random patch from an 3D image with specified patch size. The patch size should be less than the image size.  cropDepth: depth after crop  cropHeight: height after crop\n* cropWidth: width after crop  Python:  from zoo.feature.image3d.transformation import RandomCrop3D\n\ncrop = RandomCrop3D(crop_depth, crop_height, crop_width)\ntransformed_image = crop(image_set)   crop_depth: depth after crop  crop_height: height after crop  crop_width: width after crop", 
            "title": "RandomCrop3D"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#centercrop3d", 
            "text": "Scala:  import com.intel.analytics.zoo.feature.image3d.CenterCrop3D\n\n// create Crop3D transformer\nval cropper = CenterCrop3D(cropDepth, cropHeight, cropWidth)\nval outputImageSet = imageset.transform(cropper)  Crop a  cropDepth  x  cropWidth  x  cropHeight  patch from center of image. The patch size should be less than the image size.  cropDepth: depth after crop  cropHeight: height after crop\n* cropWidth: width after crop  Python:  from zoo.feature.image3d.transformation import CenterCrop3D\n\ncrop = CenterCrop3D(crop_depth, crop_height, crop_width)\ntransformed_image = crop(image_set)   crop_depth: depth after crop  crop_height: height after crop  crop_width: width after crop", 
            "title": "CenterCrop3D"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#rotation", 
            "text": "Scala:  import com.intel.analytics.zoo.feature.image3d.Rotate3D\n\n// create Crop3D transformer\nval rotAngles = Array[Double](yaw, pitch, roll)\nval rot = Rotate3D(rotAngles)\nval outputImageSet = imageset.transform(rot)  Rotate a 3D image with specified angles.\n* rotationAngles: the angles for rotation.\n   Which are the yaw(a counterclockwise rotation angle about the z-axis),\n   pitch(a counterclockwise rotation angle about the y-axis),\n   and roll(a counterclockwise rotation angle about the x-axis).  Python:  from zoo.feature.image3d.transformation import Rotate3D\n\nrot = Rotate3D(rotation_angles)\ntransformed_image = rot(image_set)", 
            "title": "Rotation"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/image/#affine-transformation", 
            "text": "Scala:  import com.intel.analytics.zoo.feature.image3d.AffineTransform3D\nimport com.intel.analytics.bigdl.tensor.Tensor\n\n// create Crop3D transformer\nval matArray = Array[Double](1, 0, 0, 0, 1.5, 1.2, 0, 1.3, 1.4)\nval matTensor = Tensor[Double](matArray, Array[Int](3, 3))\nval trans = Tensor[Double](3)\ntrans(1) = 0\ntrans(2) = 1.8\ntrans(3) = 1.1\nval aff = AffineTransform3D(mat=matTensor, translation = trans, clampMode =  clamp , padVal = 0)\nval outputImageSet = imageset.transform(aff)  Affine transformer implements affine transformation on a given tensor.\nTo avoid defects in resampling, the mapping is from destination to source.\ndst(z,y,x) = src(f(z),f(y),f(x)) where f: dst -  src   mat: [Tensor[Double], dim: DxHxW] defines affine transformation from dst to src.  translation: [Tensor[Double], dim: 3, default: (0,0,0)] defines translation in each axis.  clampMode: [String, (default: \"clamp\",'padding')] defines how to handle interpolation off the input image.  padVal: [Double, default: 0] defines padding value when clampMode=\"padding\". Setting this value when clampMode=\"clamp\" will cause an error.   Python:  from zoo.feature.image3d.transformation import AffineTransform3D\n\naffine = AffineTransform3D(affine_mat, translation, clamp_mode, pad_val)\ntransformed_image = affine(image_set)   affine_mat: numpy array in 3x3 shape.Define affine transformation from dst to src.  translation: numpy array in 3 dimension.Default value is np.zero(3). Define translation in each axis.  clamp_mode: str, default value is \"clamp\". Define how to handle interpolation off the input image.  pad_val: float, default is 0.0. Define padding value when clampMode=\"padding\". Setting this value when clampMode=\"clamp\" will cause an error.", 
            "title": "Affine Transformation"
        }, 
        {
            "location": "/APIGuide/FeatureEngineering/preprocessing/", 
            "text": "", 
            "title": "Preprocessing"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/", 
            "text": "Analytics Zoo provides a collection of pre-trained models for Object Detection. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios. User can run the inference as local program without Spark Context, or in a distributed environment such like Apache Spark, Apache Storm or Apache Flink.\n\n\nModel Load\n\n\nUse \nObjectDetector.loadModel\n(in Scala) or \nObjectDetector.load_model\n (in Python) to load an pre-trained Analytics Zoo model or third-party(BigDL) model. We just need to specify the model path and optionally weight path if exists where we previously saved the model.\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.objectdetection._\n\nval model = ObjectDetector.loadModel[Float](\n/tmp/zoo.model\n) //load from local fs\nval model = ObjectDetector.loadModel(\nhdfs://...\n) //load from hdfs\nval model = ObjectDetector.loadModel(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nfrom zoo.models.image.objectdetection import *\n\nmodel = ObjectDetector.load_model(\n/tmp/zoo.model\n) //load from local fs\nmodel = ObjectDetector.load_model(\nhdfs://...\n) //load from hdfs\nmodel = ObjectDetector.load_model(\ns3://...\n) //load from s3\n\n\n\n\nCreat image configuration\n\n\nIf the loaded model is a published Analytics Zoo model, when you call \nObjectDetector.loadModel\n(in Scala) or \nObjectDetector.load_model\n (in Python), it would create the default Image Configuration for model inference. If the loaded model is not a published Analytics Zoo model or you want to customize the configuration for model inference, you need to create your own Image Configuration.\n\n\nScala API\n\n\nImageConfigure[T: ClassTag](\n  preProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  postProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  batchPerPartition: Int = 4,\n  labelMap: Map[Int, String] = null,\n  featurePaddingParam: Option[PaddingParam[T]] = None)\n\n\n\n\n\n\npreProcessor: preprocessor of ImageSet before model inference\n\n\npostProcessor: postprocessor of ImageSet after model inference\n\n\nbatchPerPartition: batch size per partition\n\n\nlabelMap: label mapping\n\n\nfeaturePaddingParam: featurePaddingParam if the inputs have variant size\n\n\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.common._\nimport com.intel.analytics.zoo.feature.image._\n\nval preprocessing = ImageResize(256, 256)-\n ImageCenterCrop(224, 224) -\n\n                     ImageChannelNormalize(123, 117, 104) -\n\n                     ImageMatToTensor[Float]() -\n\n                     ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preProcessor=preprocessing)\n\n\n\n\nPython API\n\n\nclass ImageConfigure()\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type=\nfloat\n)\n\n\n\n\n\n\npre_processor:  preprocessor of ImageSet before model inference\n\n\npost_processor:  postprocessor of ImageSet after model inference\n\n\nbatch_per_partition:  batch size per partition\n\n\nlabel_map mapping:  from prediction result indexes to real dataset labels\n\n\nfeature_padding_param:  featurePaddingParam if the inputs have variant size\n\n\n\n\nPython example\n\n\nfrom zoo.models.image.common.image_config import *\nfrom zoo.feature.image.imagePreprocessing import *\n\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(pre_processor=preprocessing)\n\n\n\n\nPredict with loaded object detection model\n\n\nScala API\n\n\npredictImageSet(image: ImageSet, configure: ImageConfigure[T] = null)\n\n\n\n\n\n\nimage:  Analytics Zoo ImageSet to be predicted\n\n\nconfigure: Image Configure for this prediction\n\n\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.objectdetection._\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.feature.image._\n\nval imagePath=\n/tmp/image\n\nval sc = NNContext.initNNContext()\nval model = ObjectDetector.loadModel(\n/tmp/analytics-zoo_ssd-mobilenet-300x300_PASCAL_0.1.0.model\n)\nval data = ImageSet.read(image_path, sc)\nval output = model.predictImageSet(data)\n\n\n\n\nPython API\n\n\npredict_image_set(image, configure=None)\n\n\n\n\n\n\nimage:  Analytics Zoo ImageSet to be predicted\n\n\nconfigure: Image Configure for this  prediction\n\n\n\n\nPython example\n\n\nfrom zoo.common.nncontext import *\nfrom zoo.models.image.objectdetection import *\n\nsc = init_nncontext()\nmodel = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = model.predict_image_set(image_set)", 
            "title": "Object Detection"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#model-load", 
            "text": "Use  ObjectDetector.loadModel (in Scala) or  ObjectDetector.load_model  (in Python) to load an pre-trained Analytics Zoo model or third-party(BigDL) model. We just need to specify the model path and optionally weight path if exists where we previously saved the model.  Scala example  import com.intel.analytics.zoo.models.image.objectdetection._\n\nval model = ObjectDetector.loadModel[Float]( /tmp/zoo.model ) //load from local fs\nval model = ObjectDetector.loadModel( hdfs://... ) //load from hdfs\nval model = ObjectDetector.loadModel( s3://... ) //load from s3  Python example  from zoo.models.image.objectdetection import *\n\nmodel = ObjectDetector.load_model( /tmp/zoo.model ) //load from local fs\nmodel = ObjectDetector.load_model( hdfs://... ) //load from hdfs\nmodel = ObjectDetector.load_model( s3://... ) //load from s3", 
            "title": "Model Load"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#creat-image-configuration", 
            "text": "If the loaded model is a published Analytics Zoo model, when you call  ObjectDetector.loadModel (in Scala) or  ObjectDetector.load_model  (in Python), it would create the default Image Configuration for model inference. If the loaded model is not a published Analytics Zoo model or you want to customize the configuration for model inference, you need to create your own Image Configuration.  Scala API  ImageConfigure[T: ClassTag](\n  preProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  postProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  batchPerPartition: Int = 4,\n  labelMap: Map[Int, String] = null,\n  featurePaddingParam: Option[PaddingParam[T]] = None)   preProcessor: preprocessor of ImageSet before model inference  postProcessor: postprocessor of ImageSet after model inference  batchPerPartition: batch size per partition  labelMap: label mapping  featurePaddingParam: featurePaddingParam if the inputs have variant size   Scala example  import com.intel.analytics.zoo.models.image.common._\nimport com.intel.analytics.zoo.feature.image._\n\nval preprocessing = ImageResize(256, 256)-  ImageCenterCrop(224, 224) - \n                     ImageChannelNormalize(123, 117, 104) - \n                     ImageMatToTensor[Float]() - \n                     ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preProcessor=preprocessing)  Python API  class ImageConfigure()\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type= float )   pre_processor:  preprocessor of ImageSet before model inference  post_processor:  postprocessor of ImageSet after model inference  batch_per_partition:  batch size per partition  label_map mapping:  from prediction result indexes to real dataset labels  feature_padding_param:  featurePaddingParam if the inputs have variant size   Python example  from zoo.models.image.common.image_config import *\nfrom zoo.feature.image.imagePreprocessing import *\n\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(pre_processor=preprocessing)", 
            "title": "Creat image configuration"
        }, 
        {
            "location": "/APIGuide/Models/object-detection/#predict-with-loaded-object-detection-model", 
            "text": "Scala API  predictImageSet(image: ImageSet, configure: ImageConfigure[T] = null)   image:  Analytics Zoo ImageSet to be predicted  configure: Image Configure for this prediction   Scala example  import com.intel.analytics.zoo.models.image.objectdetection._\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.feature.image._\n\nval imagePath= /tmp/image \nval sc = NNContext.initNNContext()\nval model = ObjectDetector.loadModel( /tmp/analytics-zoo_ssd-mobilenet-300x300_PASCAL_0.1.0.model )\nval data = ImageSet.read(image_path, sc)\nval output = model.predictImageSet(data)  Python API  predict_image_set(image, configure=None)   image:  Analytics Zoo ImageSet to be predicted  configure: Image Configure for this  prediction   Python example  from zoo.common.nncontext import *\nfrom zoo.models.image.objectdetection import *\n\nsc = init_nncontext()\nmodel = ObjectDetector.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = model.predict_image_set(image_set)", 
            "title": "Predict with loaded object detection model"
        }, 
        {
            "location": "/APIGuide/Models/image-classification/", 
            "text": "Analytics Zoo provides a collection of pre-trained models for Image Classification. These models can be used for out-of-the-box inference if you are interested in categories already in the corresponding datasets. According to the business scenarios, users can embed the models locally, distributedly in Spark such as Apache Storm and Apache Flink.\n\n\nModel Load\n\n\nUse \nImageClassifier.loadModel\n(in Scala) or \nImageClassifier.load_model\n (in Python) to load an pre-trained analytics zoo model or third-party(BigDL) model.  \nModule\n (Scala) or \nModel\n(Python) is a utility class provided in BigDL. We just need to specify the model path and optionally weight path if exists where we previously saved the model.\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.imageclassification._\n\nval model = ImageClassifier.loadModel[Float](\n/tmp/model.zoo\n, \n/tmp/model.bin\n) //load from local fs\nval model = ImageClassifier.loadModel(\nhdfs://...\n) //load from hdfs\nval model = ImageClassifier.loadModel(\ns3://...\n) //load from s3\n\n\n\n\nPython example\n\n\nfrom zoo.models.image.imageclassification import *\n\nmodel = ImageClassifier.load_model(\n/tmp/...model\n, \n/tmp/model.bin\n) //load from local fs\nmodel = ImageClassifier.load_model(\nhdfs://...\n) //load from hdfs\nmodel = ImageClassifier.load_model(\ns3://...\n) //load from s3\n\n\n\n\nCreat image configuration\n\n\nIf the loaded model is a published Analytics Zoo model, when you call \nImageClassifier.loadModel\n(in Scala) or \nImageClassifier.load_model\n (in Python), it would create the default Image Configuration for model inference. If the loaded model is not a published Analytics Zoo model or you want to customize the configuration for model inference, you need to create your own Image Configuration. \n\n\nScala API\n\n\nImageConfigure[T: ClassTag](\n  preProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  postProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  batchPerPartition: Int = 4,\n  labelMap: Map[Int, String] = null,\n  featurePaddingParam: Option[PaddingParam[T]] = None)\n\n\n\n\n\n\npreProcessor: preprocessor of ImageFrame before model inference\n\n\npostProcessor: postprocessor of ImageFrame after model inference\n\n\nbatchPerPartition: batch size per partition\n\n\nlabelMap: label mapping\n\n\nfeaturePaddingParam: featurePaddingParam if the inputs have variant size\n\n\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.common._\nimport com.intel.analytics.zoo.feature.image._\n\nval preprocessing = ImageResize(256, 256)-\n ImageCenterCrop(224, 224) -\n\n                     ImageChannelNormalize(123, 117, 104) -\n\n                     ImageMatToTensor[Float]() -\n\n                     ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preProcessor=preprocessing)\n\n\n\n\nPython API\n\n\nclass ImageConfigure()\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type=\nfloat\n)\n\n\n\n\n\n\npre_processor:  preprocessor of ImageSet before model inference\n\n\npost_processor:  postprocessor of ImageSet after model inference\n\n\nbatch_per_partition:  batch size per partition\n\n\nlabel_map mapping:  from prediction result indexes to real dataset labels\n\n\nfeature_padding_param:  featurePaddingParam if the inputs have variant size\n\n\n\n\nPython example\n\n\nfrom zoo.models.image.common.image_config import *\nfrom zoo.feature.image.imagePreprocessing import *\n\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(pre_processor=preprocessing) \n\n\n\n\nPredict with loaded image classification model\n\n\nScala API\n\n\npredictImageSet(image: ImageSet, configure: ImageConfigure[T] = null)\n\n\n\n\n\n\nimage:  Analytics Zoo ImageSet to be predicted\n\n\nconfigure: Image Configure for this  predcition\n\n\n\n\nScala example\n\n\nimport com.intel.analytics.zoo.models.image.imageclassification._\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.feature.image._\n\nval imagePath=\n/tmp/image\n\nval sc = NNContext.initNNContext()\nval model = ImageClassifier.loadModel(\n/tmp/analytics-zoo_inception-v1_imagenet_0.1.0\n) \nval data = ImageSet.read(image_path, sc)\nval output = model.predictImageSet(data)\n\n\n\n\nPython API\n\n\npredict_image_set(image, configure=None)\n\n\n\n\n\n\nimage:  Analytics Zoo ImageSet to be predicted\n\n\nconfigure: Image Configure for this predcition\n\n\n\n\nPython example\n\n\nfrom zoo.common.nncontext import *\nfrom zoo.models.image.imageclassification import *\n\nsc = init_nncontext()\nmodel = ImageClassifier.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = model.predict_image_set(image_set)", 
            "title": "Image Classification"
        }, 
        {
            "location": "/APIGuide/Models/image-classification/#model-load", 
            "text": "Use  ImageClassifier.loadModel (in Scala) or  ImageClassifier.load_model  (in Python) to load an pre-trained analytics zoo model or third-party(BigDL) model.   Module  (Scala) or  Model (Python) is a utility class provided in BigDL. We just need to specify the model path and optionally weight path if exists where we previously saved the model.  Scala example  import com.intel.analytics.zoo.models.image.imageclassification._\n\nval model = ImageClassifier.loadModel[Float]( /tmp/model.zoo ,  /tmp/model.bin ) //load from local fs\nval model = ImageClassifier.loadModel( hdfs://... ) //load from hdfs\nval model = ImageClassifier.loadModel( s3://... ) //load from s3  Python example  from zoo.models.image.imageclassification import *\n\nmodel = ImageClassifier.load_model( /tmp/...model ,  /tmp/model.bin ) //load from local fs\nmodel = ImageClassifier.load_model( hdfs://... ) //load from hdfs\nmodel = ImageClassifier.load_model( s3://... ) //load from s3", 
            "title": "Model Load"
        }, 
        {
            "location": "/APIGuide/Models/image-classification/#creat-image-configuration", 
            "text": "If the loaded model is a published Analytics Zoo model, when you call  ImageClassifier.loadModel (in Scala) or  ImageClassifier.load_model  (in Python), it would create the default Image Configuration for model inference. If the loaded model is not a published Analytics Zoo model or you want to customize the configuration for model inference, you need to create your own Image Configuration.   Scala API  ImageConfigure[T: ClassTag](\n  preProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  postProcessor: Preprocessing[ImageFeature, ImageFeature] = null,\n  batchPerPartition: Int = 4,\n  labelMap: Map[Int, String] = null,\n  featurePaddingParam: Option[PaddingParam[T]] = None)   preProcessor: preprocessor of ImageFrame before model inference  postProcessor: postprocessor of ImageFrame after model inference  batchPerPartition: batch size per partition  labelMap: label mapping  featurePaddingParam: featurePaddingParam if the inputs have variant size   Scala example  import com.intel.analytics.zoo.models.image.common._\nimport com.intel.analytics.zoo.feature.image._\n\nval preprocessing = ImageResize(256, 256)-  ImageCenterCrop(224, 224) - \n                     ImageChannelNormalize(123, 117, 104) - \n                     ImageMatToTensor[Float]() - \n                     ImageSetToSample[Float]()\nval config = ImageConfigure[Float](preProcessor=preprocessing)  Python API  class ImageConfigure()\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type= float )   pre_processor:  preprocessor of ImageSet before model inference  post_processor:  postprocessor of ImageSet after model inference  batch_per_partition:  batch size per partition  label_map mapping:  from prediction result indexes to real dataset labels  feature_padding_param:  featurePaddingParam if the inputs have variant size   Python example  from zoo.models.image.common.image_config import *\nfrom zoo.feature.image.imagePreprocessing import *\n\npreprocessing = ChainedPreprocessing(\n                [ImageResize(256, 256), ImageCenterCrop(224, 224),\n                ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(),\n                ImageSetToSample()])\nconfig = ImageConfigure(pre_processor=preprocessing)", 
            "title": "Creat image configuration"
        }, 
        {
            "location": "/APIGuide/Models/image-classification/#predict-with-loaded-image-classification-model", 
            "text": "Scala API  predictImageSet(image: ImageSet, configure: ImageConfigure[T] = null)   image:  Analytics Zoo ImageSet to be predicted  configure: Image Configure for this  predcition   Scala example  import com.intel.analytics.zoo.models.image.imageclassification._\nimport com.intel.analytics.zoo.common.NNContext\nimport com.intel.analytics.zoo.feature.image._\n\nval imagePath= /tmp/image \nval sc = NNContext.initNNContext()\nval model = ImageClassifier.loadModel( /tmp/analytics-zoo_inception-v1_imagenet_0.1.0 ) \nval data = ImageSet.read(image_path, sc)\nval output = model.predictImageSet(data)  Python API  predict_image_set(image, configure=None)   image:  Analytics Zoo ImageSet to be predicted  configure: Image Configure for this predcition   Python example  from zoo.common.nncontext import *\nfrom zoo.models.image.imageclassification import *\n\nsc = init_nncontext()\nmodel = ImageClassifier.load_model(model_path)\nimage_set = ImageSet.read(img_path, sc)\noutput = model.predict_image_set(image_set)", 
            "title": "Predict with loaded image classification model"
        }, 
        {
            "location": "/APIGuide/Models/text-classification/", 
            "text": "Analytics Zoo provides pre-defined models having different encoders that can be used for classifying texts.\nThe model could be fed into NNFrames or BigDL Optimizer directly for training.\n\n\n\n\nBuild a TextClassifier Model\n\n\nYou can call the following API in Scala and Python respectively to create a \nTextClassifier\n with \npre-trained GloVe word embeddings as the first layer\n.\n\n\nScala\n\n\nval textClassifier = TextClassifier(classNum, embeddingFile, wordIndex = null, sequenceLength = 500, encoder = \ncnn\n, encoderOutputDim = 256)\n\n\n\n\n\n\nclassNum\n: The number of text categories to be classified. Positive integer.\n\n\nembeddingFile\n The path to the word embedding file. Currently only \nglove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt\n are supported. You can download from \nhere\n.\n\n\nwordIndex\n Map of word (String) and its corresponding index (integer). The index is supposed to \nstart from 1\n with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is null. In this case, all the words in the embeddingFile will be taken into account and you can call \nWordEmbedding.getWordIndex(embeddingFile)\n to retrieve the map.\n\n\nsequenceLength\n: The length of a sequence. Positive integer. Default is 500.\n\n\nencoder\n: The encoder for input sequences. String. \"cnn\" or \"lstm\" or \"gru\" are supported. Default is \"cnn\".\n\n\nencoderOutputDim\n: The output dimension for the encoder. Positive integer. Default is 256.\n\n\n\n\nSee \nhere\n for the Scala example that trains the TextClassifier model on 20 Newsgroup dataset and uses the model to do prediction.\n\n\nPython\n\n\ntext_classifier = TextClassifier(class_num, embedding_file, word_index=None, sequence_length=500, encoder=\ncnn\n, encoder_output_dim=256)\n\n\n\n\n\n\nclass_num\n: The number of text categories to be classified. Positive int.\n\n\nembedding_file\n The path to the word embedding file. Currently only \nglove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt\n are supported. You can download from \nhere\n.\n\n\nword_index\n Dictionary of word (string) and its corresponding index (int). The index is supposed to \nstart from 1\n with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is None. In this case, all the words in the embedding_file will be taken into account and you can call \nWordEmbedding.get_word_index(embedding_file)\n to retrieve the map.\n\n\nsequence_length\n: The length of a sequence. Positive int. Default is 500.\n\n\nencoder\n: The encoder for input sequences. String. 'cnn' or 'lstm' or 'gru' are supported. Default is 'cnn'.\n\n\nencoder_output_dim\n: The output dimension for the encoder. Positive int. Default is 256.\n\n\n\n\nSee \nhere\n for the Python example that trains the TextClassifier model on 20 Newsgroup dataset and uses the model to do prediction.\n\n\n\n\nSave Model\n\n\nAfter building and training a TextClassifier model, you can save it for future use.\n\n\nScala\n\n\ntextClassifier.saveModel(path, weightPath = null, overWrite = false)\n\n\n\n\n\n\npath\n: The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".\n\n\nweightPath\n: The path to save weights. Default is null.\n\n\noverWrite\n: Whether to overwrite the file if it already exists. Default is false.\n\n\n\n\nPython\n\n\ntext_classifier.save_model(path, weight_path=None, over_write=False)\n\n\n\n\n\n\npath\n: The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.\n\n\nweight_path\n: The path to save weights. Default is None.\n\n\nover_write\n: Whether to overwrite the file if it already exists. Default is False.\n\n\n\n\n\n\nLoad Model\n\n\nTo load a TextClassifier model (with weights) saved \nabove\n:\n\n\nScala\n\n\nTextClassifier.loadModel[Float](path, weightPath = null)\n\n\n\n\n\n\npath\n: The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".\n\n\nweightPath\n: The path for pre-trained weights if any. Default is null.\n\n\n\n\nPython\n\n\nTextClassifier.load_model(path, weight_path=None)\n\n\n\n\n\n\npath\n: The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.\n\n\nweight_path\n: The path for pre-trained weights if any. Default is None.", 
            "title": "Text Classification"
        }, 
        {
            "location": "/APIGuide/Models/text-classification/#build-a-textclassifier-model", 
            "text": "You can call the following API in Scala and Python respectively to create a  TextClassifier  with  pre-trained GloVe word embeddings as the first layer .  Scala  val textClassifier = TextClassifier(classNum, embeddingFile, wordIndex = null, sequenceLength = 500, encoder =  cnn , encoderOutputDim = 256)   classNum : The number of text categories to be classified. Positive integer.  embeddingFile  The path to the word embedding file. Currently only  glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt  are supported. You can download from  here .  wordIndex  Map of word (String) and its corresponding index (integer). The index is supposed to  start from 1  with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is null. In this case, all the words in the embeddingFile will be taken into account and you can call  WordEmbedding.getWordIndex(embeddingFile)  to retrieve the map.  sequenceLength : The length of a sequence. Positive integer. Default is 500.  encoder : The encoder for input sequences. String. \"cnn\" or \"lstm\" or \"gru\" are supported. Default is \"cnn\".  encoderOutputDim : The output dimension for the encoder. Positive integer. Default is 256.   See  here  for the Scala example that trains the TextClassifier model on 20 Newsgroup dataset and uses the model to do prediction.  Python  text_classifier = TextClassifier(class_num, embedding_file, word_index=None, sequence_length=500, encoder= cnn , encoder_output_dim=256)   class_num : The number of text categories to be classified. Positive int.  embedding_file  The path to the word embedding file. Currently only  glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt  are supported. You can download from  here .  word_index  Dictionary of word (string) and its corresponding index (int). The index is supposed to  start from 1  with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is None. In this case, all the words in the embedding_file will be taken into account and you can call  WordEmbedding.get_word_index(embedding_file)  to retrieve the map.  sequence_length : The length of a sequence. Positive int. Default is 500.  encoder : The encoder for input sequences. String. 'cnn' or 'lstm' or 'gru' are supported. Default is 'cnn'.  encoder_output_dim : The output dimension for the encoder. Positive int. Default is 256.   See  here  for the Python example that trains the TextClassifier model on 20 Newsgroup dataset and uses the model to do prediction.", 
            "title": "Build a TextClassifier Model"
        }, 
        {
            "location": "/APIGuide/Models/text-classification/#save-model", 
            "text": "After building and training a TextClassifier model, you can save it for future use.  Scala  textClassifier.saveModel(path, weightPath = null, overWrite = false)   path : The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".  weightPath : The path to save weights. Default is null.  overWrite : Whether to overwrite the file if it already exists. Default is false.   Python  text_classifier.save_model(path, weight_path=None, over_write=False)   path : The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.  weight_path : The path to save weights. Default is None.  over_write : Whether to overwrite the file if it already exists. Default is False.", 
            "title": "Save Model"
        }, 
        {
            "location": "/APIGuide/Models/text-classification/#load-model", 
            "text": "To load a TextClassifier model (with weights) saved  above :  Scala  TextClassifier.loadModel[Float](path, weightPath = null)   path : The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".  weightPath : The path for pre-trained weights if any. Default is null.   Python  TextClassifier.load_model(path, weight_path=None)   path : The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.  weight_path : The path for pre-trained weights if any. Default is None.", 
            "title": "Load Model"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/", 
            "text": "Analytics Zoo provides two Recommenders, including Wide and Deep (WND) model and Neural network-based Collaborative Filtering (NCF) model. Each model could be fed into NNFrames or BigDL Optimizer directly for training.\n\n\nRecommenders can handle models with either explict or implicit feedback, given corresponding features.\n\n\nWe also provide three user-friendly APIs to predict user item pairs, and recommend items (users) for users (items). See \nhere\n for more details.\n\n\n\n\nWide and Deep\n\n\nWide and Deep Learning Model, proposed by \nGoogle, 2016\n, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.\n\n\nAfter training the model, users can use the model to \ndo prediction and recommendation\n.\n\n\nScala\n\n\nval wideAndDeep = WideAndDeep(modelType = \nwide_n_deep\n, numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))\n\n\n\n\n\n\nmodelType\n: String. \"wide\", \"deep\", \"wide_n_deep\" are supported. Default is \"wide_n_deep\".\n\n\nnumClasses\n: The number of classes. Positive integer.\n\n\ncolumnInfo\n An instance of \nColumnFeatureInfo\n.\n\n\nhiddenLayers\n: Units of hidden layers for the deep model. Array of positive integers. Default is Array(40, 20, 10).\n\n\n\n\nSee \nhere\n for the Scala example that trains the WideAndDeep model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\nPython\n\n\nwide_and_deep = WideAndDeep(class_num, column_info, model_type=\nwide_n_deep\n, hidden_layers=(40, 20, 10))\n\n\n\n\n\n\nclass_num\n: The number of classes. Positive int.\n\n\ncolumn_info\n: An instance of \nColumnFeatureInfo\n.\n\n\nmodel_type\n: String. 'wide', 'deep' and 'wide_n_deep' are supported. Default is 'wide_n_deep'.\n\n\nhidden_layers\n: Units of hidden layers for the deep model. Tuple of positive int. Default is (40, 20, 10).\n\n\n\n\nSee \nhere\n for the Python notebook that trains the WideAndDeep model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\n\n\nNeural network-based Collaborative Filtering\n\n\nNCF (\nHe, 2015\n) leverages a multi-layer perceptrons to learn the user\u2013item interaction function. At the mean time, NCF can express and generalize matrix factorization under its framework. \nincludeMF\n(Boolean) is provided for users to build a \nNeuralCF\n model with or without matrix factorization. \n\n\nAfter training the model, users can use the model to \ndo prediction and recommendation\n.\n\n\nScala\n\n\nval ncf = NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)\n\n\n\n\n\n\nuserCount\n: The number of users. Positive integer.\n\n\nitemCount\n: The number of items. Positive integer.\n\n\nnumClasses\n: The number of classes. Positive integer.\n\n\nuserEmbed\n: Units of user embedding. Positive integer. Default is 20.\n\n\nitemEmbed\n: Units of item embedding. Positive integer. Default is 20.\n\n\nhiddenLayers\n: Units hiddenLayers for MLP. Array of positive integers. Default is Array(40, 20, 10).\n\n\nincludeMF\n: Whether to include Matrix Factorization. Boolean. Default is true.\n\n\nmfEmbed\n: Units of matrix factorization embedding. Positive integer. Default is 20.\n\n\n\n\nSee \nhere\n for the Scala example that trains the NeuralCF model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\nPython\n\n\nncf = NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)\n\n\n\n\n\n\nuser_count\n: The number of users. Positive int.\n\n\nitem_count\n: The number of classes. Positive int.\n\n\nclass_num:\n The number of classes. Positive int.\n\n\nuser_embed\n: Units of user embedding. Positive int. Default is 20.\n\n\nitem_embed\n: itemEmbed Units of item embedding. Positive int. Default is 20.\n\n\nhidden_layers\n: Units of hidden layers for MLP. Tuple of positive int. Default is (40, 20, 10).\n\n\ninclude_mf\n: Whether to include Matrix Factorization. Boolean. Default is True.\n\n\nmf_embed\n: Units of matrix factorization embedding. Positive int. Default is 20.\n\n\n\n\nSee \nhere\n for the Python notebook that trains the NeuralCF model on MovieLens 1M dataset and uses the model to do prediction and recommendation.\n\n\n\n\nPrediction and Recommendation\n\n\nPredict for user-item pairs\n\n\nGive prediction for each pair of user and item. Return RDD of \nUserItemPrediction\n.\n\n\nScala\n\n\npredictUserItemPair(featureRdd)\n\n\n\n\nPython\n\n\npredict_user_item_pair(feature_rdd)\n\n\n\n\nParameters:\n\n\n\n\nfeatureRdd\n: RDD of \nUserItemFeature\n.\n\n\n\n\nRecommend for users\n\n\nRecommend a number of items for each user. Return RDD of \nUserItemPrediction\n.\n\n\nScala\n\n\nrecommendForUser(featureRdd, maxItems)\n\n\n\n\nPython\n\n\nrecommend_for_user(feature_rdd, max_items)\n\n\n\n\nParameters:\n\n\n\n\nfeatureRdd\n: RDD of \nUserItemFeature\n.\n\n\nmaxItems\n: The number of items to be recommended to each user. Positive integer.\n\n\n\n\nRecommend for items\n\n\nRecommend a number of users for each item. Return RDD of \nUserItemPrediction\n.\n\n\nScala\n\n\nrecommendForItem(featureRdd, maxUsers)\n\n\n\n\nPython\n\n\nrecommend_for_item(feature_rdd, max_users)\n\n\n\n\nParameters:\n\n\n\n\nfeatureRdd\n: RDD of \nUserItemFeature\n.\n\n\nmaxUsers\n: The number of users to be recommended to each item. Positive integer.\n\n\n\n\n\n\nModel Save\n\n\nAfter building and training a WideAndDeep or NeuralCF model, you can save it for future use.\n\n\nScala\n\n\nwideAndDeep.saveModel(path, weightPath = null, overWrite = false)\n\nncf.saveModel(path, weightPath = null, overWrite = false)\n\n\n\n\n\n\npath\n: The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".\n\n\nweightPath\n: The path to save weights. Default is null.\n\n\noverWrite\n: Whether to overwrite the file if it already exists. Default is false.\n\n\n\n\nPython\n\n\nwide_and_deep.save_model(path, weight_path=None, over_write=False)\n\nncf.save_model(path, weight_path=None, over_write=False)\n\n\n\n\n\n\npath\n: The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.\n\n\nweight_path\n: The path to save weights. Default is None.\n\n\nover_write\n: Whether to overwrite the file if it already exists. Default is False.\n\n\n\n\n\n\nModel Load\n\n\nTo load a WideAndDeep or NeuralCF model (with weights) saved \nabove\n:\n\n\nScala\n\n\nWideAndDeep.loadModel[Float](path, weightPath = null)\n\nNeuralCF.loadModel[Float](path, weightPath = null)\n\n\n\n\n\n\npath\n: The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".\n\n\nweightPath\n: The path for pre-trained weights if any. Default is null.\n\n\n\n\nPython\n\n\nWideAndDeep.load_model(path, weight_path=None)\n\nNeuralCF.load_model(path, weight_path=None)\n\n\n\n\n\n\npath\n: The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.\n\n\nweight_path\n: The path for pre-trained weights if any. Default is None.\n\n\n\n\n\n\nUserItemFeature\n\n\nRepresent records of user-item with features.\n\n\nEach record should contain the following fields:\n\n\n\n\nuserId\n: Positive integer.\n\n\nitem_id\n: Positive integer.\n\n\nsample\n: \nSample\n which consists of feature(s) and label(s).\n\n\n\n\nScala\n\n\nUserItemFeature(userId, itemId, sample)\n\n\n\n\nPython\n\n\nUserItemFeature(user_id, item_id, sample)\n\n\n\n\n\n\nUserItemPrediction\n\n\nRepresent the prediction results of user-item pairs.\n\n\nEach prediction record will contain the following information:\n\n\n\n\nuserId\n: Positive integer.\n\n\nitemId\n: Positive integer.\n\n\nprediction\n: The prediction (rating) for the user on the item.\n\n\nprobability\n: The probability for the prediction.\n\n\n\n\nScala\n\n\nUserItemPrediction(userId, itemId, prediction, probability)\n\n\n\n\nPython\n\n\nUserItemPrediction(user_id, item_id, prediction, probability)\n\n\n\n\n\n\nColumnFeatureInfo\n\n\nAn instance of \nColumnFeatureInfo\n contains the same data information shared by the \nWideAndDeep\n model and its feature generation part.\n\n\nYou can choose to include the following information for feature engineering and the \nWideAndDeep\n model:\n\n\n\n\nwideBaseCols\n: Data of \nwideBaseCols\n together with \nwideCrossCols\n will be fed into the wide model.\n\n\nwideBaseDims\n: Dimensions of \nwideBaseCols\n. The dimensions of the data in \nwideBaseCols\n should be within the range of \nwideBaseDims\n.\n\n\nwideCrossCols\n: Data of \nwideCrossCols\n will be fed into the wide model.\n\n\nwideCrossDims\n: Dimensions of \nwideCrossCols\n. The dimensions of the data in \nwideCrossCols\n should be within the range of \nwideCrossDims\n.\n\n\nindicatorCols\n: Data of \nindicatorCols\n will be fed into the deep model as multi-hot vectors. \n\n\nindicatorDims\n: Dimensions of \nindicatorCols\n. The dimensions of the data in \nindicatorCols\n should be within the range of \nindicatorDims\n.\n\n\nembedCols\n: Data of \nembedCols\n will be fed into the deep model as embeddings.\n\n\nembedInDims\n: Input dimension of the data in \nembedCols\n. The dimensions of the data in \nembedCols\n should be within the range of \nembedInDims\n.\n\n\nembedOutDims\n: The dimensions of embeddings for \nembedCols\n.\n\n\ncontinuousCols\n: Data of \ncontinuousCols\n will be treated as continuous values for the deep model.\n\n\nlabel\n: The name of the 'label' column. String. Default is \"label\".\n\n\n\n\nRemark:\n\n\nFields that involve \nCols\n should be an array of String (Scala) or a list of String (Python) indicating the name of the columns in the data.\n\n\nFields that involve \nDims\n should be an array of integers (Scala) or a list of integers (Python) indicating the dimensions of the corresponding columns.\n\n\nIf any field is not specified, it will by default to be an empty array (Scala) or an empty list (Python).\n\n\nScala\n\n\nColumnFeatureInfo(\n    wideBaseCols = Array[String](),\n    wideBaseDims = Array[Int](),\n    wideCrossCols = Array[String](),\n    wideCrossDims = Array[Int](),\n    indicatorCols = Array[String](),\n    indicatorDims = Array[Int](),\n    embedCols = Array[String](),\n    embedInDims = Array[Int](),\n    embedOutDims = Array[Int](),\n    continuousCols = Array[String](),\n    label = \nlabel\n)\n\n\n\n\nPython\n\n\nColumnFeatureInfo(\n    wide_base_cols=None,\n    wide_base_dims=None,\n    wide_cross_cols=None,\n    wide_cross_dims=None,\n    indicator_cols=None,\n    indicator_dims=None,\n    embed_cols=None,\n    embed_in_dims=None,\n    embed_out_dims=None,\n    continuous_cols=None,\n    label=\nlabel\n)", 
            "title": "Recommendation"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#wide-and-deep", 
            "text": "Wide and Deep Learning Model, proposed by  Google, 2016 , is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.  After training the model, users can use the model to  do prediction and recommendation .  Scala  val wideAndDeep = WideAndDeep(modelType =  wide_n_deep , numClasses, columnInfo, hiddenLayers = Array(40, 20, 10))   modelType : String. \"wide\", \"deep\", \"wide_n_deep\" are supported. Default is \"wide_n_deep\".  numClasses : The number of classes. Positive integer.  columnInfo  An instance of  ColumnFeatureInfo .  hiddenLayers : Units of hidden layers for the deep model. Array of positive integers. Default is Array(40, 20, 10).   See  here  for the Scala example that trains the WideAndDeep model on MovieLens 1M dataset and uses the model to do prediction and recommendation.  Python  wide_and_deep = WideAndDeep(class_num, column_info, model_type= wide_n_deep , hidden_layers=(40, 20, 10))   class_num : The number of classes. Positive int.  column_info : An instance of  ColumnFeatureInfo .  model_type : String. 'wide', 'deep' and 'wide_n_deep' are supported. Default is 'wide_n_deep'.  hidden_layers : Units of hidden layers for the deep model. Tuple of positive int. Default is (40, 20, 10).   See  here  for the Python notebook that trains the WideAndDeep model on MovieLens 1M dataset and uses the model to do prediction and recommendation.", 
            "title": "Wide and Deep"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#neural-network-based-collaborative-filtering", 
            "text": "NCF ( He, 2015 ) leverages a multi-layer perceptrons to learn the user\u2013item interaction function. At the mean time, NCF can express and generalize matrix factorization under its framework.  includeMF (Boolean) is provided for users to build a  NeuralCF  model with or without matrix factorization.   After training the model, users can use the model to  do prediction and recommendation .  Scala  val ncf = NeuralCF(userCount, itemCount, numClasses, userEmbed = 20, itemEmbed = 20, hiddenLayers = Array(40, 20, 10), includeMF = true, mfEmbed = 20)   userCount : The number of users. Positive integer.  itemCount : The number of items. Positive integer.  numClasses : The number of classes. Positive integer.  userEmbed : Units of user embedding. Positive integer. Default is 20.  itemEmbed : Units of item embedding. Positive integer. Default is 20.  hiddenLayers : Units hiddenLayers for MLP. Array of positive integers. Default is Array(40, 20, 10).  includeMF : Whether to include Matrix Factorization. Boolean. Default is true.  mfEmbed : Units of matrix factorization embedding. Positive integer. Default is 20.   See  here  for the Scala example that trains the NeuralCF model on MovieLens 1M dataset and uses the model to do prediction and recommendation.  Python  ncf = NeuralCF(user_count, item_count, class_num, user_embed=20, item_embed=20, hidden_layers=(40, 20, 10), include_mf=True, mf_embed=20)   user_count : The number of users. Positive int.  item_count : The number of classes. Positive int.  class_num:  The number of classes. Positive int.  user_embed : Units of user embedding. Positive int. Default is 20.  item_embed : itemEmbed Units of item embedding. Positive int. Default is 20.  hidden_layers : Units of hidden layers for MLP. Tuple of positive int. Default is (40, 20, 10).  include_mf : Whether to include Matrix Factorization. Boolean. Default is True.  mf_embed : Units of matrix factorization embedding. Positive int. Default is 20.   See  here  for the Python notebook that trains the NeuralCF model on MovieLens 1M dataset and uses the model to do prediction and recommendation.", 
            "title": "Neural network-based Collaborative Filtering"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#prediction-and-recommendation", 
            "text": "Predict for user-item pairs  Give prediction for each pair of user and item. Return RDD of  UserItemPrediction .  Scala  predictUserItemPair(featureRdd)  Python  predict_user_item_pair(feature_rdd)  Parameters:   featureRdd : RDD of  UserItemFeature .   Recommend for users  Recommend a number of items for each user. Return RDD of  UserItemPrediction .  Scala  recommendForUser(featureRdd, maxItems)  Python  recommend_for_user(feature_rdd, max_items)  Parameters:   featureRdd : RDD of  UserItemFeature .  maxItems : The number of items to be recommended to each user. Positive integer.   Recommend for items  Recommend a number of users for each item. Return RDD of  UserItemPrediction .  Scala  recommendForItem(featureRdd, maxUsers)  Python  recommend_for_item(feature_rdd, max_users)  Parameters:   featureRdd : RDD of  UserItemFeature .  maxUsers : The number of users to be recommended to each item. Positive integer.", 
            "title": "Prediction and Recommendation"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#model-save", 
            "text": "After building and training a WideAndDeep or NeuralCF model, you can save it for future use.  Scala  wideAndDeep.saveModel(path, weightPath = null, overWrite = false)\n\nncf.saveModel(path, weightPath = null, overWrite = false)   path : The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".  weightPath : The path to save weights. Default is null.  overWrite : Whether to overwrite the file if it already exists. Default is false.   Python  wide_and_deep.save_model(path, weight_path=None, over_write=False)\n\nncf.save_model(path, weight_path=None, over_write=False)   path : The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.  weight_path : The path to save weights. Default is None.  over_write : Whether to overwrite the file if it already exists. Default is False.", 
            "title": "Model Save"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#model-load", 
            "text": "To load a WideAndDeep or NeuralCF model (with weights) saved  above :  Scala  WideAndDeep.loadModel[Float](path, weightPath = null)\n\nNeuralCF.loadModel[Float](path, weightPath = null)   path : The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like \"hdfs://[host]:[port]/xxx\". Amazon S3 path should be like \"s3a://bucket/xxx\".  weightPath : The path for pre-trained weights if any. Default is null.   Python  WideAndDeep.load_model(path, weight_path=None)\n\nNeuralCF.load_model(path, weight_path=None)   path : The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.  weight_path : The path for pre-trained weights if any. Default is None.", 
            "title": "Model Load"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#useritemfeature", 
            "text": "Represent records of user-item with features.  Each record should contain the following fields:   userId : Positive integer.  item_id : Positive integer.  sample :  Sample  which consists of feature(s) and label(s).   Scala  UserItemFeature(userId, itemId, sample)  Python  UserItemFeature(user_id, item_id, sample)", 
            "title": "UserItemFeature"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#useritemprediction", 
            "text": "Represent the prediction results of user-item pairs.  Each prediction record will contain the following information:   userId : Positive integer.  itemId : Positive integer.  prediction : The prediction (rating) for the user on the item.  probability : The probability for the prediction.   Scala  UserItemPrediction(userId, itemId, prediction, probability)  Python  UserItemPrediction(user_id, item_id, prediction, probability)", 
            "title": "UserItemPrediction"
        }, 
        {
            "location": "/APIGuide/Models/recommendation/#columnfeatureinfo", 
            "text": "An instance of  ColumnFeatureInfo  contains the same data information shared by the  WideAndDeep  model and its feature generation part.  You can choose to include the following information for feature engineering and the  WideAndDeep  model:   wideBaseCols : Data of  wideBaseCols  together with  wideCrossCols  will be fed into the wide model.  wideBaseDims : Dimensions of  wideBaseCols . The dimensions of the data in  wideBaseCols  should be within the range of  wideBaseDims .  wideCrossCols : Data of  wideCrossCols  will be fed into the wide model.  wideCrossDims : Dimensions of  wideCrossCols . The dimensions of the data in  wideCrossCols  should be within the range of  wideCrossDims .  indicatorCols : Data of  indicatorCols  will be fed into the deep model as multi-hot vectors.   indicatorDims : Dimensions of  indicatorCols . The dimensions of the data in  indicatorCols  should be within the range of  indicatorDims .  embedCols : Data of  embedCols  will be fed into the deep model as embeddings.  embedInDims : Input dimension of the data in  embedCols . The dimensions of the data in  embedCols  should be within the range of  embedInDims .  embedOutDims : The dimensions of embeddings for  embedCols .  continuousCols : Data of  continuousCols  will be treated as continuous values for the deep model.  label : The name of the 'label' column. String. Default is \"label\".   Remark:  Fields that involve  Cols  should be an array of String (Scala) or a list of String (Python) indicating the name of the columns in the data.  Fields that involve  Dims  should be an array of integers (Scala) or a list of integers (Python) indicating the dimensions of the corresponding columns.  If any field is not specified, it will by default to be an empty array (Scala) or an empty list (Python).  Scala  ColumnFeatureInfo(\n    wideBaseCols = Array[String](),\n    wideBaseDims = Array[Int](),\n    wideCrossCols = Array[String](),\n    wideCrossDims = Array[Int](),\n    indicatorCols = Array[String](),\n    indicatorDims = Array[Int](),\n    embedCols = Array[String](),\n    embedInDims = Array[Int](),\n    embedOutDims = Array[Int](),\n    continuousCols = Array[String](),\n    label =  label )  Python  ColumnFeatureInfo(\n    wide_base_cols=None,\n    wide_base_dims=None,\n    wide_cross_cols=None,\n    wide_cross_dims=None,\n    indicator_cols=None,\n    indicator_dims=None,\n    embed_cols=None,\n    embed_in_dims=None,\n    embed_out_dims=None,\n    continuous_cols=None,\n    label= label )", 
            "title": "ColumnFeatureInfo"
        }, 
        {
            "location": "/powered-by/", 
            "text": "Powered By\n\n\n\n\n\n\nMicrosoft Azure\n: \nUse Analytics Zoo to Inject AI Into Customer Service Platforms on Microsoft Azure: Part 1\n \n\n\nAlibaba\n: \nDeploy Analytics Zoo in Aliyun EMR\n \n\n\nBaosight\n: \nLSTM-Based Time Series Anomaly Detection Using Analytics Zoo for Apache Spark* and BigDL at Baosight\n\n\nMidea\n: \nIndustrial Inspection Platform in Midea\n and KUKA\n: Using Distributed TensorFlow* on Analytics Zoo\n \n\n\nJob2Career\n:\nJob recommendations leveraging deep learning using Analytics Zoo on Apache Spark and BigDL", 
            "title": "Powered by"
        }, 
        {
            "location": "/powered-by/#powered-by", 
            "text": "Microsoft Azure :  Use Analytics Zoo to Inject AI Into Customer Service Platforms on Microsoft Azure: Part 1    Alibaba :  Deploy Analytics Zoo in Aliyun EMR    Baosight :  LSTM-Based Time Series Anomaly Detection Using Analytics Zoo for Apache Spark* and BigDL at Baosight  Midea :  Industrial Inspection Platform in Midea  and KUKA : Using Distributed TensorFlow* on Analytics Zoo    Job2Career : Job recommendations leveraging deep learning using Analytics Zoo on Apache Spark and BigDL", 
            "title": "Powered By"
        }, 
        {
            "location": "/known-issues/", 
            "text": "If you encounter the following exception when calling the Python API of Analytics Zoo when you are using Python 3.5 or 3.6:\n\n\n\n\nPy4JJavaError: An error occurred while calling z:org.apache.spark.bigdl.api.python.BigDLSerDe.loads.\n: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\n\n\n\nyou may need to check whether your input argument involves Numpy types (such as \nnumpy.int64\n). See \nhere\n for the related issue.\n\n\nFor example, invoking \nnp.min\n, \nnp.max\n, \nnp.unique\n, etc. will return type \nnumpy.int64\n. One way to solve this is to use \nint()\n to convert a number of type \nnumpy.int64\n to a Python int.", 
            "title": "FAQ and Known Issues"
        }
    ]
}