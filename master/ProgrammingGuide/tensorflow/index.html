<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>TFPark - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "TFPark API", url: "#tfpark-api", children: [
              {title: "TFDataset", url: "#tfdataset" },
              {title: "KerasModel", url: "#kerasmodel" },
              {title: "TFEstimator", url: "#tfestimator" },
          ]},
          {title: "Low level API", url: "#low-level-api", children: [
              {title: "Concepts", url: "#concepts" },
              {title: "Training", url: "#training" },
          ]},
          {title: "Relationship to TFNet", url: "#relationship-to-tfnet", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>TFPark</strong></h1>
    <hr>
    <p>Analytics-Zoo provides a set APIs for running TensorFlow model on Spark in a distributed fashion.</p>
<p><strong>Remarks</strong>:</p>
<ul>
<li>You need to install <strong>tensorflow==1.10</strong> on your driver node.</li>
<li>Your operating system (OS) is required to be one of the following 64-bit systems:
<strong>Ubuntu 16.04 or later</strong> and <strong>macOS 10.12.6 or later</strong>.</li>
<li>To run on other systems, you need to manually compile the TensorFlow source code. Instructions can
  be found <a href="https://github.com/tensorflow/tensorflow/tree/v1.10.0/tensorflow/java">here</a>.</li>
</ul>
<h1 id="tfpark-api">TFPark API</h1>
<p>TFPark is a set of high-level api modeling after tf.keras and tf.estimator to help user to train and evaluate TensorFlow
models on Spark and BigDL. Users can define their model using <code>tf.keras</code> API or using <code>model_fn</code> similar to <code>tf.estimator</code>.</p>
<h2 id="tfdataset">TFDataset</h2>
<p><strong>TFDatasets</strong> represents a distributed collection of elements (backed by a RDD) to be fed into a TensorFlow graph.
TFDatasets can be created from numpy.ndarrays, an rdd of numpy.ndarrays as well as ImageSet, TextSet and FeatureSet.
It acts as an interface connecting RDD data to TensorFlow models.</p>
<pre><code class="python">from zoo import init_nncontext
from zoo.pipeline.api.net import TFDataset
from tensorflow as tf

sc = init_nncontext()

# Each record in the train_rdd consists of a list of NumPy ndrrays
train_rdd = sc.parallelize(file_list)
  .map(lambda x: read_image_and_label(x))
  .map(lambda image_label: decode_to_ndarrays(image_label))

# TFDataset represents a distributed set of elements,
# in which each element contains one or more TensorFlow Tensor objects. 
dataset = TFDataset.from_rdd(train_rdd,
                             features=(tf.float32, [28, 28, 1]),
                             labels=(tf.int32, []),
                             batch_size=BATCH_SIZE)
</code></pre>

<p>More on TFDataset API <a href="../../APIGuide/TFPark/tf-dataset/">API Guide</a></p>
<h2 id="kerasmodel">KerasModel</h2>
<p>KerasModel enables user to use <code>tf.keras</code> API to define TensorFlow models and perform training or evaluation on top
of Spark and BigDL in a distributed fashion.</p>
<ol>
<li>Create a KerasModel</li>
</ol>
<pre><code class="python">from zoo.tfpark import KerasModel, TFDataset
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
     tf.keras.layers.Dense(64, activation='relu'),
     tf.keras.layers.Dense(10, activation='softmax'),
     ]
)

model.compile(optimizer=tf.keras.optimizers.RMSprop(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
keras_model = KerasModel(model)

</code></pre>

<ol>
<li>Perform training on TFDataset and save model</li>
</ol>
<pre><code class="python">keras_model.fit(training_dataset, epochs=max_epoch)

model.save_weights(&quot;/tmp/model.h5&quot;)

</code></pre>

<ol>
<li>Loading saved model and preform evaluation or inference</li>
</ol>
<pre><code class="python">model.load_weights(&quot;/tmp/model.h5&quot;)

evaluation_results = model.evaluate(eval_dataset)

predictions = model.predict(pred_dataset)
</code></pre>

<p>More on KerasModel API <a href="../../APIGuide/TFPark/model/">API Guide</a></p>
<h2 id="tfestimator">TFEstimator</h2>
<p>TFEstimator wraps a model defined by <code>model_fn</code>. The <code>model_fn</code> is almost identical to TensorFlow's <code>model_fn</code>
except users are required to return a <code>TFEstimator</code> object. Users do not need to construct backward graph
(calling <code>optimizer.minimize(...)</code>) but set a <code>loss</code> tensor in <code>TFEstimator</code>.</p>
<ol>
<li>Define a <code>model_fn</code></li>
</ol>
<pre><code class="python">import tensorflow as tf
from zoo.tfpark.estimator import TFEstimator, TFEstimatorSpec
def model_fn(features, labels, mode):

    hidden = tf.layers.dense(features, 32, activation=tf.nn.relu)

    logits = tf.layers.dense(hidden, 10)

    if mode == tf.estimator.ModeKeys.EVAL or mode == tf.estimator.ModeKeys.TRAIN:
        loss = tf.reduce_mean(
            tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))
        return TFEstimatorSpec(mode, predictions=logits, loss=loss)
    else:
        return TFEstimatorSpec(mode, predictions=logits)

</code></pre>

<ol>
<li>Define a input_fn</li>
</ol>
<pre><code class="python">import tensorflow as tf
sc = init_nncontext()
def input_fn(mode):
    if mode == tf.estimator.ModeKeys.TRAIN:
        training_rdd = get_data_rdd(&quot;train&quot;, sc)
        dataset = TFDataset.from_rdd(training_rdd,
                                     features=(tf.float32, [28, 28, 1]),
                                     labels=(tf.int32, []),
                                     batch_size=320)
    elif mode == tf.estimator.ModeKeys.EVAL:
        validation_rdd = get_data_rdd(&quot;validation&quot;, sc)
        dataset = TFDataset.from_rdd(testing_rdd,
                                     features=(tf.float32, [28, 28, 1]),
                                     labels=(tf.int32, []),
                                     batch_size=320)
    else:
        testing_rdd = get_data_rdd(&quot;test&quot;, sc)
        dataset = TFDataset.from_rdd(testing_rdd,
                                     features=(tf.float32, [28, 28, 1]),
                                     batch_per_thread=80)
    return dataset
</code></pre>

<ol>
<li>Create TFEstimator and perform training, evaluation or inference</li>
</ol>
<pre><code class="python">estimator = TFEstimator(model_fn, tf.train.AdamOptimizer(), model_dir=&quot;/tmp/estimator&quot;)
estimator.train(input_fn, steps=10000)
evaluation_result = estimator.evaluate(input_fn, [&quot;acc&quot;])
predictions = estimator.predict(input_fn)
</code></pre>

<p>More on TFEstimator API <a href="../../APIGuide/TFPark/estimator/">API Guide</a></p>
<h1 id="low-level-api">Low level API</h1>
<h2 id="concepts">Concepts</h2>
<ul>
<li>
<p><strong>TFOptimizer</strong> is the class that does all the hard work in distributed training, such as model
distribution and parameter synchronization. It takes the user specified <strong>loss</strong> (a TensorFlow scalar tensor) as
an argument and runs stochastic gradient descent using the given <strong>optimMethod</strong> on all the <strong>Variables</strong> that
contribute to this loss.</p>
</li>
<li>
<p><strong>TFPredictor</strong> takes a list of user specified TensorFlow tensors as the model outputs, and feed all the
elements in TFDatasets to produce those outputs; it returns a Spark RDD with each of its records representing the
model prediction for the corresponding input elements.</p>
</li>
</ul>
<h2 id="training">Training</h2>
<p>1.Data wrangling and analysis using PySpark</p>
<pre><code class="python">from zoo import init_nncontext
from zoo.pipeline.api.net import TFDataset
from tensorflow as tf

sc = init_nncontext()

# Each record in the train_rdd consists of a list of NumPy ndrrays
train_rdd = sc.parallelize(file_list)
  .map(lambda x: read_image_and_label(x))
  .map(lambda image_label: decode_to_ndarrays(image_label))

# TFDataset represents a distributed set of elements,
# in which each element contains one or more TensorFlow Tensor objects. 
dataset = TFDataset.from_rdd(train_rdd,
                             features=(tf.float32, [28, 28, 1]),
                             labels=(tf.int32, []),
                             batch_size=BATCH_SIZE)
</code></pre>

<p>2.Deep learning model development using TensorFlow</p>
<pre><code class="python">import tensorflow as tf

slim = tf.contrib.slim

images, labels = dataset.tensors
squeezed_labels = tf.squeeze(labels)
with slim.arg_scope(lenet.lenet_arg_scope()):
     logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)

loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=squeezed_labels))
</code></pre>

<p>You can also construct your model using Keras provided by Tensorflow.</p>
<pre><code class="python">from tensorflow.keras.models import Model
from tensorflow.keras.layers import *

data = Input(shape=[28, 28, 1])

x = Flatten()(data)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)

model = Model(inputs=data, outputs=predictions)

model.compile(optimizer='rmsprop',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'])
</code></pre>

<p>3.Distributed training on Spark and BigDL</p>
<pre><code class="python">from zoo.tfpark import TFOptimizer
from bigdl.optim.optimizer import MaxIteration, Adam, MaxEpoch, TrainSummary

optimizer = TFOptimizer.from_loss(loss, Adam(1e-3))
optimizer.set_train_summary(TrainSummary(&quot;/tmp/az_lenet&quot;, &quot;lenet&quot;))
optimizer.optimize(end_trigger=MaxEpoch(5))
</code></pre>

<p>For Keras model:</p>
<pre><code class="python">from zoo.tfpark import TFOptimizer
from bigdl.optim.optimizer import MaxIteration, MaxEpoch, TrainSummary

optimizer = TFOptimizer.from_keras(keras_model=model, dataset=dataset)
optimizer.set_train_summary(TrainSummary(&quot;/tmp/az_lenet&quot;, &quot;lenet&quot;))
optimizer.optimize(end_trigger=MaxEpoch(5))
</code></pre>

<p>4.Save the variable to checkpoint</p>
<pre><code class="python">saver = tf.train.Saver()
saver.save(optimizer.sess, &quot;/tmp/lenet/&quot;)
</code></pre>

<p>For Keras model, you can also Keras' <code>save_weights</code> api.</p>
<pre><code class="python">model.save_weights(&quot;/tmp/keras.h5&quot;)
</code></pre>

<h3 id="inference">Inference</h3>
<p>1.Data processing using PySpark</p>
<pre><code class="python">from zoo import init_nncontext
from zoo.pipeline.api.net import TFDataset
from tensorflow as tf

sc = init_nncontext()

# Each record in the train_rdd consists of a list of NumPy ndrrays
testing_rdd = sc.parallelize(file_list)
  .map(lambda x: read_image_and_label(x))
  .map(lambda image_label: decode_to_ndarrays(image_label))

# TFDataset represents a distributed set of elements,
# in which each element contains one or more TensorFlow Tensor objects. 
dataset = TFDataset.from_rdd(testing_rdd,
                             features=(tf.float32, [28, 28, 1]),
                             batch_per_thread=4)
</code></pre>

<p>2.Reconstruct the model for inference and load the checkpoint</p>
<pre><code class="python">import tensorflow as tf

slim = tf.contrib.slim

images, labels = dataset.tensors
with slim.arg_scope(lenet.lenet_arg_scope()):
     logits, end_points = lenet.lenet(images, num_classes=10, is_training=False)

sess = tf.Session()
saver = tf.train.Saver()
saver.restore(sess, &quot;/tmp/lenet&quot;)
</code></pre>

<p>As before, you can also construct and restore your model using Keras provided by Tensorflow.</p>
<pre><code class="python">from tensorflow.keras.models import Model
from tensorflow.keras.layers import *

data = Input(shape=[28, 28, 1])

x = Flatten()(data)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)

model = Model(inputs=data, outputs=predictions)

model.load_weights(&quot;/tmp/mnist_keras.h5&quot;)

</code></pre>

<p>3.Run predictions</p>
<pre><code class="python">predictor = TFPredictor.from_outputs(sess, [logits])
predictions_rdd = predictor.predict()
</code></pre>

<p>For keras model:</p>
<pre><code class="python">predictor = TFPredictor.from_keras(model, dataset)
predictions_rdd = predictor.predict()
</code></pre>

<h1 id="relationship-to-tfnet">Relationship to TFNet</h1>
<p><strong>TFNet</strong> is a layer representing a TensorFlow sub-graph (specified by the input and output TensorFlow tensors).
It implements the standard BigDL layer API, and can be used with other Analytics-Zoo/BigDL layers
to construct more complex models for training or inference using the standard Analytics-Zoo/BigDL API. </p>
<p>You can think of <code>TFDatasets</code>, <code>TFOptimizer</code>, <code>TFPredictor</code> as a set API for training/testing TensorFlow models
on Spark/BigDL, while <code>TFNet</code> as an Analytics-Zoo/BigDL layer initialized using TensorFlow graph.</p>
<p>For more information on TFNet, please refer to the <a href="../../APIGuide/PipelineAPI/net/##TFNet">API Guide</a></p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>