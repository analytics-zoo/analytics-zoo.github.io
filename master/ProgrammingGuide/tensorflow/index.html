<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Distributed Tensoflow on Spark/BigDL - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Concepts", url: "#concepts", children: [
          ]},
          {title: "Training", url: "#training", children: [
              {title: "Inference", url: "#inference" },
              {title: "Relationship to TFNet", url: "#relationship-to-tfnet" },
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Distributed Tensoflow on Spark/BigDL</strong></h1>
    <hr>
    <p>Analytics-Zoo provides a set APIs for running tensorflow model on Spark in a distributed fashion.</p>
<h2 id="concepts">Concepts</h2>
<ul>
<li>
<p><strong>TFDatasets</strong> represents a distributed collection of elements to be fed into a Tensorflow graph.
TFDatasets can be created directly from an RDD; each record in the RDD should be a list of numpy.ndarray
representing the input data. TFDatasets must be used with the TFOptimizer or TFPredictor (to be described next).</p>
</li>
<li>
<p><strong>TFOptimizer</strong> is the class that does all the hard work in distributed training, such as model
distribution and parameter synchronization. It takes the user specified <strong>loss</strong> (a TensorFlow scalar tensor) as
an argument and runs stochastic gradient descent using the given <strong>optimMethod</strong> on all the <strong>Variables</strong> that
contribute to this loss.</p>
</li>
<li>
<p><strong>TFPredictor</strong> takes a list of user specified TensorFlow tensors as the model outputs, and feed all the
elements in TFDatasets to produce those outputs; it returns a Spark RDD with each of its records representing the
model prediction for the corresponding input elements.</p>
</li>
</ul>
<h2 id="training">Training</h2>
<p>1.Data wrangling and analysis using PySpark</p>
<pre><code class="python">from zoo import init_nncontext
from zoo.pipeline.api.net import TFDataset
from tensorflow as tf

sc = init_nncontext()

# Each record in the train_rdd consists of a list of NumPy ndrrays
train_rdd = sc.parallelize(file_list)
  .map(lambda x: read_image_and_label(x))
  .map(lambda image_label: decode_to_ndarrays(image_label))

# TFDataset represents a distributed set of elements,
# in which each element contains one or more Tensorflow Tensor objects. 
dataset = TFDataset.from_rdd(train_rdd,
                             names=[&quot;features&quot;, &quot;labels&quot;],
                             shapes=[[28, 28, 1], [1]],
                             types=[tf.float32, tf.int32],
                             batch_size=BATCH_SIZE)
</code></pre>

<p>2.Deep learning model development using TensorFlow</p>
<pre><code class="python">import tensorflow as tf

slim = tf.contrib.slim

images, labels = dataset.tensors
squeezed_labels = tf.squeeze(labels)
with slim.arg_scope(lenet.lenet_arg_scope()):
     logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)

loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=squeezed_labels))
</code></pre>

<p>3.Distributed training on Spark and BigDL</p>
<pre><code class="python">from zoo.pipeline.api.net import TFOptimizer
from bigdl.optim.optimizer import MaxIteration, Adam, MaxEpoch, TrainSummary

optimizer = TFOptimizer(loss, Adam(1e-3))
optimizer.set_train_summary(TrainSummary(&quot;/tmp/az_lenet&quot;, &quot;lenet&quot;))
optimizer.optimize(end_trigger=MaxEpoch(5))
</code></pre>

<p>4.Save the variable to checkpoint</p>
<pre><code class="python">saver = tf.train.Saver()
saver.save(optimizer.sess, &quot;/tmp/lenet/&quot;)
</code></pre>

<h3 id="inference">Inference</h3>
<p>1.Data processing using PySpark</p>
<pre><code class="python">from zoo import init_nncontext
from zoo.pipeline.api.net import TFDataset
from tensorflow as tf

sc = init_nncontext()

# Each record in the train_rdd consists of a list of NumPy ndrrays
testing_rdd = sc.parallelize(file_list)
  .map(lambda x: read_image_and_label(x))
  .map(lambda image_label: decode_to_ndarrays(image_label))

# TFDataset represents a distributed set of elements,
# in which each element contains one or more Tensorflow Tensor objects. 
dataset = TFDataset.from_rdd(testing_rdd,
                             names=[&quot;features&quot;],
                             shapes=[[28, 28, 1]],
                             types=[tf.float32])
</code></pre>

<p>2.Reconstruct the model for inference and load the checkpoint</p>
<pre><code class="python">import tensorflow as tf

slim = tf.contrib.slim

images, labels = dataset.tensors
with slim.arg_scope(lenet.lenet_arg_scope()):
     logits, end_points = lenet.lenet(images, num_classes=10, is_training=False)

sess = tf.Session()
saver = tf.train.Saver()
saver.restore(sess, &quot;/tmp/lenet&quot;)
</code></pre>

<p>3.Run predictions</p>
<pre><code class="python">predictor = TFPredictor(sess, [logits])
predictions_rdd = predictor.predict()
</code></pre>

<h3 id="relationship-to-tfnet">Relationship to TFNet</h3>
<p><strong>TFNet</strong> is a layer representing a TensorFlow sub-graph (specified by the input and output TensorFlow tensors).
It implements the standard BigDL layer API, and can be used with other Analytics-Zoo/BigDL layers
to construct more complex models for training or inference using the standard Analytics-Zoo/BigDL API. </p>
<p>You can think of <code>TFDatasets</code>, <code>TFOptimizer</code>, <code>TFPredictor</code> as a set API for training/testing TensorFlow models
on Spark/BigDL, while <code>TFNet</code> as an Analytics-Zoo/BigDL layer initialized using TensorFlow graph.</p>
<p>For more information on TFNet, please refer to the <a href="../../APIGuide/PipelineAPI/net/#tfnet">API Guide</a></p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>