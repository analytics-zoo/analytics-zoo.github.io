<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Cluster Serving - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Programming Guide", url: "#programming-guide", children: [
              {title: "Quick Start", url: "#quick-start" },
              {title: "Workflow Overview", url: "#workflow-overview" },
              {title: "Deploy your Own Cluster Serving", url: "#deploy-your-own-cluster-serving" },
              {title: "Optional Operations", url: "#optional-operations" },
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Cluster Serving</strong></h1>
    <hr>
    <h1 id="programming-guide">Programming Guide</h1>
<p>Analytics Zoo Cluster Serving is a lightweight distributed, real-time serving solution that supports a wide range of deep learning models (such as TensorFlow, PyTorch, Caffe, BigDL and OpenVINO models). It provides a simple pub/sub API, so that the users can easily send their inference requests to the input queue (using a simple Python API); Cluster Serving will then automatically manage the scale-out and real-time model inference across a large cluster (using distributed streaming frameworks such as Apache Spark Streaming, Apache Flink, etc.) </p>
<p>The overall architecture of Analytics Zoo Cluster Serving solution is illustrated as below: </p>
<p><img alt="overview" src="../cluster_serving_overview.jpg" /></p>
<p>This page contains the guide for you to run Analytics Zoo Cluster Serving, including following:</p>
<ul>
<li>
<p><a href="#quick-start">Quick Start</a></p>
</li>
<li>
<p><a href="#workflow-overview">Workflow Overview</a> </p>
</li>
<li>
<p><a href="#deploy-your-own-cluster-serving">Deploy Your Own Cluster Serving</a></p>
</li>
<li>
<p><a href="#1-installation">Installation</a></p>
</li>
<li>
<p><a href="#2-configuration">Configuration</a> </p>
</li>
<li>
<p><a href="#3-launching-service">Launching Service</a></p>
</li>
<li>
<p><a href="#4-model-inference">Model inference</a></p>
</li>
<li>
<p><a href="#optional-operations">Optional Operations</a></p>
<ul>
<li>
<p><a href="#update-model-or-configurations">Update Model or Configurations</a></p>
</li>
<li>
<p><a href="#logs-and-visualization">Logs and Visualization</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="quick-start">Quick Start</h2>
<p>This section provides a quick start example for you to run Analytics Zoo Cluster Serving. To simplify the example, we use docker to run Cluster Serving. If you do not have docker installed, <a href="https://docs.docker.com/install/">install docker</a> first. The quick start example contains all the necessary components so the first time users can get it up and running within minutes:</p>
<ul>
<li>A docker image for Analytics Zoo Cluster Serving (with all dependencies installed)</li>
<li>A sample configuration file</li>
<li>A sample trained TensorFlow model, and sample data for inference</li>
<li>A sample Python client program</li>
</ul>
<p>Use one command to run Cluster Serving container. (We provide quick start model in older version of docker image, for newest version, please refer to following sections and we remove the model to reduce the docker image size).</p>
<pre><code>docker run -itd --name cluster-serving --net=host intelanalytics/zoo-cluster-serving:0.7.0
</code></pre>

<p>Log into the container using <code>docker exec -it cluster-serving bash</code>. </p>
<p>We already prepared <code>analytics-zoo</code> and <code>opencv-python</code> with pip in this container. And prepared model in <code>model</code> directory with following structure.</p>
<pre><code>cluster-serving | 
               -- | model
                 -- frozen_graph.pb
                 -- graph_meta.json
</code></pre>

<p>Start Cluster Serving using <code>cluster-serving-start</code>. </p>
<p>Run python program <code>python quick_start.py</code> to push data into queue and get inference result. </p>
<p>Then you can see the inference output in console. </p>
<pre><code>image: fish1.jpeg, classification-result:class: 5's prob: 0.18204997
image: dog1.jpeg, classification-result:class: 267's prob: 0.27166227
image: cat1.jpeg, classification-result:class: 292's prob: 0.32633427
</code></pre>

<p>Wow! You made it!</p>
<p>Note that the Cluster Serving quick start example will run on your local node only. Check the <a href="#deploy-your-own-cluster-serving">Deploy Your Own Cluster Serving</a> section for how to configure and run Cluster Serving in a distributed fashion.</p>
<p>For more details, you could also see the log and performance by go to <code>localhost:6006</code> in your browser and refer to <a href="#logs-and-visualization">Logs and Visualization</a>, or view the source code of <code>quick_start.py</code> <a href="https://github.com/intel-analytics/analytics-zoo/blob/master/pyzoo/zoo/serving/quick_start.py">here</a>, or refer to <a href="../APIGuide/">API Guide</a>.</p>
<h2 id="workflow-overview">Workflow Overview</h2>
<p>The figure below illustrates the simple 3-step "Prepare-Launch-Inference" workflow for Cluster Serving.</p>
<p><img alt="steps" src="../cluster_serving_steps.jpg" /></p>
<h4 id="1-install-and-prepare-cluster-serving-environment-on-a-local-node">1. Install and prepare Cluster Serving environment on a local node:</h4>
<ul>
<li>Copy a previously trained model to the local node; currently TensorFlow, PyTorch, Caffe, BigDL and OpenVINO models are supported.</li>
<li>Install Analytics Zoo on the local node (e.g., using a single pip install command)</li>
<li>Configure Cluster Server on the local node, including the file path to the trained model and the address of the cluster (such as Apache Hadoop YARN cluster, Spark Cluster, K8s cluster, etc.).
Please note that you only need to deploy the Cluster Serving solution on a single local node, and NO modifications are needed for the (YARN or K8s) cluster. </li>
</ul>
<h4 id="2-launch-the-cluster-serving-service">2. Launch the Cluster Serving service</h4>
<p>You can launch the Cluster Serving service by running the startup script on the local node. Under the hood, Cluster Serving will automatically deploy the trained model and serve the model inference requests across the cluster in a distributed fashion. You may monitor its runtime status (such as inference throughput) using TensorBoard. </p>
<h4 id="3-distributed-real-time-streaming-inference">3. Distributed, real-time (streaming) inference</h4>
<p>Cluster Serving provides a simple pub/sub API to the users, so that you can easily send the inference requests to an input queue (currently Redis Streams is used) using a simple Python API.</p>
<p>Cluster Serving will then read the requests from the Redis stream, run the distributed real-time inference across the cluster (using Spark Streaming or Flink), and return the results back through Redis. As a result, you may get the inference results again using a simple Python API.</p>
<h2 id="deploy-your-own-cluster-serving">Deploy your Own Cluster Serving</h2>
<h3 id="1-installation">1. Installation</h3>
<p>It is recommended to install Cluster Serving by pulling the pre-built Docker image to your local node, which have packaged all the required dependencies. Alternatively, you may also manually install Cluster Serving (through either pip or direct downloading) as well as Spark, Redis and TensorBoard (for visualizing the serving status) on the local node.</p>
<h4 id="docker">Docker</h4>
<pre><code>docker pull intelanalytics/zoo-cluster-serving
</code></pre>

<p>then, (or directly run <code>docker run</code>, it will pull the image if it does not exist)</p>
<pre><code>docker run --name cluster-serving --net=host -itd intelanalytics/zoo-cluster-serving:0.8.1 bash
</code></pre>

<p>Log into the container</p>
<pre><code>docker exec -it cluster-serving bash
</code></pre>

<p><code>cd ./cluster-serving</code>, you can see all the environments are prepared.</p>
<h5 id="yarn-user">Yarn user</h5>
<p>For Yarn user using docker, you have to set additional config, thus you need to call following when starting the container</p>
<pre><code>docker run --name cluster-serving --net=host -v /path/to/HADOOP_CONF_DIR:/opt/work/HADOOP_CONF_DIR -e HADOOP_CONF_DIR=/opt/work/HADOOP_CONF_DIR -itd intelanalytics/zoo-cluster-serving:0.8.1 bash
</code></pre>

<h4 id="manual-installation">Manual installation</h4>
<h5 id="requirements">Requirements</h5>
<p>Non-Docker users need to install <a href="https://archive.apache.org/dist/flink/flink-1.10.0/">Flink</a>, 1.10.0 by default, for users choose Spark as backend, install <a href="https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz">Spark</a>, 2.4.3 by default, <a href="https://redis.io/topics/quickstart">Redis</a>, 0.5.0 by default and <a href="https://www.tensorflow.org/tensorboard/get_started">TensorBoard</a> if you choose Spark backend and need visualization.</p>
<p>After preparing dependencies above, make sure the environment variable <code>$FLINK_HOME</code> (/path/to/flink-FLINK_VERSION-bin), or <code>$SPARK_HOME</code>(for Spark user only) (/path/to/spark-SPARK_VERSION-bin-hadoop-HADOOP_VERSION), <code>$REDIS_HOME</code>(/path/to/redis-REDIS_VERSION) is set before following steps. </p>
<p>For Spark user only, use <code>pip install tensorboard</code> to install TensorBoard.</p>
<h5 id="install-cluster-serving-by-download-release">Install Cluster Serving by download release</h5>
<p>For users who need to deploy and start Cluster Serving, download Cluster Serving zip from <a href="">here</a> and unzip it, then run <code>source cluster-serving-prepare.sh</code>. A demo <code>cluster-serving-demo.sh</code> is also prepared and users can run it to check if Cluster Serving could work.</p>
<p>For users who need to do inference, aka. predict data only, download Analytics Zoo python zip from <a href="">here</a> and run <code>export PYTHONPATH=$PYTHONPATH:/path/to/zip</code> to add this zip to <code>PYTHONPATH</code> environment variable.</p>
<h5 id="install-cluster-serving-by-pip">Install Cluster Serving by pip</h5>
<p>Download package from <a href="">here</a>, run following command to install Cluster Serving</p>
<pre><code>pip install *.whl
</code></pre>

<p>For users who need to deploy and start Cluster Serving, run <code>cluster-serving-init</code> to download and prepare dependencies.</p>
<p>For users who need to do inference, aka. predict data only, the environment is ready.</p>
<h3 id="2-configuration">2. Configuration</h3>
<h4 id="how-to-config">How to Config</h4>
<p>After <a href="#1-installation">Installation</a>, you will see a config file <code>config.yaml</code> in your current working directory. This file contains all the configurations that you can customize for your Cluster Serving. See an example of <code>config.yaml</code> below.</p>
<pre><code>## Analytics Zoo Cluster Serving Config Example

model:
  # model path must be set
  path: /opt/work/model
  # the inputs of the tensorflow model, separated by &quot;,&quot;
  inputs:
  # the outputs of the tensorflow model, separated by &quot;,&quot;
  outputs:
data:
  # default, localhost:6379
  src:
  # default, image (image &amp; tensor are supported)
  data_type: 
  # default, 3,224,224
  image_shape:
  # must be provided given data_type is tensor. eg: [1,2] (tensor) [[1],[2,1,2],[3]] (table)
  tensor_shape: 
  # default, topN(1)
  filter:
params:
  # default, 4
  batch_size:
  # default: OFF
  performance_mode:
spark:
  # default, local[*], change this to spark://, yarn, k8s:// etc if you want to run on cluster
  master: local[*]
  # default, 4g
  driver_memory:
  # default, 1g
  executor_memory:
  # default, 1
  num_executors:
  # default, 4
  executor_cores:
  # default, 4
  total_executor_cores:
</code></pre>

<h4 id="preparing-model">Preparing Model</h4>
<p>Currently Analytics Zoo Cluster Serving supports TensorFlow, OpenVINO, PyTorch, BigDL, Caffe models. Supported types are listed below.</p>
<p>You need to put your model file into a directory and the directory could have layout like following according to model type, note that only one model is allowed in your directory.</p>
<p><strong>Tensorflow</strong></p>
<p><strong><em>Tensorflow checkpoint</em></strong>
Please refer to <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/tensorflow/freeze_checkpoint">freeze checkpoint example</a></p>
<p><strong><em>Tensorflow frozen model</em></strong></p>
<pre><code>|-- model
   |-- frozen_graph.pb
   |-- graph_meta.json
</code></pre>

<p><strong><em>Tensorflow saved model</em></strong></p>
<pre><code>|-- model
   |-- saved_model.pb
   |-- variables
       |-- variables.data-00000-of-00001
       |-- variables.index
</code></pre>

<p>Note: you can specify model inputs and outputs in the config.yaml file. If the inputs or outputs are not provided, the signature "serving_default" will be used to find input and output tensors.</p>
<p><strong>Caffe</strong></p>
<pre><code>|-- model
   |-- xx.prototxt
   |-- xx.caffemodel
</code></pre>

<p><strong>Pytorch</strong></p>
<pre><code>|-- model
   |-- xx.pt
</code></pre>

<p><strong>BigDL</strong></p>
<pre><code>|--model
   |-- xx.model
</code></pre>

<p><strong>OpenVINO</strong></p>
<pre><code>|-- model
   |-- xx.xml
   |-- xx.bin
</code></pre>

<p>Put the model in any of your local directory, and set <code>model:/path/to/dir</code>.</p>
<h4 id="data-configuration">Data Configuration</h4>
<p>The field <code>data</code> contains your input data configuration.</p>
<ul>
<li>src: the queue you subscribe for your input data, e.g. default config of Redis on local machine is <code>localhost:6379</code>. Note that please use the host address in your network instead of localhost or 127.0.0.1 when you run serving in cluster, and make sure other nodes in cluster could also recognize this address.</li>
<li>shape: the shape of your input data. e.g. [[1],[3,224,224],[3]], if your model contains only one input, brackets could be omitted.</li>
<li>filter: the post-processing of pipeline, could be none. Except none, currently supported filters are,</li>
</ul>
<p>Top-N, e.g. <code>topN(1)</code> represents Top-1 result is kept and returned with index. User should follow this schema <code>topN(n)</code>. Noted if the top-N number is larger than model output size of the the final layer, it would just return all the outputs.</p>
<h4 id="other-configuration">Other Configuration</h4>
<p>The field <code>params</code> contains your inference parameter configuration.</p>
<ul>
<li>core_number: the batch size you use for model inference, usually the core number of your machine is recommended. Thus you could just provide your machine core number at this field. We recommend this value to be not smaller than 4 and not larger than 512. In general, using larger batch size means higher throughput, but also increase the latency between batches accordingly.</li>
<li>performance_mode: The performance mode will utilize your CPU resource to achieve better inference performance on a single node. <strong>Note:</strong> numactl and net-tools should be installed in your system, and spark master should be <code>local[*]</code> in the config.yaml file.</li>
</ul>
<p>For Spark users only, the field <code>spark</code> contains your spark configuration.</p>
<ul>
<li>master: Your cluster address, same as parameter <code>master</code> in spark</li>
<li>driver_memory: same as parameter <code>driver-memory</code> in spark</li>
<li>executor_memory: same as parameter <code>executor-memory</code> in spark</li>
<li>num_executors: same as parameter <code>num-executors</code> in spark</li>
<li>executor_cores: same as paramter <code>executor-cores</code> in spark</li>
<li>total_executor_cores: same as parameter <code>total-executor-cores</code> in spark</li>
</ul>
<p>For more details of these config, please refer to <a href="https://spark.apache.org/docs/latest/configuration.html">Spark Official Document</a></p>
<h3 id="3-launching-service">3. Launching Service</h3>
<h4 id="start-and-stop">Start and Stop</h4>
<p>We provide following scripts to start, stop, restart Cluster Serving. </p>
<h5 id="start">Start</h5>
<p>You can use following command to start Cluster Serving.</p>
<pre><code>cluster-serving-start
</code></pre>

<p>This command will start Redis and TensorBoard (for spark users only) if they are not running.</p>
<p>If you want to pass config <code>config.yaml</code> manually, run <code>cluster-serving-start -c config_path</code>
For spark users, if you choose spark streaming, run <code>cluster-serving-start --backend spark</code>.</p>
<h5 id="stop">Stop</h5>
<p>You can use following command to stop Cluster Serving. Data in Redis and TensorBoard service will persist.</p>
<pre><code>cluster-serving-stop
</code></pre>

<h5 id="restart">Restart</h5>
<p>You can use following command to restart Cluster Serving.</p>
<pre><code>cluster-serving-restart
</code></pre>

<h5 id="shut-down">Shut Down</h5>
<p>You can use following command to shutdown Cluster Serving. This operation will stop all running services related to Cluster Serving, specifically, Redis and TensorBoard. Note that your data in Redis will be removed when you shutdown. </p>
<pre><code>cluster-serving-shutdown
</code></pre>

<p>If you are using Docker, you could also run <code>docker rm</code> to shutdown Cluster Serving.</p>
<h4 id="http-server-for-sync-api-only">HTTP Server (for sync API only)</h4>
<p>If you want to use sync API for inference, you should start a provided HTTP server first. User can submit HTTP requests to the HTTP server through RESTful APIs. The HTTP server will parse the input requests and pub them to Redis input queues, then retrieve the output results and render them as json results in HTTP responses.</p>
<h5 id="prepare">Prepare</h5>
<p>User can download a analytics-zoo-${VERSION}-http.jar from the Nexus Repository with GAVP: </p>
<pre><code>&lt;groupId&gt;com.intel.analytics.zoo&lt;/groupId&gt;
&lt;artifactId&gt;analytics-zoo-bigdl_${BIGDL_VERSION}-spark_${SPARK_VERSION}&lt;/artifactId&gt;
&lt;version&gt;${ZOO_VERSION}&lt;/version&gt;
</code></pre>

<p>User can also build from the source code:</p>
<pre><code>mvn clean package -P spark_2.4+ -Dmaven.test.skip=true
</code></pre>

<h5 id="start-the-http-server">Start the HTTP Server</h5>
<p>User can start the HTTP server with following command.</p>
<pre><code>java -jar analytics-zoo-bigdl_${BIGDL_VERSION}-spark_${SPARK_VERSION}-${ZOO_VERSION}-http.jar
</code></pre>

<p>And check the status of the HTTP server with:</p>
<pre><code>curl  http://${BINDED_HOST_IP}:${BINDED_HOST_PORT}/
</code></pre>

<p>If you get a response like "welcome to analytics zoo web serving frontend", that means the HTTP server is started successfully.</p>
<h5 id="start-options">Start options</h5>
<p>User can pass options to the HTTP server when start it:</p>
<pre><code>java -jar analytics-zoo-bigdl_${BIGDL_VERSION}-spark_${SPARK_VERSION}-${ZOO_VERSION}-http.jar --redisHost=&quot;172.16.0.109&quot;
</code></pre>

<p>All the supported parameter are listed here:
* <strong>interface</strong>: the binded server interface, default is "0.0.0.0"
* <strong>port</strong>: the binded server port, default is 10020
* <strong>redisHost</strong>: the host IP of redis server, default is "localhost"
* <strong>redisPort</strong>: the host port of redis server, default is 6379
* <strong>redisInputQueue</strong>: the input queue of redis server, default is "serving_stream"
* <strong>redisOutputQueue</strong>: the output queue of redis server, default is "result:" 
* <strong>parallelism</strong>: the parallelism of requests processing, default is 1000
* <strong>timeWindow</strong>: the timeWindow wait to pub inputs to redis, default is 0
* <strong>countWindow</strong>: the timeWindow wait to ub inputs to redis, default is 56
* <strong>tokenBucketEnabled</strong>: the switch to enable/disable RateLimiter, default is false
* <strong>tokensPerSecond</strong>: the rate of permits per second, default is 100
* <strong>tokenAcquireTimeout</strong>: acquires a permit from this RateLimiter if it can be obtained without exceeding the specified timeout(ms), default is 100</p>
<p><strong>User can adjust these options to tune the performance of the HTTP server.</strong></p>
<h3 id="4-model-inference">4. Model Inference</h3>
<p>We support Python API and HTTP RESTful API for conducting inference with Data Pipeline in Cluster Serving. </p>
<h4 id="python-api">Python API</h4>
<p>For Python API, the requirements of python packages are <code>opencv-python</code>(for raw image only), <code>pyyaml</code>, <code>redis</code>. You can use <code>InputQueue</code> and <code>OutputQueue</code> to connect to data pipeline by providing the pipeline url, e.g. <code>my_input_queue = InputQueue(host, port)</code> and <code>my_output_queue = OutputQueue(host, port)</code>. If parameters are not provided, default url <code>localhost:6379</code> would be used.</p>
<p>We provide some basic usages here, for more details, please see <a href="../APIGuide/">API Guide</a>.</p>
<h5 id="async-api">Async API</h5>
<p>Async API provide method to enqueue data, the method would not block and user can query the data anytime afterwards.</p>
<p>To input data to queue, you need a <code>InputQueue</code> instance, and using <code>enqueue</code> method, for each input, give a key correspond to your model or give arbitrary key if your model does not care about it.</p>
<p>To enqueue an image</p>
<pre><code>from zoo.serving.client import InputQueue
input_api = InputQueue()
input_api.enqueue('my-image1', user_define_key={&quot;path: 'path/to/image1'})
</code></pre>

<p>To enqueue an instance containing 1 image and 2 ndarray</p>
<pre><code>from zoo.serving.client import InputQueue
import numpy as np
input_api = InputQueue()
t1 = np.array([1,2])
t2 = np.array([[1,2], [3,4]])
input_api.enqueue('my-instance', img={&quot;path&quot;: 'path/to/image'}, tensor1=t1, tensor2=t2)
</code></pre>

<p>There are 4 types of inputs in total, string, image, tensor, sparse tensor, which could represents nearly all types of models. For more details of usage, go to <a href="../APIGuide/">API Guide</a></p>
<p>To get data from queue, you need a <code>OutputQueue</code> instance, and using <code>query</code> or <code>dequeue</code> method. The <code>query</code> method takes image uri as parameter and returns the corresponding result. The <code>dequeue</code> method takes no parameter and just returns all results and also delete them in data queue. See following example.</p>
<pre><code>from zoo.serving.client import OutputQueue
output_api = OutputQueue()
img1_result = output_api.query('img1')
all_result = output_api.dequeue() # the output queue is empty after this code
</code></pre>

<p>Consider the code above,</p>
<pre><code>img1_result = output_api.query('img1')
</code></pre>

<p>The <code>img1_result</code> is a json format string, like following:</p>
<pre><code>'{&quot;class_1&quot;:&quot;prob_1&quot;,&quot;class_2&quot;:&quot;prob_2&quot;,...,&quot;class_n&quot;,&quot;prob_n&quot;}'
</code></pre>

<p>Where <code>n</code> is the number of <code>top_n</code> in your configuration file. This string could be parsed by <code>json.loads</code>.</p>
<pre><code>import json
result_class_prob_map = json.loads(img1_result)
</code></pre>

<h5 id="sync-api">Sync API</h5>
<p>Sync API provide method to predict data, the method would block until the result is available.</p>
<p>User need to create a <code>InputQueue</code> instance with <code>sync=True</code> and <code>frontend_url=frontend_server_url</code> argument.</p>
<pre><code>from zoo.serving.client import InputQueue
input_api = InputQueue(sync=True, frontend_url=frontend_server_url)
response = input_api.predict(request_json_string)
print(response.text)
</code></pre>

<p>example of <code>request_json_string</code> is</p>
<pre><code>
'{
  &quot;instances&quot; : [ {
    &quot;ids&quot; : [ 100.0, 88.0 ]
  }]
}'
</code></pre>

<p>This API is also a python support of <a href="#restful-api">Restful API</a> section, so for more details of input format, refer to it.</p>
<h4 id="restful-api">RESTful API</h4>
<p>This part describes API endpoints and end-to-end examples on usage. 
The requests and responses are in JSON format. The composition of them depends on the requests type or verb. See the APIs for details.
In case of error, all APIs will return a JSON object in the response body with error as key and the error message as the value:</p>
<pre><code>{
  &quot;error&quot;: &lt;error message string&gt;
}
</code></pre>

<h5 id="predict-api">Predict API</h5>
<p>URL</p>
<pre><code>POST http://host:port/predict
</code></pre>

<p>Request Example for images as inputs:</p>
<pre><code>curl -d \
'{
  &quot;instances&quot;: [
    {
      &quot;image&quot;: &quot;/9j/4AAQSkZJRgABAQEASABIAAD/7RcEUGhvdG9za...&quot;
    },
    {
      &quot;image&quot;: &quot;/9j/4AAQSkZJRgABAQEASABIAAD/7RcEUGhvdG9za...&quot;
    },
    {
      &quot;image&quot;: &quot;/9j/4AAQSkZJRgABAQEASABIAAD/7RcEUGhvdG9za...&quot;
    },
    {
      &quot;image&quot;: &quot;/9j/4AAQSkZJRgABAQEASABIAAD/7RcEUGhvdG9za...&quot;
    },
    {
      &quot;image&quot;: &quot;/9j/4AAQSkZJRgABAQEASABIAAD/7RcEUGhvdG9za...&quot;
    }
  ]
}' \
-X POST http://host:port/predict
</code></pre>

<p>Response Example</p>
<pre><code>{
  &quot;predictions&quot;: [
    &quot;{value=[[903,0.1306194]]}&quot;,
    &quot;{value=[[903,0.1306194]]}&quot;,
    &quot;{value=[[903,0.1306194]]}&quot;,
    &quot;{value=[[903,0.1306194]]}&quot;,
    &quot;{value=[[903,0.1306194]]}&quot;
  ]
}
</code></pre>

<p>Request Example for tensor as inputs:</p>
<pre><code>curl -d \
'{
  &quot;instances&quot; : [ {
    &quot;ids&quot; : [ 100.0, 88.0 ]
  }, {
    &quot;ids&quot; : [ 100.0, 88.0 ]
  }, {
    &quot;ids&quot; : [ 100.0, 88.0 ]
  }, {
    &quot;ids&quot; : [ 100.0, 88.0 ]
  }, {
    &quot;ids&quot; : [ 100.0, 88.0 ]
  } ]
}' \
-X POST http://host:port/predict
</code></pre>

<p>Response Example</p>
<pre><code>{
  &quot;predictions&quot;: [
    &quot;{value=[[1,0.6427843]]}&quot;,
    &quot;{value=[[1,0.6427843]]}&quot;,
    &quot;{value=[[1,0.6427843]]}&quot;,
    &quot;{value=[[1,0.6427843]]}&quot;,
    &quot;{value=[[1,0.6427842]]}&quot;
  ]
}
</code></pre>

<p>Another request example for composition of scalars and tensors.</p>
<pre><code>curl -d \
 '{
  &quot;instances&quot; : [ {
    &quot;intScalar&quot; : 12345,
    &quot;floatScalar&quot; : 3.14159,
    &quot;stringScalar&quot; : &quot;hello, world. hello, arrow.&quot;,
    &quot;intTensor&quot; : [ 7756, 9549, 1094, 9808, 4959, 3831, 3926, 6578, 1870, 1741 ],
    &quot;floatTensor&quot; : [ 0.6804766, 0.30136853, 0.17394465, 0.44770062, 0.20275897, 0.32762378, 0.45966738, 0.30405098, 0.62053126, 0.7037923 ],
    &quot;stringTensor&quot; : [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ],
    &quot;intTensor2&quot; : [ [ 1, 2 ], [ 3, 4 ], [ 5, 6 ] ],
    &quot;floatTensor2&quot; : [ [ [ 0.2, 0.3 ], [ 0.5, 0.6 ] ], [ [ 0.2, 0.3 ], [ 0.5, 0.6 ] ] ],
    &quot;stringTensor2&quot; : [ [ [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ], [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ] ], [ [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ], [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ] ] ]
  }, {
    &quot;intScalar&quot; : 12345,
    &quot;floatScalar&quot; : 3.14159,
    &quot;stringScalar&quot; : &quot;hello, world. hello, arrow.&quot;,
    &quot;intTensor&quot; : [ 7756, 9549, 1094, 9808, 4959, 3831, 3926, 6578, 1870, 1741 ],
    &quot;floatTensor&quot; : [ 0.6804766, 0.30136853, 0.17394465, 0.44770062, 0.20275897, 0.32762378, 0.45966738, 0.30405098, 0.62053126, 0.7037923 ],
    &quot;stringTensor&quot; : [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ],
    &quot;intTensor2&quot; : [ [ 1, 2 ], [ 3, 4 ], [ 5, 6 ] ],
    &quot;floatTensor2&quot; : [ [ [ 0.2, 0.3 ], [ 0.5, 0.6 ] ], [ [ 0.2, 0.3 ], [ 0.5, 0.6 ] ] ],
    &quot;stringTensor2&quot; : [ [ [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ], [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ] ], [ [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ], [ [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ], [ &quot;come&quot;, &quot;on&quot;, &quot;united&quot; ] ] ] ]
  } ]
}' \
-X POST http://host:port/predict
</code></pre>

<p>Another request example for composition of sparse and dense tensors.</p>
<pre><code>curl -d \
'{
  &quot;instances&quot; : [ {
    &quot;sparseTensor&quot; : {
      &quot;shape&quot; : [ 100, 10000, 10 ],
      &quot;data&quot; : [ 0.2, 0.5, 3.45, 6.78 ],
      &quot;indices&quot; : [ [ 1, 1, 1 ], [ 2, 2, 2 ], [ 3, 3, 3 ], [ 4, 4, 4 ] ]
    },
    &quot;intTensor2&quot; : [ [ 1, 2 ], [ 3, 4 ], [ 5, 6 ] ]
  }, {
    &quot;sparseTensor&quot; : {
      &quot;shape&quot; : [ 100, 10000, 10 ],
      &quot;data&quot; : [ 0.2, 0.5, 3.45, 6.78 ],
      &quot;indices&quot; : [ [ 1, 1, 1 ], [ 2, 2, 2 ], [ 3, 3, 3 ], [ 4, 4, 4 ] ]
    },
    &quot;intTensor2&quot; : [ [ 1, 2 ], [ 3, 4 ], [ 5, 6 ] ]
  } ]
}' \
-X POST http://host:port/predict
</code></pre>

<h5 id="metrics-api">Metrics API</h5>
<p>URL</p>
<pre><code>GET http://host:port/metrics
</code></pre>

<p>Response example:</p>
<pre><code>[
  {
    name: &quot;zoo.serving.redis.get&quot;,
    count: 810,
    meanRate: 12.627772820651845,
    min: 0,
    max: 25,
    mean: 0.9687099303718213,
    median: 0.928579,
    stdDev: 0.8150031623593447,
    _75thPercentile: 1.000047,
    _95thPercentile: 1.141443,
    _98thPercentile: 1.268665,
    _99thPercentile: 1.608387,
    _999thPercentile: 25.874584
  },
  {
    name: &quot;zoo.serving.redis.put&quot;,
    count: 192,
    meanRate: 2.9928448518681816,
    min: 4,
    max: 207,
    mean: 8.470988823179553,
    median: 6.909573,
    stdDev: 13.269285415774808,
    _75thPercentile: 8.262833,
    _95thPercentile: 14.828704,
    _98thPercentile: 18.860232,
    _99thPercentile: 19.825203,
    _999thPercentile: 207.541874
  },
  {
    name: &quot;zoo.serving.redis.wait&quot;,
    count: 192,
    meanRate: 2.992786169232195,
    min: 82,
    max: 773,
    mean: 93.03099107296806,
    median: 88.952799,
    stdDev: 45.54085374821418,
    _75thPercentile: 91.893393,
    _95thPercentile: 118.370628,
    _98thPercentile: 119.941905,
    _99thPercentile: 121.158649,
    _999thPercentile: 773.497556
  },
  {
    name: &quot;zoo.serving.request.metrics&quot;,
    count: 1,
    meanRate: 0.015586927261874562,
    min: 18,
    max: 18,
    mean: 18.232472,
    median: 18.232472,
    stdDev: 0,
    _75thPercentile: 18.232472,
    _95thPercentile: 18.232472,
    _98thPercentile: 18.232472,
    _99thPercentile: 18.232472,
    _999thPercentile: 18.232472
  },
  {
    name: &quot;zoo.serving.request.overall&quot;,
    count: 385,
    meanRate: 6.000929977336221,
    min: 18,
    max: 894,
    mean: 94.5795886310155,
    median: 89.946348,
    stdDev: 49.63620144068503,
    _75thPercentile: 93.851032,
    _95thPercentile: 121.148026,
    _98thPercentile: 123.118267,
    _99thPercentile: 124.053326,
    _999thPercentile: 894.004612
  },
  {
    name: &quot;zoo.serving.request.predict&quot;,
    count: 192,
    meanRate: 2.9925722215434205,
    min: 85,
    max: 894,
    mean: 96.63308151066575,
    median: 92.323305,
    stdDev: 53.17110030594844,
    _75thPercentile: 94.839714,
    _95thPercentile: 122.564496,
    _98thPercentile: 123.974892,
    _99thPercentile: 125.636335,
    _999thPercentile: 894.062819
  }
]
</code></pre>

<h2 id="optional-operations">Optional Operations</h2>
<h3 id="update-model-or-configurations">Update Model or Configurations</h3>
<p>To update your model, you could replace your model file in your model directory, and restart Cluster Serving by <code>cluster-serving-restart</code>. Note that you could also change your configurations in <code>config.yaml</code> and restart serving.</p>
<h3 id="logs-and-visualization">Logs and Visualization</h3>
<h4 id="logs">Logs</h4>
<p>We use log to save Cluster Serving information and error. To see log, please refer to <code>cluster-serving.log</code>.</p>
<h4 id="visualization">Visualization</h4>
<p>To visualize Cluster Serving performance, go to your flink job UI, default <code>localhost:8081</code>, and go to Cluster Serving job -&gt; metrics. Add <code>numRecordsOut</code> to see total record number and <code>numRecordsOutPerSecond</code> to see throughput.</p>
<p>See example of visualization:</p>
<p><img alt="Example Chart" src="../serving-visualization.png" /></p>
<h5 id="spark-streaming-visualization">Spark Streaming Visualization</h5>
<p>TensorBoard is integrated into Spark Streaming Cluster Serving. TensorBoard service is started with Cluster Serving. Once your serving starts, you can go to <code>localhost:6006</code> to see visualization of your serving.</p>
<p>Analytics Zoo Cluster Serving provides 2 attributes in Tensorboard so far, <code>Serving Throughput</code> and <code>Total Records Number</code>.</p>
<ul>
<li>
<p><code>Serving Throughput</code>: The overall throughput, including preprocessing and postprocessing of your serving. The line should be relatively stable after first few records. If this number has a drop and remains lower than previous, you might have lost the connection of some nodes in your cluster.</p>
</li>
<li>
<p><code>Total Records Number</code>: The total number of records that Cluster Serving gets so far.</p>
</li>
</ul>
<p>See example of visualization:</p>
<p><img alt="Example Chart" src="../example-chart.png" /></p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>