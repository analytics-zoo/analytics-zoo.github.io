<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Advanced Activations - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "PReLU", url: "#prelu", children: [
          ]},
          {title: "ELU", url: "#elu", children: [
          ]},
          {title: "SReLU", url: "#srelu", children: [
          ]},
          {title: "ThresholdedReLU", url: "#thresholdedrelu", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Advanced Activations</strong></h1>
    <hr>
    <h2 id="prelu"><strong>PReLU</strong></h2>
<p>Applies parametric ReLU, where parameter varies the slope of the negative part.</p>
<p>It follows: f(x) = max(0, x) + a * min(0, x)</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">PReLU(nOutputPlane = 0, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">PReLU(nOutputPlane=0, input_shape=None)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><code>nOutputPlane</code>: Input map number. Default is 0,
                  which means using PReLU in shared version and has only one parameter.</li>
<li><code>inputShape</code>:  A Single Shape, does not include the batch dimension.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.PReLU
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(PReLU[Float](inputShape = Shape(3)))
val input = Tensor[Float](2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.9026888      -1.0402212      1.3878769
-0.17167428     0.08202032      1.2682742
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
-0.2256722      -0.2600553      1.3878769
-0.04291857     0.08202032      1.2682742
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.models import Sequential
from zoo.pipeline.api.keras.layers import PReLU

model = Sequential()
model.add(PReLU(input_shape=(3, )))
input = np.random.random([2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[0.61639702 0.08877075 0.93652509]
 [0.38800821 0.76286851 0.95777973]]
</code></pre>

<p>Output is</p>
<pre><code class="python">[[0.616397   0.08877075 0.9365251 ]
 [0.3880082  0.7628685  0.9577797 ]]
</code></pre>

<hr />
<h2 id="elu"><strong>ELU</strong></h2>
<p>Exponential Linear Unit.</p>
<p>It follows: f(x) =  alpha * (exp(x) - 1.) for x &lt; 0, f(x) = x for x &gt;= 0.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">ELU(alpha = 1.0, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">ELU(alpha=1.0, input_shape=None, name=None)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><code>alpha</code>: Scale for the negative factor. Default is 1.0.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.ELU
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(ELU[Float](inputShape = Shape(3)))
val input = Tensor[Float](2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.13405465 0.05160992  -1.4711418
1.5808829   -1.3145303  0.6709266
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
-0.1254577  0.05160992  -0.77033687
1.5808829   -0.73139954 0.6709266
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import ELU
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(ELU(input_shape=(3, )))
input = np.random.random([2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[0.90404922 0.23530925 0.49711093]
 [0.43009161 0.22446032 0.90144771]]
</code></pre>

<p>Output is</p>
<pre><code class="python">[[0.9040492  0.23530924 0.49711093]
 [0.43009162 0.22446032 0.9014477 ]]
</code></pre>

<hr />
<h2 id="srelu"><strong>SReLU</strong></h2>
<p>S-shaped Rectified Linear Unit.</p>
<p>It follows: f(x) = t^r + a^r(x - t^r) for x &gt;= t^r, f(x) = x for t^r &gt; x &gt; t^l, f(x) = t^l + a^l(x - t^l) for x &lt;= t^l.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">SReLU(tLeftInit = &quot;zero&quot;, aLeftInit = &quot;glorot_uniform&quot;, tRightInit = &quot;glorot_uniform&quot;, aRightInit = &quot;one&quot;, sharedAxes = null, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">SReLU(t_left_init=&quot;zero&quot;, a_left_init=&quot;glorot_uniform&quot;, t_right_init=&quot;glorot_uniform&quot;, a_right_init=&quot;one&quot;, shared_axes=None, input_shape=None, name=None)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><code>tLeftInit</code>: String representation of the initialization method for the left part intercept. See <a href="../initialization/#available-initialization-methods">here</a> for available initialization strings. Default is 'zero'.</li>
<li><code>aLeftInit</code>: String representation of the initialization method for the left part slope. See <a href="../initialization/#available-initialization-methods">here</a> for available initialization strings. Default is 'glorot_uniform'.</li>
<li><code>tRightInit</code>: String representation of ithe nitialization method for the right part intercept. See <a href="../initialization/#available-initialization-methods">here</a> for available initialization strings. Default is 'glorot_uniform'.</li>
<li><code>aRightInit</code>: String representation of the initialization method for the right part slope. See <a href="../initialization/#available-initialization-methods">here</a> for available initialization strings. Default is 'one'.</li>
<li><code>sharedAxes</code>: The axes along which to share learnable parameters for the activation function. Default is null.
For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set 'sharedAxes = Array(1,2)'.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.SReLU
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(SReLU[Float](inputShape = Shape(2, 3)))
val input = Tensor[Float](2, 2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
0.5599429   0.22811626  -0.027771426
-0.56582874 1.9261217   1.2686813

(2,.,.) =
0.7538568   0.8725621   0.19803657
0.49057     0.0537252   0.8684544

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
(1,.,.) =
0.5599429   0.22811626  -0.009864618
0.07011698  1.9261217   1.2686813

(2,.,.) =
0.7538568   0.87256205  0.19803657
0.49057     0.0537252   0.8684544

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import SReLU
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(SReLU(input_shape=(2, 3)))
input = np.random.random([2, 2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[[0.42998132 0.47736492 0.9554154 ]
  [0.93264942 0.56851545 0.39508313]]

 [[0.5164102  0.22304862 0.44380779]
  [0.69137804 0.26413953 0.60638032]]]
</code></pre>

<p>Output is</p>
<pre><code class="python">[[[0.42998132 0.47736493 0.9554154 ]
  [0.93264943 0.5685154  0.39508313]]

 [[0.5164102  0.22304863 0.44380778]
  [0.69137806 0.26413953 0.60638034]]]
</code></pre>

<hr />
<h2 id="thresholdedrelu"><strong>ThresholdedReLU</strong></h2>
<p>Thresholded Rectified Linear Unit.</p>
<p>It follows: f(x) = x for x &gt; theta, f(x) = 0 otherwise.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">ThresholdedReLU(theta = 1.0, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">ThresholdedReLU(theta=1.0, input_shape=None, name=None)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><code>theta</code>: Threshold location of activation. Default is 1.0.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.ThresholdedReLU
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(ThresholdedReLU[Float](inputShape = Shape(3)))
val input = Tensor[Float](2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
2.220999    1.2022058   -1.0015608
0.6532913   0.31831574  1.6747104
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
2.220999    1.2022058   0.0
0.0         0.0         1.6747104
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import ThresholdedReLU
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(ThresholdedReLU(input_shape=(3, )))
input = np.random.random([2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[0.91854565 0.58317415 0.33089385]
 [0.82472184 0.70572913 0.32803604]]
</code></pre>

<p>Output is</p>
<pre><code class="python">[[0.0   0.0   0.0]
 [0.0   0.0   0.0]]
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>