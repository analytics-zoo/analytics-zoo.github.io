<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Recurrent Layers - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "SimpleRNN", url: "#simplernn", children: [
          ]},
          {title: "LSTM", url: "#lstm", children: [
          ]},
          {title: "GRU", url: "#gru", children: [
          ]},
          {title: "Highway", url: "#highway", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Recurrent Layers</strong></h1>
    <hr>
    <h2 id="simplernn"><strong>SimpleRNN</strong></h2>
<p>A fully-connected recurrent neural network cell. The output is to be fed back to input.</p>
<p>The input of this layer should be 3D, i.e. (batch, time steps, input dim).</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">SimpleRNN(outputDim, activation = &quot;tanh&quot;, returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">SimpleRNN(output_dim, activation=&quot;tanh&quot;, return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, name=None)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>outputDim</code>: Hidden unit size. Dimension of internal projections and final output.</li>
<li><code>activation</code>: String representation of the activation function to use. See <a href="../activation/#available-activations">here</a> for available activation strings. Default is 'tanh'.</li>
<li><code>returnSequences</code>: Whether to return the full sequence or only return the last output in the output sequence. Default is false.</li>
<li><code>goBackwards</code>: Whether the input sequence will be processed backwards. Default is false.</li>
<li><code>wRegularizer</code>: An instance of <a href="../../../APIGuide/Regularizers/">Regularizer</a>, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.</li>
<li><code>uRegularizer</code>: An instance of <a href="../../../APIGuide/Regularizers/">Regularizer</a>, applied the recurrent weights matrices. Default is null.</li>
<li><code>bRegularizer</code>: An instance of <a href="../../../APIGuide/Regularizers/">Regularizer</a>, applied to the bias. Default is null.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.SimpleRNN
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(SimpleRNN[Float](8, activation = &quot;relu&quot;, inputShape = Shape(4, 5)))
val input = Tensor[Float](2, 4, 5).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
0.71328646  0.24269831  -0.75013286 -1.6663225  0.35494477
0.073439054 -1.1181073  -0.6577777  1.3154761   0.15396282
0.41183218  -1.2667576  -0.11167632 0.946616    0.06427766
0.013886308 -0.20620999 1.1173447   1.9083043   1.7680032

(2,.,.) =
-2.3510098  -0.8492037  0.042268332 -0.43801674 -0.010638754
1.298793    -0.24814601 0.31325665  -0.19119295 -2.072075
-0.11629801 0.27296612  0.94443846  0.37293285  -0.82289046
0.6044998   0.93386084  -1.3502276  -1.7753356  1.6173482

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x4x5]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
0.0  0.020557694  0.0   0.39700085  0.622244  0.0   0.36524248  0.88961613
0.0  1.4797685    0.0   0.0         0.0       0.0   0.0         0.0
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import SimpleRNN
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(SimpleRNN(8, activation=&quot;relu&quot;, input_shape=(4, 5)))
input = np.random.random([2, 4, 5])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[[0.43400622 0.65452575 0.94952774 0.96210478 0.05286231]
  [0.2162183  0.33225502 0.09725628 0.80813221 0.29556109]
  [0.19720487 0.35077585 0.80904872 0.80576513 0.82035253]
  [0.36175687 0.63291153 0.08437936 0.71581099 0.790709  ]]

 [[0.35387003 0.36532078 0.9834315  0.07562338 0.05600369]
  [0.65927201 0.14652252 0.10848068 0.88225065 0.88871385]
  [0.23627135 0.72620104 0.60391828 0.51571874 0.73550574]
  [0.80773506 0.35121494 0.66889362 0.530684   0.52066982]]]
</code></pre>

<p>Output is:</p>
<pre><code class="python">[[0.77534926 0.23742369 0.14946866 0.0        0.16289112 0.0  0.71689016 0.24594748]
 [0.8987881  0.06123672 0.3312829  0.29757586 0.0        0.0  1.0179179  0.23447856]]
</code></pre>

<hr />
<h2 id="lstm"><strong>LSTM</strong></h2>
<p>Long Short Term Memory unit architecture.</p>
<p>The input of this layer should be 3D, i.e. (batch, time steps, input dim).</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">LSTM(outputDim, activation = &quot;tanh&quot;, innerActivation = &quot;hard_sigmoid&quot;, returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">LSTM(output_dim, activation=&quot;tanh&quot;, inner_activation=&quot;hard_sigmoid&quot;, return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, input_shape=None, name=None)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>outputDim</code>: Hidden unit size. Dimension of internal projections and final output.</li>
<li><code>activation</code>: String representation of the activation function to use. See <a href="../activation/#available-activations">here</a> for available activation strings. Default is 'tanh'.</li>
<li><code>innerActivation</code>: String representation of the activation function for inner cells. See <a href="../activation/#available-activations">here</a> for available activation strings. Default is 'hard_sigmoid'.</li>
<li><code>returnSequences</code>: Whether to return the full sequence or only return the last output in the output sequence. Default is false.</li>
<li><code>goBackwards</code>: Whether the input sequence will be processed backwards. Default is false.</li>
<li><code>wRegularizer</code>: An instance of <a href="../../../APIGuide/Regularizers/">Regularizer</a>, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.</li>
<li><code>uRegularizer</code>: An instance of <a href="../../../APIGuide/Regularizers/">Regularizer</a>, applied the recurrent weights matrices. Default is null.</li>
<li><code>bRegularizer</code>: An instance of <a href="../../../APIGuide/Regularizers/">Regularizer</a>, applied to the bias. Default is null.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
<li><code>name</code>: String to set the name of the layer. If not specified, its name will by default to be a generated string.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.LSTM
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(LSTM[Float](8, inputShape = Shape(2, 3)))
val input = Tensor[Float](2, 2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
0.6857518   0.21570909  -0.019308459
0.17754157  0.25172755  -1.189466

(2,.,.) =
0.23807438  1.6879119   -0.36335373
0.9826865   0.49549296  0.8100107

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
0.13552098  -0.043483295    -0.10553853 0.19386405  0.18295142  0.037892513 -0.05510225 -0.2420117
-0.04152686 -0.13908584 0.18151914  0.14170776  0.15598273  0.18968433  -0.042683482    -0.05782121
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import LSTM
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(LSTM(8, input_shape = (2, 3)))
input = np.random.random([2, 2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">array([[[ 0.67619723,  0.5168176 ,  0.8093504 ],
        [ 0.93787417,  0.53016934,  0.51934568]],

       [[ 0.57334472,  0.40007739,  0.65670337],
        [ 0.74457042,  0.15209156,  0.02015092]]])
</code></pre>

<p>Output is:</p>
<pre><code class="python">array([[-0.01563799,  0.16000053, -0.20192699,  0.08859081, -0.14184587,
         0.11160418,  0.19090165,  0.03475797],
       [-0.02395577,  0.10148412, -0.13211192,  0.05772379, -0.16488783,
         0.13513438,  0.15624164,  0.02866406]], dtype=float32)
</code></pre>

<hr />
<h2 id="gru"><strong>GRU</strong></h2>
<p>Gated Recurrent Unit architecture.</p>
<p>The input of this layer should be 3D, i.e. (batch, time steps, input dim).</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">GRU(outputDim, activation = &quot;tanh&quot;, innerActivation = &quot;hard_sigmoid&quot;, returnSequences = false, goBackwards = false, wRegularizer = null, uRegularizer = null, bRegularizer = null, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">GRU(output_dim, activation=&quot;tanh&quot;, inner_activation=&quot;hard_sigmoid&quot;, return_sequences=False, go_backwards=False, W_regularizer=None, U_regularizer=None, b_regularizer=None, input_shape=None, name=None)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>outputDim</code>: Hidden unit size. Dimension of internal projections and final output.</li>
<li><code>activation</code>: String representation of the activation function to use. See <a href="../activation/#available-activations">here</a> for available activation strings. Default is 'tanh'.</li>
<li><code>innerActivation</code>: String representation of the activation function for inner cells. See <a href="../activation/#available-activations">here</a> for available activation strings. Default is 'hard_sigmoid'.</li>
<li><code>returnSequences</code>: Whether to return the full sequence or only return the last output in the output sequence. Default is false.</li>
<li><code>goBackwards</code>: Whether the input sequence will be processed backwards. Default is false.</li>
<li><code>wRegularizer</code>: An instance of <a href="https://bigdl-project.github.io/master/#APIGuide/Regularizers/">Regularizer</a>, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.</li>
<li><code>uRegularizer</code>: An instance of <a href="https://bigdl-project.github.io/master/#APIGuide/Regularizers/">Regularizer</a>, applied the recurrent weights matrices. Default is null.</li>
<li><code>bRegularizer</code>: An instance of <a href="https://bigdl-project.github.io/master/#APIGuide/Regularizers/">Regularizer</a>, applied to the bias. Default is null.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.GRU
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(GRU[Float](8, inputShape = Shape(2, 3)))
val input = Tensor[Float](2, 2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
(1,.,.) =
-0.010477358 -1.1201298  -0.86472356
0.12688802   -0.6696582  0.08027417

(2,.,.) =
0.1724209    -0.52319324 -0.8808063
0.17918338   -0.552886   -0.11891741

[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
-0.12018716  -0.31560755    0.2867627   0.6728765   0.13287778  0.2112865   0.13381396  -0.4267934
-0.18521798  -0.30512968    0.14875418  0.63962734  0.1841841   0.25272882  0.016909363 -0.38463163
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x8]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import GRU
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(GRU(8, input_shape=(2, 3)))
input = np.random.random([2, 2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[[0.25026651 0.35433442 0.01417391]
  [0.77236921 0.97315472 0.66090386]]

 [[0.76037554 0.41029034 0.68725938]
  [0.17888889 0.67670088 0.70580547]]]
</code></pre>

<p>Output is:</p>
<pre><code class="python">[[-0.03584666  0.07984452 -0.06159414 -0.13331707  0.34015405 -0.07107028  0.12444386 -0.06606203]
 [ 0.02881907  0.04856917 -0.15306929 -0.24991018  0.23814955  0.0303434   0.06634206 -0.15335503]]
</code></pre>

<hr />
<h2 id="highway"><strong>Highway</strong></h2>
<p>Densely connected highway network.</p>
<p>Highway layers are a natural extension of LSTMs to feedforward networks.</p>
<p>The input of this layer should be 2D, i.e. (batch, input dim).</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">Highway(activation = null, wRegularizer = null, bRegularizer = null, bias = true, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">Highway(activation=None, W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, name=None)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>activation</code>: String representation of the activation function to use. See <a href="../activation/#available-activations">here</a> for available activation strings. Default is null.</li>
<li><code>wRegularizer</code>: An instance of <a href="https://bigdl-project.github.io/master/#APIGuide/Regularizers/">Regularizer</a>, (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.</li>
<li><code>bRegularizer</code>: An instance of <a href="https://bigdl-project.github.io/master/#APIGuide/Regularizers/">Regularizer</a>, applied to the bias. Default is null.</li>
<li><code>bias</code>: Whether to include a bias (i.e. make the layer affine rather than linear). Default is true.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <a href="../../keras-api-scala/#shape"><code>Shape</code></a> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.Highway
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val model = Sequential[Float]()
model.add(Highway[Float](inputShape = Shape(3)))
val input = Tensor[Float](2, 3).randn()
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Float] =
-0.26041138 0.4286919   1.723103
1.4516269   0.5557163   -0.1149741
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
-0.006746907    -0.109112576    1.3375516
0.6065166   0.41575465  -0.06849813
[com.intel.analytics.bigdl.tensor.DenseTensor of size 2x3]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import Highway
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(Highway(input_shape=(3, )))
input = np.random.random([2, 3])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">[[0.5762107  0.45679288 0.00370956]
 [0.24133312 0.38104653 0.05249192]]
</code></pre>

<p>Output is:</p>
<pre><code class="python">[[0.5762107  0.4567929  0.00370956]
 [0.24133313 0.38104653 0.05249191]]
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>