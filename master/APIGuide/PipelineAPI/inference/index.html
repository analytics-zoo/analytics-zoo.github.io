<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Inference - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Inference Model", url: "#inference-model", children: [
          ]},
          {title: "Overview", url: "#overview", children: [
          ]},
          {title: "Highlights", url: "#highlights", children: [
          ]},
          {title: "Primary APIs for Java", url: "#primary-apis-for-java", children: [
          ]},
          {title: "Examples", url: "#examples", children: [
          ]},
          {title: "Primary APIs for Scala", url: "#primary-apis-for-scala", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Inference</strong></h1>
    <hr>
    <h2 id="inference-model">Inference Model</h2>
<h2 id="overview">Overview</h2>
<p>Inference is a package in Analytics Zoo aiming to provide high level APIs to speed-up development. It 
allows user to conveniently use pre-trained models from Analytics Zoo, Caffe, Tensorflow and OpenVINO Intermediate Representation(IR).
Inference provides multiple Scala interfaces.</p>
<h2 id="highlights">Highlights</h2>
<ol>
<li>
<p>Easy-to-use APIs for loading and prediction with deep learning models of Analytics Zoo, Caffe, Tensorflow and OpenVINO Intermediate Representation(IR).</p>
</li>
<li>
<p>Support transformation of various input data type, thus supporting future prediction tasks.</p>
</li>
<li>
<p>Transparently support the OpenVINO toolkit, which deliver a significant boost for inference speed (<a href="https://software.intel.com/en-us/blogs/2018/05/15/accelerate-computer-vision-from-edge-to-cloud-with-openvino-toolkit">up to 19.9x</a>).</p>
</li>
</ol>
<h2 id="primary-apis-for-java">Primary APIs for Java</h2>
<p><strong>load</strong></p>
<p>AbstractInferenceModel provides <code>load</code> API for loading a pre-trained model,
thus we can conveniently load various kinds of pre-trained models in java applications. The load result of
<code>AbstractInferenceModel</code> is an <code>AbstractModel</code>.
We just need to specify the model path and optionally weight path if exists where we previously saved the model.</p>
<p><strong><em>load</em></strong></p>
<p><code>load</code> method is to load a BigDL model.</p>
<p><strong><em>loadCaffe</em></strong></p>
<p><code>loadCaffe</code> method is to load a caffe model.</p>
<p><strong><em>loadTF</em></strong></p>
<p><code>loadTF</code> method is to load a tensorflow model. There are two backends to load a tensorflow model and to do the predictions: TFNet and OpenVINO. For OpenVINO backend, <a href="https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">supported tensorflow models</a> are listed below:</p>
<pre><code>inception_v1
inception_v2
inception_v3
inception_v4
inception_resnet_v2
mobilenet_v1
nasnet_large
nasnet_mobile
resnet_v1_50
resnet_v2_50
resnet_v1_101
resnet_v2_101
resnet_v1_152
resnet_v2_152
vgg_16
vgg_19
faster_rcnn_inception_resnet_v2_atrous_coco
faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco
faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid
faster_rcnn_inception_resnet_v2_atrous_oid
faster_rcnn_nas_coco
faster_rcnn_nas_lowproposals_coco
faster_rcnn_resnet101_coco
faster_rcnn_resnet101_kitti
faster_rcnn_resnet101_lowproposals_coco
mask_rcnn_inception_resnet_v2_atrous_coco
mask_rcnn_inception_v2_coco
mask_rcnn_resnet101_atrous_coco
mask_rcnn_resnet50_atrous_coco
ssd_inception_v2_coco
ssd_mobilenet_v1_coco
ssd_mobilenet_v2_coco
ssdlite_mobilenet_v2_coco
</code></pre>
<p><strong><em>loadOpenVINO</em></strong></p>
<p><code>loadOpenVINO</code> method is to load an OpenVINO Intermediate Representation(IR).</p>
<p><strong><em>loadOpenVINOInt8</em></strong></p>
<p><code>loadOpenVINO</code> method is to load an OpenVINO Int8 Intermediate Representation(IR).</p>
<p><strong>predict</strong></p>
<p>AbstractInferenceModel provides <code>predict</code> API for prediction with loaded model.
The predict result of<code>AbstractInferenceModel</code> is a <code>List&lt;List&lt;JTensor&gt;&gt;</code> by default.</p>
<p><strong>predictInt8</strong></p>
<p>AbstractInferenceModel provides <code>predictInt8</code> API for prediction with loaded int8 model.
The predictInt8 result of<code>AbstractInferenceModel</code> is a <code>List&lt;List&lt;JTensor&gt;&gt;</code> by default.</p>
<h2 id="examples">Examples</h2>
<p>It's very easy to apply abstract inference model for inference with below code piece.
You will need to write a subclass that extends AbstractinferenceModel.</p>
<pre><code class="java">import com.intel.analytics.zoo.pipeline.inference.AbstractInferenceModel;
import com.intel.analytics.zoo.pipeline.inference.JTensor;

public class TextClassificationModel extends AbstractInferenceModel {
    public TextClassificationModel() {
        super();
    }
 }
TextClassificationModel model = new TextClassificationModel();
model.load(modelPath, weightPath);
List&lt;List&lt;JTensor&gt;&gt; result = model.predict(inputList);
</code></pre>

<h2 id="primary-apis-for-scala">Primary APIs for Scala</h2>
<p><strong>InferenceModel</strong></p>
<p><code>InferenceModel</code> is a thead-safe wrapper of AbstractModels, which can be used to load models and do the predictions.</p>
<p><strong><em>doLoad</em></strong></p>
<p><code>doLoad</code> method is to load a bigdl, analytics-zoo model.</p>
<p><strong><em>doLoadCaffe</em></strong></p>
<p><code>doLoadCaffe</code> method is to load a caffe model.</p>
<p><strong><em>doLoadTF</em></strong></p>
<p><code>doLoadTF</code> method is to load a tensorflow model. The model can be loaded as a <code>FloatModel</code> or an <code>OpenVINOModel</code>. There are two backends to load a tensorflow model: TFNet and OpenVINO. </p>
<p><span id="jump">For OpenVINO backend, <a href="https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html">supported tensorflow models</a> are listed below:</span></p>
<pre><code>inception_v1
inception_v2
inception_v3
inception_v4
inception_resnet_v2
mobilenet_v1
nasnet_large
nasnet_mobile
resnet_v1_50
resnet_v2_50
resnet_v1_101
resnet_v2_101
resnet_v1_152
resnet_v2_152
vgg_16
vgg_19
faster_rcnn_inception_resnet_v2_atrous_coco
faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco
faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid
faster_rcnn_inception_resnet_v2_atrous_oid
faster_rcnn_nas_coco
faster_rcnn_nas_lowproposals_coco
faster_rcnn_resnet101_coco
faster_rcnn_resnet101_kitti
faster_rcnn_resnet101_lowproposals_coco
mask_rcnn_inception_resnet_v2_atrous_coco
mask_rcnn_inception_v2_coco
mask_rcnn_resnet101_atrous_coco
mask_rcnn_resnet50_atrous_coco
ssd_inception_v2_coco
ssd_mobilenet_v1_coco
ssd_mobilenet_v2_coco
ssdlite_mobilenet_v2_coco
</code></pre>
<p><strong><em>doLoadOpenVINO</em></strong></p>
<p><code>doLoadOpenVINO</code> method is to load an OpenVINO Intermediate Representation(IR).</p>
<p><strong><em>doLoadOpenVINOInt8</em></strong></p>
<p><code>doLoadOpenVINOInt8</code> method is to load an OpenVINO Int8 Intermediate Representation(IR).</p>
<p><strong><em>doReload</em></strong></p>
<p><code>doReload</code> method is to reload the bigdl, analytics-zoo model.</p>
<p><strong><em>doPredict</em></strong></p>
<p><code>doPredict</code> method is to do the prediction.</p>
<p><strong><em>doPredictInt8</em></strong></p>
<p><code>doPredict</code> method is to do the prediction with Int8 model. If model doesn't support predictInt8, will throw RuntimeException with <code>does not support predictInt8</code> message.</p>
<p><strong>InferenceSupportive</strong></p>
<p><code>InferenceSupportive</code> is a trait containing several methods for type transformation, which transfer a model input 
to a valid data type, thus supporting future inference model prediction tasks.</p>
<p>For example, method <code>transferTensorToJTensor</code> convert a model input of data type <code>Tensor</code> 
to <a href="https://github.com/intel-analytics/analytics-zoo/blob/88afc2d921bb50341d8d7e02d380fa28f49d246b/zoo/src/main/java/com/intel/analytics/zoo/pipeline/inference/JTensor.java"><code>JTensor</code></a>
, which will be the input for a FloatInferenceModel.</p>
<p><strong>AbstractModel</strong></p>
<p><code>AbstractModel</code> is an abstract class to provide APIs for basic functions - <code>predict</code> interface for prediction, <code>copy</code> interface for coping the model into the queue of AbstractModels, <code>release</code> interface for releasing the model and <code>isReleased</code> interface for checking the state of model release.  </p>
<p><strong>FloatModel</strong></p>
<p><code>FloatModel</code> is an extending class of <code>AbstractModel</code> and achieves all <code>AbstractModel</code> interfaces. </p>
<p><strong>OpenVINOModel</strong></p>
<p><code>OpenVINOModel</code> is an extending class of <code>AbstractModel</code>. It achieves all <code>AbstractModel</code> functions.</p>
<p><strong>InferenceModelFactory</strong></p>
<p><code>InferenceModelFactory</code> is an object with APIs for loading pre-trained Analytics Zoo models, Caffe models, Tensorflow models and OpenVINO Intermediate Representations(IR).
Analytics Zoo models, Caffe models, Tensorflow models can be loaded as FloatModels. The load result of it is a <code>FloatModel</code>
Tensorflow models and OpenVINO Intermediate Representations(IR) can be loaded as OpenVINOModels. The load result of it is an <code>OpenVINOModel</code>. 
The load result of it is a <code>FloatModel</code> or an <code>OpenVINOModel</code>. </p>
<p><strong>OpenVinoInferenceSupportive</strong></p>
<p><code>OpenVinoInferenceSupportive</code> is an extending object of <code>InferenceSupportive</code> and focus on the implementation of loading pre-trained models, including tensorflow models and OpenVINO Intermediate Representations(IR). 
There are two backends to load a tensorflow model: TFNet and OpenVINO. For OpenVINO backend, <a href="#jump">supported tensorflow models</a> are listed in the section of <code>doLoadTF</code> method of <code>InferenceModel</code> API above. </p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>