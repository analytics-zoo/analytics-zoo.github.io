<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Inference - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Load pre-trained model", url: "#load-pre-trained-model", children: [
              {title: "Load pre-trained Analytics Zoo model", url: "#load-pre-trained-analytics-zoo-model" },
              {title: "Load pre-trained Caffe model", url: "#load-pre-trained-caffe-model" },
              {title: "Load pre-trained TensorFlow model", url: "#load-pre-trained-tensorflow-model" },
              {title: "Load OpenVINO model", url: "#load-openvino-model" },
          ]},
          {title: "Predict with loaded model", url: "#predict-with-loaded-model", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Inference</strong></h1>
    <hr>
    <p>Inference Model is a package in Analytics Zoo aiming to provide high-level APIs to speed-up development. It allows user to conveniently use pre-trained models from Analytics Zoo, Caffe, Tensorflow and OpenVINO Intermediate Representation(IR). Inference Model provides Java, Scala and Python interfaces.</p>
<p><strong>Highlights</strong></p>
<ol>
<li>Easy-to-use APIs for loading and prediction with deep learning models of Analytics Zoo, Caffe, Tensorflow and OpenVINO Intermediate Representation(IR).</li>
<li>Support transformation of various input data type, thus supporting future prediction tasks.</li>
<li>Transparently support the OpenVINO toolkit, which deliver a significant boost for inference speed (<a href="https://software.intel.com/en-us/blogs/2018/05/15/accelerate-computer-vision-from-edge-to-cloud-with-openvino-toolkit">up to 19.9x</a>).</li>
</ol>
<p><strong>Basic usage of Inference Model:</strong></p>
<ol>
<li>Directly use InferenceModel or write a subclass extends <code>InferenceModel</code> (<code>AbstractInferenceModel</code> in Java).</li>
<li>Load pre-trained models with corresponding <code>load</code> methods, e.g, <code>doLoadBigDL</code> for Analytics Zoo, and <code>doLoadTensorflow</code> for TensorFlow.</li>
<li>Do prediction with <code>predict</code> method.</li>
</ol>
<p><strong>OpenVINO requirements:</strong></p>
<p><a href="https://software.intel.com/en-us/openvino-toolkit/documentation/system-requirements">System requirements</a>:</p>
<pre><code>Ubuntu 16.04.3 LTS (64 bit)
Windows 10 (64 bit)
CentOS 7.4 (64 bit)
macOS 10.13, 10.14 (64 bit)
</code></pre>
<p>Python requirements:</p>
<pre><code>tensorflow&gt;=1.2.0
networkx&gt;=1.11
numpy&gt;=1.12.0
protobuf==3.6.1
</code></pre>
<p><strong>Supported models:</strong></p>
<ol>
<li><a href="https://analytics-zoo.github.io/master/##built-in-deep-learning-models">Analytics Zoo Models</a></li>
<li><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">Caffe Models</a></li>
<li><a href="https://github.com/tensorflow/models">TensorFlow Models</a></li>
<li><a href="https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models">OpenVINO models</a></li>
</ol>
<h2 id="load-pre-trained-model"><strong>Load pre-trained model</strong></h2>
<h3 id="load-pre-trained-analytics-zoo-model"><strong>Load pre-trained Analytics Zoo model</strong></h3>
<p>Load Analytics Zoo model with corresponding <code>load</code> methods (<code>load</code> for Java and Python, <code>doLoad</code> for Scala).</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadBigDL(modelPath, weightPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadBigDL(modelPath, weightPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_bigdl(modelPath, weightPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
<li><code>weightPath</code>: String. Path of pre-trained model weight. Default is <code>null</code>.</li>
</ul>
<h3 id="load-pre-trained-caffe-model"><strong>Load pre-trained Caffe model</strong></h3>
<p>Load Caffe model with <code>loadCaffe</code> methods (<code>loadCaffe</code> for Java, <code>doLoadCaffe</code> for Scala and <code>load_caffe</code> Python).</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadCaffe(modelPath, weightPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadCaffe(modelPath, weightPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_caffe(modelPath, weightPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
<li><code>weightPath</code>: String. Path of pre-trained model weight.</li>
</ul>
<h3 id="load-pre-trained-tensorflow-model"><strong>Load pre-trained TensorFlow model</strong></h3>
<p>Load model into <code>TFNet</code> with corresponding <code>loadTensorflow</code> methods (<code>loadTensorflow</code> for Java, <code>doLoadTensorflow</code> for Scala and <code>load_tensorflow</code> for Python)</p>
<p>We provide <code>loadTensorflow</code> with the following parameters:</p>
<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
<li><code>modelType</code>: String. Type of pre-trained model file.</li>
<li><code>Inputs</code>: Array[String]. The inputs of the model.</li>
<li><code>Outputs</code>: Array[String]. The outputs of the model.</li>
<li><code>intraOpParallelismThreads</code>: Int. The number of intraOpParallelismThreads.</li>
<li><code>interOpParallelismThreads</code>: Int. The number of interOpParallelismThreads.</li>
<li><code>usePerSessionThreads</code>: Boolean. Whether to perSessionThreads</li>
</ul>
<p>Note that we prepare several implementations with less parameters based on this method, e.g., <code>loadTensorflow(modelPath, modelType)</code> for frozenModel.</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadTensorflow(modelPath, modelType);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadTensorflow(modelPath, modelType)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_tensorflow(modelPath, modelType)
</code></pre>

<h3 id="load-openvino-model"><strong>Load OpenVINO model</strong></h3>
<p>Load OpenVINO model with <code>loadOpenVINO</code> methods (<code>loadOpenVINO</code> for Java, <code>doLoadOpenVINO</code> for Scala and <code>load_openvino</code> Python).</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadOpenVINO(modelPath, weightPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadOpenVINO(modelPath, weightPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_openvino(modelPath, weightPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained OpenVINO model.</li>
<li><code>weightPath</code>: String. Path of pre-trained OpenVINO model weight.</li>
</ul>
<h2 id="predict-with-loaded-model"><strong>Predict with loaded model</strong></h2>
<p>After loading pre-trained models with load methods, we can make prediction with unified <code>predict</code> method.</p>
<ul>
<li><code>predictInput</code>: JList[JList[JTensor]] or <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/tensor">Tensor</a> for Scale and Java, Numpy for Python. Input data for prediction. <a href="https://github.com/intel-analytics/analytics-zoo/blob/master/zoo/src/main/java/com/intel/analytics/zoo/pipeline/inference/JTensor.java">JTensor</a> is a 1D List, with Array[Int] shape.</li>
<li><code>predictOutput</code>: JList[JList[JTensor]] or <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/tensor">Tensor</a> for Scale and Java, Numpy for Python. Prediction result.</li>
</ul>
<p>Do prediction with <code>predict</code> methods (<code>predict</code> for Java and Python, <code>doPredict</code> for Scala).</p>
<p><strong>Java</strong></p>
<pre><code class="java">List&lt;List&lt;JTensor&gt;&gt; predictOutput = model.predict(predictInput);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val predictOutput = model.doPredict(predictInput)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">predict_output = model.predict(predict_input)
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>