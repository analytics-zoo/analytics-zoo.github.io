<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Inference - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Load pre-trained model", url: "#load-pre-trained-model", children: [
              {title: "Load pre-trained Analytics Zoo model", url: "#load-pre-trained-analytics-zoo-model" },
              {title: "Load pre-trained Caffe model", url: "#load-pre-trained-caffe-model" },
              {title: "Load TensorFlow model", url: "#load-tensorflow-model" },
              {title: "Load OpenVINO model", url: "#load-openvino-model" },
          ]},
          {title: "Predict with loaded model", url: "#predict-with-loaded-model", children: [
              {title: "predict", url: "#predict" },
              {title: "predictInt8 and loadTFAsCalibratedOpenVINO", url: "#predictint8-and-loadtfascalibratedopenvino" },
          ]},
          {title: "Supportive classes", url: "#supportive-classes", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Inference</strong></h1>
    <hr>
    <p>Inference Model is a package in Analytics Zoo aiming to provide high-level APIs to speed-up development. It allows user to conveniently use pre-trained models from Analytics Zoo, Caffe, Tensorflow and OpenVINO Intermediate Representation(IR). Inference Model provides Java, Scala and Python interfaces.</p>
<p><strong>Highlights</strong></p>
<ol>
<li>Easy-to-use APIs for loading and prediction with deep learning models of Analytics Zoo, Caffe, Tensorflow and OpenVINO Intermediate Representation(IR).</li>
<li>Support transformation of various input data type, thus supporting future prediction tasks.</li>
<li>Transparently support the OpenVINO toolkit, which deliver a significant boost for inference speed (<a href="https://software.intel.com/en-us/blogs/2018/05/15/accelerate-computer-vision-from-edge-to-cloud-with-openvino-toolkit">up to 19.9x</a>).</li>
</ol>
<p><strong>Basic usage of Inference Model:</strong></p>
<ol>
<li>Directly use InferenceModel or write a subclass extends <code>InferenceModel</code> (<code>AbstractInferenceModel</code> in Java).</li>
<li>Load pre-trained models with corresponding <code>load</code> methods, e.g, doLoad for Analytics Zoo, and doLoadTF for TensorFlow.</li>
<li>Do prediction with <code>predict</code> method.</li>
</ol>
<p><strong>OpenVINO requirements:</strong></p>
<p><a href="https://software.intel.com/en-us/openvino-toolkit/documentation/system-requirements">System requirements</a>:</p>
<pre><code>Ubuntu 16.04.3 LTS (64 bit)
Windows 10 (64 bit)
CentOS 7.4 (64 bit)
macOS 10.13, 10.14 (64 bit)
</code></pre>
<p>Python requirements:</p>
<pre><code>tensorflow&gt;=1.2.0
networkx&gt;=1.11
numpy&gt;=1.12.0
protobuf==3.6.1
</code></pre>
<p><strong>Supported models:</strong></p>
<ol>
<li><a href="https://analytics-zoo.github.io/master/##built-in-deep-learning-models">Analytics Zoo Models</a></li>
<li><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">Caffe Models</a></li>
<li><a href="https://github.com/tensorflow/models">TensorFlow Models</a></li>
<li><a href="https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models">OpenVINO models</a></li>
</ol>
<h2 id="load-pre-trained-model"><strong>Load pre-trained model</strong></h2>
<h3 id="load-pre-trained-analytics-zoo-model"><strong>Load pre-trained Analytics Zoo model</strong></h3>
<p>Load Analytics Zoo model with corresponding <code>load</code> methods (<code>load</code> for Java and Python, <code>doLoad</code> for Scala).</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.load(modelPath, weightPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoad(modelPath, weightPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load(modelPath, weightPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
<li><code>weightPath</code>: String. Path of pre-trained model weight. Default is <code>null</code>.</li>
</ul>
<h3 id="load-pre-trained-caffe-model"><strong>Load pre-trained Caffe model</strong></h3>
<p>Load Caffe model with <code>loadCaffe</code> methods (<code>loadCaffe</code> for Java, <code>doLoadCaffe</code> for Scala and <code>load_caffe</code> Python).</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadCaffe(modelPath, weightPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadCaffe(modelPath, weightPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_caffe(modelPath, weightPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
<li><code>weightPath</code>: String. Path of pre-trained model weight.</li>
</ul>
<h3 id="load-tensorflow-model"><strong>Load TensorFlow model</strong></h3>
<p>There are two backends to load a tensorflow model: TensorFlow and OpenVINO. When using TensorFlow as backend, tensorflow model will be loaded into <code>FloatModel</code>. Otherwise, it will be coverted into OpenVINO model, and loaded into <code>OpenVINOModel</code>.</p>
<p><strong>1. Load with TensorFlow backend</strong></p>
<p>Load model into <code>FloatModel</code> with TensorFlow backend, with corresponding <code>loadTF</code> methods (<code>loadTF</code> for Java, <code>doLoadTF</code> for Scala and <code>load_tf</code> Python)</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadTF(modelPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadTF(modelPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_tf(modelPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
</ul>
<p><strong>2. Load with OpenVINO backend</strong></p>
<p>Load model into <code>OpenVINOModel</code> with OpenVINO backend, with corresponding <code>loadTF</code> methods (<code>loadTF</code> for Java, <code>doLoadTF</code> for Scala and <code>load_tf</code> Python). Note that OpenVINO cannot directly load TensorFlow models. We need to <a href="../(https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html)">covert TensorFlow models into OpenVINO models</a>, then load models into OpenVINO.</p>
<p>Herein Analytics Zoo, we merge these two steps into one, and provide <code>loadOpenVINOModelForTF</code> with the following parameters:</p>
<ul>
<li><code>modelPath</code>: String. Path of pre-trained tensorflow model.</li>
<li><code>modelType</code>: String. Type the type of the tensorflow model.</li>
<li><code>checkpointPath</code>: String. Path of the tensorflow checkpoint file</li>
<li><code>inputShape</code>: Array[Int]. Input shape that should be fed to an input node(s) of the model</li>
<li><code>ifReverseInputChannels</code>: Boolean. If need reverse input channels. switch the input channels order from RGB to BGR (or vice versa).</li>
<li><code>meanValues</code>: Array[Int]. All input values coming from original network inputs will be divided by this value.</li>
<li><code>scale</code>: Float. Scale value, to be used for the input image per channel.</li>
<li><code>outputDir</code>: String. Path of pre-trained tensorflow model.</li>
</ul>
<p>Note that we prepare several implementations with less parameters based on this method, e.g., <code>loadTF(modelPath, modelType)</code>.</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadTF(modelPath, modelType);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadTF(modelPath, modelType)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_tf(modelPath, modelType)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained model.</li>
<li><code>weightPath</code>: String. Path of pre-trained model weight.</li>
</ul>
<h3 id="load-openvino-model"><strong>Load OpenVINO model</strong></h3>
<p>Load OpenVINO model with <code>loadOpenVINO</code> methods (<code>loadOpenVINO</code> for Java, <code>doLoadOpenVINO</code> for Scala and <code>load_openvino</code> Python).</p>
<p><strong>Java</strong></p>
<pre><code class="java">public class ExtendedInferenceModel extends AbstractInferenceModel {
}
ExtendedInferenceModel model = new ExtendedInferenceModel();
model.loadOpenVINO(modelPath, weightPath);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val model = new InferenceModel()
model.doLoadOpenVINO(modelPath, weightPath)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model = InferenceModel()
model.load_openvino(modelPath, weightPath)
</code></pre>

<ul>
<li><code>modelPath</code>: String. Path of pre-trained OpenVINO model.</li>
<li><code>weightPath</code>: String. Path of pre-trained OpenVINO model weight.</li>
</ul>
<h2 id="predict-with-loaded-model"><strong>Predict with loaded model</strong></h2>
<p>After loading pre-trained models with load methods, we can make prediction with unified <code>predict</code> method.</p>
<ul>
<li><code>predictInput</code>: JList[JList[JTensor]] or <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/tensor">Tensor</a> for Scale and Java, Numpy for Python. Input data for prediction. <a href="https://github.com/intel-analytics/analytics-zoo/blob/master/zoo/src/main/java/com/intel/analytics/zoo/pipeline/inference/JTensor.java">JTensor</a> is a 1D List, with Array[Int] shape.</li>
<li><code>predictOutput</code>: JList[JList[JTensor]] or <a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/tensor">Tensor</a> for Scale and Java, Numpy for Python. Prediction result.</li>
</ul>
<h3 id="predict"><strong>predict</strong></h3>
<p>Do prediction with <code>predict</code> methods (<code>predict</code> for Java and Python, <code>doPredict</code> for Scala).</p>
<p><strong>Java</strong></p>
<pre><code class="java">List&lt;List&lt;JTensor&gt;&gt; predictOutput = model.predict(predictInput);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val predictOutput = model.doPredict(predictInput)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">predict_output = model.predict(predict_input)
</code></pre>

<h3 id="predictint8-and-loadtfascalibratedopenvino"><strong>predictInt8</strong> and <strong>loadTFAsCalibratedOpenVINO</strong></h3>
<p>Do prediction with int8 optimized model. Powered by <a href="https://en.wikichip.org/wiki/x86/avx512vnni">VNNI</a> and <a href="https://www.intel.ai/intel-deep-learning-boost/">Intel Deep Learning Boost</a>. Currently, this API is only for OpenVINO. For Analytics Zoo model, int8 optimized model can directly make prediction with <code>predict</code> method.</p>
<p>To load an OpenVINO int8 optimized model from TensorFlow, we build <code>loadTFAsCalibratedOpenVINO</code> methods with 4 more parameters than <code>loadOpenVINOModelForTF</code>. Note that in this step, we need to prepare <code>validation images</code> and <code>OpenCV 4.0 libs</code> for int8 optimization. For more details, please refer to <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/vnni/openvino">this example</a>.</p>
<ul>
<li><code>networkType</code>: String. Type of an inferred network, "C" to calibrate Classification, "OD" to calibrate Object Detection, "RawC" to collect only statistics for Classification, "RawOD" to collect only statistics for Object Detection.</li>
<li><code>validationFilePath</code>: String. Path to a file with validation images path and target labels.</li>
<li><code>subset</code>: Int. Number of pictures from the whole validation set to create the calibration dataset.</li>
<li><code>opencvLibPath</code>: String. lib path where libopencv_imgcodecs.so.4.0, libopencv_core.so.4.0 and libopencv_imgproc.so.4.0 can be found.</li>
</ul>
<p><strong>Java</strong></p>
<pre><code class="java">List&lt;List&lt;JTensor&gt;&gt; predictOutput = model.predictInt8(predictInput);
</code></pre>

<p><strong>Scala</strong></p>
<pre><code class="scala">val predictOutput = model.doPredictInt8(predictInput)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">predict_output = model.predict_int8(predict_input)
</code></pre>

<h2 id="supportive-classes"><strong>Supportive classes</strong></h2>
<p><strong>InferenceModel</strong></p>
<p><code>doOptimizeTF</code> method in Scala is designed for coverting TensorFlow model into OpenVINO model.</p>
<p><code>doCalibrateTF</code> method in Scala is designed for optimizing OpenVINO model into OpenVINO int8 optimized model.</p>
<p>Pipline of these API:</p>
<p>TensorFlow model -<code>doOptimizeTF</code>-&gt; OpenVINO model -<code>doCalibrateTF</code>-&gt; OpenVINO int8 optimized model</p>
<p><strong>InferenceSupportive</strong></p>
<p><code>InferenceSupportive</code> is a trait containing several methods for type transformation, which transfer a model input 
to a valid data type, thus supporting future inference model prediction tasks.</p>
<p>For example, method <code>transferTensorToJTensor</code> convert a model input of data type <code>Tensor</code> 
to <a href="https://github.com/intel-analytics/analytics-zoo/blob/88afc2d921bb50341d8d7e02d380fa28f49d246b/zoo/src/main/java/com/intel/analytics/zoo/pipeline/inference/JTensor.java"><code>JTensor</code></a>
, which will be the input for a FloatInferenceModel.</p>
<p><strong>AbstractModel</strong></p>
<p><code>AbstractModel</code> is an abstract class to provide APIs for basic functions - <code>predict</code> interface for prediction, <code>copy</code> interface for coping the model into the queue of AbstractModels, <code>release</code> interface for releasing the model and <code>isReleased</code> interface for checking the state of model release.  </p>
<p><strong>FloatModel</strong></p>
<p><code>FloatModel</code> is an extending class of <code>AbstractModel</code> and achieves all <code>AbstractModel</code> interfaces. </p>
<p><strong>OpenVINOModel</strong></p>
<p><code>OpenVINOModel</code> is an extending class of <code>AbstractModel</code>. It achieves all <code>AbstractModel</code> functions.</p>
<p><strong>InferenceModelFactory</strong></p>
<p><code>InferenceModelFactory</code> is an object with APIs for loading pre-trained Analytics Zoo models, Caffe models, Tensorflow models and OpenVINO Intermediate Representations(IR). Analytics Zoo models, Caffe models, Tensorflow models can be loaded as FloatModels. The load result of it is a <code>FloatModel</code> Tensorflow models and OpenVINO Intermediate Representations(IR) can be loaded as OpenVINOModels. The load result of it is an <code>OpenVINOModel</code>. 
The load result of it is a <code>FloatModel</code> or an <code>OpenVINOModel</code>.</p>
<p><strong>OpenVinoInferenceSupportive</strong></p>
<p><code>OpenVinoInferenceSupportive</code> is an extending object of <code>InferenceSupportive</code> and focuses on the implementation of loading pre-trained models, including tensorflow models and OpenVINO Intermediate Representations(IR).</p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>