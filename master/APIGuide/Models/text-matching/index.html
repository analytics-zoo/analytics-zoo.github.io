<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Text Matching - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Build a KNRM Model", url: "#build-a-knrm-model", children: [
          ]},
          {title: "Save Model", url: "#save-model", children: [
          ]},
          {title: "Load Model", url: "#load-model", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Text Matching</strong></h1>
    <hr>
    <p>Analytics Zoo provides a pre-defined KNRM model that can be used for text matching (e.g. question answering).
For training, you can use Keras-Style API methods or alternatively feed the model into NNFrames and BigDL Optimizer.
More text matching models will be supported in the future.</p>
<hr />
<h2 id="build-a-knrm-model"><strong>Build a KNRM Model</strong></h2>
<p>Kernel-pooling Neural Ranking Model with RBF kernel. See <a href="https://arxiv.org/abs/1706.06613">here</a> for more details.</p>
<p>You can call the following API in Scala and Python respectively to create a <code>KNRM</code> with <em>pre-trained GloVe word embeddings</em>.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">val knrm = KNRM(text1Length, text2Length, embeddingFile, wordIndex = null, trainEmbed = true, kernelNum = 21, sigma = 0.1, exactSigma = 0.001, targetMode = &quot;ranking&quot;)
</code></pre>

<ul>
<li><code>text1Length</code>: Sequence length of text1 (query).</li>
<li><code>text2Length</code>: Sequence length of text2 (doc).</li>
<li><code>embeddingFile</code>: The path to the word embedding file. Currently only <em>glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt</em> are supported. You can download from <a href="https://nlp.stanford.edu/projects/glove/">here</a>.</li>
<li><code>wordIndex</code>: Map of word (String) and its corresponding index (integer). The index is supposed to <strong>start from 1</strong> with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is null. In this case, all the words in the embeddingFile will be taken into account and you can call <code>WordEmbedding.getWordIndex(embeddingFile)</code> to retrieve the map.</li>
<li><code>trainEmbed</code>: Boolean. Whether to train the embedding layer or not. Default is true.</li>
<li><code>kernelNum</code>: Integer &gt; 1. The number of kernels to use. Default is 21.</li>
<li><code>sigma</code>: Double. Defines the kernel width, or the range of its softTF count. Default is 0.1.</li>
<li><code>exactSigma</code>: Double. The sigma used for the kernel that harvests exact matches in the case where RBF mu=1.0. Default is 0.001.</li>
<li><code>targetMode</code>: String. The target mode of the model. Either 'ranking' or 'classification'. For ranking, the output will be the relevance score between text1 and text2 and you are recommended to use 'rank_hinge' as loss for pairwise training.
For classification, the last layer will be sigmoid and the output will be the probability between 0 and 1 indicating whether text1 is related to text2 and
you are recommended to use 'binary_crossentropy' as loss for binary classification. Default mode is 'ranking'.</li>
</ul>
<p>See <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/qaranker">here</a> for the Scala example that trains a KNRM model on WikiQA dataset.</p>
<p><strong>Python</strong></p>
<pre><code class="python">knrm = KNRM(text1_length, text2_length, embedding_file, word_index=None, train_embed=True, kernel_num=21, sigma=0.1, exact_sigma=0.001, target_mode=&quot;ranking&quot;)
</code></pre>

<ul>
<li><code>text1_length</code>: Sequence length of text1 (query).</li>
<li><code>text2_length</code>: Sequence length of text2 (doc).</li>
<li><code>embedding_file</code>: The path to the word embedding file. Currently only <em>glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.42B.300d.txt, glove.840B.300d.txt</em> are supported. You can download from <a href="https://nlp.stanford.edu/projects/glove/">here</a>.</li>
<li><code>word_index</code>: Dictionary of word (string) and its corresponding index (int). The index is supposed to <strong>start from 1</strong> with 0 reserved for unknown words. During the prediction, if you have words that are not in the wordIndex for the training, you can map them to index 0. Default is None. In this case, all the words in the embedding_file will be taken into account and you can call <code>WordEmbedding.get_word_index(embedding_file)</code> to retrieve the dictionary.</li>
<li><code>train_embed</code>: Boolean. Whether to train the embedding layer or not. Default is True.</li>
<li><code>kernel_num</code>: Int &gt; 1. The number of kernels to use. Default is 21.</li>
<li><code>sigma</code>: Float. Defines the kernel width, or the range of its softTF count. Default is 0.1.</li>
<li><code>exact_sigma</code>: Float. The sigma used for the kernel that harvests exact matches in the case where RBF mu=1.0. Default is 0.001.</li>
<li><code>target_mode</code>: String. The target mode of the model. Either 'ranking' or 'classification'. For ranking, the output will be the relevance score between text1 and text2 and you are recommended to use 'rank_hinge' as loss for pairwise training.
For classification, the last layer will be sigmoid and the output will be the probability between 0 and 1 indicating whether text1 is related to text2 and
you are recommended to use 'binary_crossentropy' as loss for binary classification. Default mode is 'ranking'.</li>
</ul>
<p>See <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/qaranker">here</a> for the Python example that trains a KNRM model on WikiQA dataset.</p>
<hr />
<h2 id="save-model"><strong>Save Model</strong></h2>
<p>After building and training a KNRM model, you can save it for future use.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">knrm.saveModel(path, weightPath = null, overWrite = false)
</code></pre>

<ul>
<li><code>path</code>: The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like "hdfs://[host]:[port]/xxx". Amazon S3 path should be like "s3a://bucket/xxx".</li>
<li><code>weightPath</code>: The path to save weights. Default is null.</li>
<li><code>overWrite</code>: Whether to overwrite the file if it already exists. Default is false.</li>
</ul>
<p><strong>Python</strong></p>
<pre><code class="python">knrm.save_model(path, weight_path=None, over_write=False)
</code></pre>

<ul>
<li><code>path</code>: The path to save the model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.</li>
<li><code>weight_path</code>: The path to save weights. Default is None.</li>
<li><code>over_write</code>: Whether to overwrite the file if it already exists. Default is False.</li>
</ul>
<hr />
<h2 id="load-model"><strong>Load Model</strong></h2>
<p>To load a KNRM model (with weights) saved <a href="#save-model">above</a>:</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">KNRM.loadModel(path, weightPath = null)
</code></pre>

<ul>
<li><code>path</code>: The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like "hdfs://[host]:[port]/xxx". Amazon S3 path should be like "s3a://bucket/xxx".</li>
<li><code>weightPath</code>: The path for pre-trained weights if any. Default is null.</li>
</ul>
<p><strong>Python</strong></p>
<pre><code class="python">KNRM.load_model(path, weight_path=None)
</code></pre>

<ul>
<li><code>path</code>: The path for the pre-defined model. Local file system, HDFS and Amazon S3 are supported. HDFS path should be like 'hdfs://[host]:[port]/xxx'. Amazon S3 path should be like 's3a://bucket/xxx'.</li>
<li><code>weight_path</code>: The path for pre-trained weights if any. Default is None.</li>
</ul>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>