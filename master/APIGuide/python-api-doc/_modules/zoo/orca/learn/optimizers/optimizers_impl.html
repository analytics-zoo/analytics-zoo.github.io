
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>zoo.orca.learn.optimizers.optimizers_impl &#8212; analytics-zoo  documentation</title>
    <link rel="stylesheet" href="../../../../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../../../index.html">analytics-zoo  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../../../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for zoo.orca.learn.optimizers.optimizers_impl</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Copyright 2018 Analytics Zoo Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="kn">from</span> <span class="nn">bigdl.util.common</span> <span class="kn">import</span> <span class="n">DOUBLEMAX</span>
<span class="kn">from</span> <span class="nn">zoo.orca.learn.optimizers.schedule</span> <span class="kn">import</span> <span class="n">Scheduler</span>


<div class="viewcode-block" id="Optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Optimizer">[docs]</a><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

<div class="viewcode-block" id="Optimizer.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Optimizer.get_optimizer">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.SGD">[docs]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A plain implementation of SGD</span>

<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param weightdecay weight decay</span>
<span class="sd">    :param momentum momentum</span>
<span class="sd">    :param dampening dampening for momentum</span>
<span class="sd">    :param nesterov enables Nesterov momentum</span>
<span class="sd">    :param learningrates 1D tensor of individual learning rates</span>
<span class="sd">    :param weightdecays 1D tensor of individual weight decays</span>
<span class="sd">    &gt;&gt;&gt; sgd = SGD()</span>
<span class="sd">    creating: createDefault</span>
<span class="sd">    creating: createSGD</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">weightdecay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">dampening</span><span class="o">=</span><span class="n">DOUBLEMAX</span><span class="p">,</span>
                 <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">learningrate_schedule</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">learningrates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">weightdecays</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">SGD</span> <span class="k">as</span> <span class="n">BSGD</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">learningrate_schedule</span><span class="p">,</span> <span class="n">Scheduler</span><span class="p">),</span>\
            <span class="s2">&quot;learningrate_schedule should be an zoo.orca.learn.optimizers.schedule.Scheduler,&quot;</span> \
            <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="n">learningrate_schedule</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BSGD</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span>
                              <span class="n">learningrate_decay</span><span class="p">,</span>
                              <span class="n">weightdecay</span><span class="p">,</span>
                              <span class="n">momentum</span><span class="p">,</span>
                              <span class="n">dampening</span><span class="p">,</span>
                              <span class="n">nesterov</span><span class="p">,</span>
                              <span class="n">learningrate_schedule</span><span class="o">.</span><span class="n">get_scheduler</span><span class="p">(),</span>
                              <span class="n">learningrates</span><span class="p">,</span>
                              <span class="n">weightdecays</span><span class="p">,</span>
                              <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="SGD.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.SGD.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="Adagrad"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adagrad">[docs]</a><span class="k">class</span> <span class="nc">Adagrad</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adagrad. See the original paper:</span>
<span class="sd">    http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</span>

<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param weightdecay weight decay</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adagrad()</span>
<span class="sd">    creating: createAdagrad</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">weightdecay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">Adagrad</span> <span class="k">as</span> <span class="n">BAdagrad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BAdagrad</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span> <span class="n">learningrate_decay</span><span class="p">,</span>
                                  <span class="n">weightdecay</span><span class="p">,</span> <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Adagrad.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adagrad.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="LBFGS"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.LBFGS">[docs]</a><span class="k">class</span> <span class="nc">LBFGS</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This implementation of L-BFGS relies on a user-provided line</span>
<span class="sd">    search function (state.lineSearch). If this function is not</span>
<span class="sd">    provided, then a simple learningRate is used to produce fixed</span>
<span class="sd">    size steps. Fixed size steps are much less costly than line</span>
<span class="sd">    searches, and can be useful for stochastic problems.</span>
<span class="sd">    The learning rate is used even when a line search is provided.</span>
<span class="sd">    This is also useful for large-scale stochastic problems, where</span>
<span class="sd">    opfunc is a noisy approximation of f(x). In that case, the learning</span>
<span class="sd">    rate allows a reduction of confidence in the step size.</span>

<span class="sd">    :param max_iter Maximum number of iterations allowed</span>
<span class="sd">    :param max_eval Maximum number of function evaluations</span>
<span class="sd">    :param tolfun Termination tolerance on the first-order optimality</span>
<span class="sd">    :param tolx Termination tol on progress in terms of func/param changes</span>
<span class="sd">    :param ncorrection</span>
<span class="sd">    :param learningrate</span>
<span class="sd">    :param verbose</span>
<span class="sd">    :param linesearch A line search function</span>
<span class="sd">    :param linesearch_options If no line search provided, then a fixed step size is used</span>
<span class="sd">    &gt;&gt;&gt; lbfgs = LBFGS()</span>
<span class="sd">    creating: createLBFGS</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">max_eval</span><span class="o">=</span><span class="n">DOUBLEMAX</span><span class="p">,</span>
                 <span class="n">tolfun</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">tolx</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span>
                 <span class="n">ncorrection</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">linesearch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">linesearch_options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">LBFGS</span> <span class="k">as</span> <span class="n">BLBFGS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BLBFGS</span><span class="p">(</span>
            <span class="n">max_iter</span><span class="p">,</span>
            <span class="n">max_eval</span><span class="p">,</span>
            <span class="n">tolfun</span><span class="p">,</span>
            <span class="n">tolx</span><span class="p">,</span>
            <span class="n">ncorrection</span><span class="p">,</span>
            <span class="n">learningrate</span><span class="p">,</span>
            <span class="n">verbose</span><span class="p">,</span>
            <span class="n">linesearch</span><span class="p">,</span>
            <span class="n">linesearch_options</span><span class="p">,</span>
            <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="LBFGS.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.LBFGS.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="Adadelta"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adadelta">[docs]</a><span class="k">class</span> <span class="nc">Adadelta</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adadelta implementation for SGD: http://arxiv.org/abs/1212.5701</span>

<span class="sd">    :param decayrate interpolation parameter rho</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adadelta()</span>
<span class="sd">    creating: createAdadelta</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">decayrate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">Adadelta</span> <span class="k">as</span> <span class="n">BAdadelta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BAdadelta</span><span class="p">(</span><span class="n">decayrate</span><span class="p">,</span>
                                   <span class="n">epsilon</span><span class="p">,</span>
                                   <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Adadelta.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adadelta.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adam http://arxiv.org/pdf/1412.6980.pdf</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param beta1 first moment coefficient</span>
<span class="sd">    :param beta2 second moment coefficient</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adam = Adam()</span>
<span class="sd">    creating: createAdam</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">Adam</span> <span class="k">as</span> <span class="n">BAdam</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BAdam</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span>
                               <span class="n">learningrate_decay</span><span class="p">,</span>
                               <span class="n">beta1</span><span class="p">,</span>
                               <span class="n">beta2</span><span class="p">,</span>
                               <span class="n">epsilon</span><span class="p">,</span>
                               <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Adam.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adam.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="ParallelAdam"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.ParallelAdam">[docs]</a><span class="k">class</span> <span class="nc">ParallelAdam</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adam http://arxiv.org/pdf/1412.6980.pdf</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param beta1 first moment coefficient</span>
<span class="sd">    :param beta2 second moment coefficient</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; pAdam = ParallelAdam()</span>
<span class="sd">    creating: createParallelAdam</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">parallel_num</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">ParallelAdam</span> <span class="k">as</span> <span class="n">BParallelAdam</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BParallelAdam</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span>
                                       <span class="n">learningrate_decay</span><span class="p">,</span>
                                       <span class="n">beta1</span><span class="p">,</span>
                                       <span class="n">beta2</span><span class="p">,</span>
                                       <span class="n">epsilon</span><span class="p">,</span>
                                       <span class="n">parallel_num</span><span class="p">,</span>
                                       <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="ParallelAdam.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.ParallelAdam.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="Ftrl"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Ftrl">[docs]</a><span class="k">class</span> <span class="nc">Ftrl</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Ftrl https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf.</span>
<span class="sd">    Support L1 penalty, L2 penalty and shrinkage-type L2 penalty.</span>

<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_power double, must be less or equal to zero. Default is -0.5.</span>
<span class="sd">    :param initial_accumulator_value double, the starting value for accumulators,</span>
<span class="sd">        require zero or positive values.</span>
<span class="sd">    :param l1_regularization_strength double, must be greater or equal to zero. Default is zero.</span>
<span class="sd">    :param l2_regularization_strength double, must be greater or equal to zero. Default is zero.</span>
<span class="sd">    :param l2_shrinkage_regularization_strength double, must be greater or equal to zero.</span>
<span class="sd">        Default is zero. This differs from l2RegularizationStrength above. L2 above is a</span>
<span class="sd">        stabilization penalty, whereas this one is a magnitude penalty.</span>
<span class="sd">    &gt;&gt;&gt; ftrl = Ftrl()</span>
<span class="sd">    creating: createFtrl</span>
<span class="sd">    &gt;&gt;&gt; ftrl2 = Ftrl(1e-2, -0.1, 0.2, 0.3, 0.4, 0.5)</span>
<span class="sd">    creating: createFtrl</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">learningrate_power</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">initial_accumulator_value</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">l1_regularization_strength</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">l2_regularization_strength</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">l2_shrinkage_regularization_strength</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">Ftrl</span> <span class="k">as</span> <span class="n">BFtrl</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BFtrl</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span>
                               <span class="n">learningrate_power</span><span class="p">,</span>
                               <span class="n">initial_accumulator_value</span><span class="p">,</span>
                               <span class="n">l1_regularization_strength</span><span class="p">,</span>
                               <span class="n">l2_regularization_strength</span><span class="p">,</span>
                               <span class="n">l2_shrinkage_regularization_strength</span><span class="p">,</span>
                               <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Ftrl.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Ftrl.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="Adamax"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adamax">[docs]</a><span class="k">class</span> <span class="nc">Adamax</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of Adamax http://arxiv.org/pdf/1412.6980.pdf</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param beta1 first moment coefficient</span>
<span class="sd">    :param beta2 second moment coefficient</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = Adamax()</span>
<span class="sd">    creating: createAdamax</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span>
                 <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-38</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">Adamax</span> <span class="k">as</span> <span class="n">BAdamax</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BAdamax</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span>
                                 <span class="n">beta1</span><span class="p">,</span>
                                 <span class="n">beta2</span><span class="p">,</span>
                                 <span class="n">epsilon</span><span class="p">,</span>
                                 <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Adamax.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.Adamax.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="RMSprop"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.RMSprop">[docs]</a><span class="k">class</span> <span class="nc">RMSprop</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An implementation of RMSprop</span>
<span class="sd">    :param learningrate learning rate</span>
<span class="sd">    :param learningrate_decay learning rate decay</span>
<span class="sd">    :param decayrate decay rate, also called rho</span>
<span class="sd">    :param epsilon for numerical stability</span>
<span class="sd">    &gt;&gt;&gt; adagrad = RMSprop()</span>
<span class="sd">    creating: createRMSprop</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">learningrate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
                 <span class="n">learningrate_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">decayrate</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">bigdl.optim.optimizer</span> <span class="kn">import</span> <span class="n">RMSprop</span> <span class="k">as</span> <span class="n">BRMSprop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BRMSprop</span><span class="p">(</span><span class="n">learningrate</span><span class="p">,</span>
                                  <span class="n">learningrate_decay</span><span class="p">,</span>
                                  <span class="n">decayrate</span><span class="p">,</span>
                                  <span class="n">epsilon</span><span class="p">,</span>
                                  <span class="n">bigdl_type</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="RMSprop.get_optimizer"><a class="viewcode-back" href="../../../../../zoo.orca.learn.optimizers.html#zoo.orca.learn.optimizers.optimizers_impl.RMSprop.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../../../index.html">analytics-zoo  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../../../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Intel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.4.
    </div>
  </body>
</html>