<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Working with Texts - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "TextSet", url: "#textset", children: [
          ]},
          {title: "Read texts as TextSet", url: "#read-texts-as-textset", children: [
              {title: "Read texts from a directory", url: "#read-texts-from-a-directory" },
              {title: "Read texts from csv file", url: "#read-texts-from-csv-file" },
              {title: "Read texts from parquet file", url: "#read-texts-from-parquet-file" },
          ]},
          {title: "Build Text Transformation Pipeline", url: "#build-text-transformation-pipeline", children: [
          ]},
          {title: "Text Training", url: "#text-training", children: [
          ]},
          {title: "Word Index Save and Load", url: "#word-index-save-and-load", children: [
              {title: "Save word index", url: "#save-word-index" },
              {title: "Load word index", url: "#load-word-index" },
          ]},
          {title: "Text Prediction", url: "#text-prediction", children: [
          ]},
          {title: "Examples", url: "#examples", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Working with Texts</strong></h1>
    <hr>
    <p>Analytics Zoo provides a series of text related APIs for end-to-end text processing pipeline,
including text loading, pre-processing, training and inference, etc.</p>
<hr />
<h2 id="textset"><strong>TextSet</strong></h2>
<p><code>TextSet</code> is a collection of TextFeatures where each <code>TextFeature</code> keeps information of a single text record.</p>
<p><code>TextSet</code> can either be a <code>DistributedTextSet</code> consisting of text RDD or a <code>LocalTextSet</code> consisting of text array.</p>
<hr />
<h2 id="read-texts-as-textset"><strong>Read texts as TextSet</strong></h2>
<h4 id="read-texts-from-a-directory"><strong>Read texts from a directory</strong></h4>
<p>Read texts with labels from a directory.</p>
<p>Under this specified directory path, there are supposed to be several subdirectories, each of which contains a number of text files belonging to this category. 
Each category will be a given a label (starting from 0) according to its position in the ascending order sorted among all subdirectories. 
Each text will be a given a label according to the directory where it is located.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">textSet = TextSet.read(path, sc = null, minPartitions = 1)
</code></pre>

<ul>
<li><code>path</code>: String. Folder path to texts. Local or distributed file system (such as HDFS) are supported. If you want to read from HDFS, sc needs to be specified.</li>
<li><code>sc</code>: An instance of SparkContext. If specified, texts will be read as a DistributedTextSet. 
Default is null and in this case texts will be read as a LocalTextSet. </li>
<li><code>minPartitions</code>: Integer. A suggestion value of the minimal partition number for input texts.
Only need to specify this when sc is not null. Default is 1.</li>
</ul>
<p><strong>Python</strong></p>
<pre><code class="python">text_set = TextSet.read(path, sc=None, min_partitions=1)
</code></pre>

<ul>
<li><code>path</code>: String. Folder path to texts. Local or distributed file system (such as HDFS) are supported. If you want to read from HDFS, sc needs to be defined.</li>
<li><code>sc</code>: An instance of SparkContext. If specified, texts will be read as a DistributedTextSet. 
Default is None and in this case texts will be read as a LocalTextSet. </li>
<li><code>min_partitions</code>: Int. A suggestion value of the minimal partition number for input texts.
Only need to specify this when sc is not None. Default is 1.</li>
</ul>
<h4 id="read-texts-from-csv-file"><strong>Read texts from csv file</strong></h4>
<p>Read texts with id from csv file.</p>
<p>Each record is supposed to contain id(String) and text(String) in order.</p>
<p>Note that the csv file should be without header.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">textSet = TextSet.readCSV(path, sc = null, minPartitions = 1)
</code></pre>

<ul>
<li><code>path</code>: String. The path to the csv file. Local or distributed file system (such as HDFS) are supported. If you want to read from HDFS, sc needs to be specified.</li>
<li><code>sc</code>: An instance of SparkContext. If specified, texts will be read as a DistributedTextSet. 
Default is null and in this case texts will be read as a LocalTextSet. </li>
<li><code>minPartitions</code>: Integer. A suggestion value of the minimal partition number for input texts.
Only need to specify this when sc is not null. Default is 1.</li>
</ul>
<p><strong>Python</strong></p>
<pre><code class="python">text_set = TextSet.read_csv(path, sc=None, min_partitions=1)
</code></pre>

<ul>
<li><code>path</code>: String. The path to the csv file. Local or distributed file system (such as HDFS) are supported. If you want to read from HDFS, sc needs to be defined.</li>
<li><code>sc</code>: An instance of SparkContext. If specified, texts will be read as a DistributedTextSet. 
Default is None and in this case texts will be read as a LocalTextSet. </li>
<li><code>min_partitions</code>: Int. A suggestion value of the minimal partition number for input texts.
Only need to specify this when sc is not None. Default is 1.</li>
</ul>
<h4 id="read-texts-from-parquet-file"><strong>Read texts from parquet file</strong></h4>
<p>Read texts with id from parquet file with schema id(String) and text(String). Return a DistributedTextSet.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">textSet = TextSet.readParquet(path, sqlContext)
</code></pre>

<ul>
<li><code>path</code>: The path to the parquet file.</li>
<li><code>sqlContext</code>: An instance of SQLContext.</li>
</ul>
<p><strong>Python</strong></p>
<pre><code class="python">text_set = TextSet.read_parquet(path, sc)
</code></pre>

<ul>
<li><code>path</code>: The path to the parquet file.</li>
<li><code>sc</code>: An instance of SparkContext.</li>
</ul>
<hr />
<h2 id="build-text-transformation-pipeline"><strong>Build Text Transformation Pipeline</strong></h2>
<p>You can easily call transformation methods of a TextSet one by one to build the text transformation pipeline. Please refer to <a href="../../APIGuide/FeatureEngineering/text/#textset-transformations">here</a> for more details.</p>
<p><strong>Scala Example</strong></p>
<pre><code class="scala">transformedTextSet = textSet.tokenize().normalize().word2idx().shapeSequence(len).generateSample()
</code></pre>

<p><strong>Python Example</strong></p>
<pre><code class="python">transformed_text_set = text_set.tokenize().normalize().word2idx().shape_sequence(len).generate_sample()
</code></pre>

<hr />
<h2 id="text-training"><strong>Text Training</strong></h2>
<p>After doing text transformation, you can directly feed the transformed TextSet into the model for training.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">model.fit(transformedTextSet, batchSize, nbEpoch)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">model.fit(transformed_text_set, batch_size, nb_epoch)
</code></pre>

<hr />
<h2 id="word-index-save-and-load"><strong>Word Index Save and Load</strong></h2>
<h4 id="save-word-index"><strong>Save word index</strong></h4>
<p>After training the model, you can save the word index correspondence to text file, which can be used for future inference. Each separate line will be "word id".</p>
<p>For LocalTextSet, save txt to a local file system.</p>
<p>For DistributedTextSet, save txt to a local or distributed file system (such as HDFS).</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">transformedTextSet.saveWordIndex(path)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">transformed_text_set.save_word_index(path)
</code></pre>

<h4 id="load-word-index"><strong>Load word index</strong></h4>
<p>During text prediction, you can load the saved word index back, so that the prediction TextSet uses exactly the same word index as the training process. Each separate line should be "word id".</p>
<p>For LocalTextSet, load txt to a local file system.</p>
<p>For DistributedTextSet, load txt to a local or distributed file system (such as HDFS).</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">textSet.loadWordIndex(path)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">text_set.load_word_index(path)
</code></pre>

<hr />
<h2 id="text-prediction"><strong>Text Prediction</strong></h2>
<p>Given a raw TextSet to do prediction, you need to first load the saved word index back as instructed <a href="#load-word-index">above</a> and go through the same transformation
process as what you did in your training. Note that here you do not need to specify any argument when calling <code>word2idx</code> in the preprocessing pipeline as now you are using exactly
the loaded word index.</p>
<p>Then you can directly feed the transformed TextSet into the model for prediction and the prediction result will be stored in each TextFeature.</p>
<p><strong>Scala</strong></p>
<pre><code class="scala">predictionTextSet = model.predict(transformedTextSet)
</code></pre>

<p><strong>Python</strong></p>
<pre><code class="python">prediction_text_set = model.predict(transformed_text_set)
</code></pre>

<hr />
<h2 id="examples"><strong>Examples</strong></h2>
<p>You can refer to our TextClassification example for TextSet transformation, training and inference.</p>
<p>See <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/textclassification">here</a> for the Scala example.</p>
<p>See <a href="https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/textclassification">here</a> for the Python example.</p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>