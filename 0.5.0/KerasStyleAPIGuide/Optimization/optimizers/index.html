<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Optimizers - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Usage of optimizers", url: "#usage-of-optimizers", children: [
          ]},
          {title: "Available optimizers", url: "#available-optimizers", children: [
          ]},
          {title: "SGD", url: "#sgd", children: [
          ]},
          {title: "Adam", url: "#adam", children: [
          ]},
          {title: "Adamax", url: "#adamax", children: [
          ]},
          {title: "Adadelta", url: "#adadelta", children: [
          ]},
          {title: "Adagrad", url: "#adagrad", children: [
          ]},
          {title: "Rmsprop", url: "#rmsprop", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Optimizers</strong></h1>
    <hr>
    <h2 id="usage-of-optimizers">Usage of optimizers</h2>
<p>An optimizer is one of the two arguments required for compiling a model.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">model.compile(loss = &quot;mean_squared_error&quot;, optimizer = &quot;sgd&quot;)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model.compile(loss='mean_squared_error', optimizer='sgd')
</code></pre>

<p><strong>Scala:</strong></p>
<pre><code class="scala">model.compile(loss = &quot;mean_squared_error&quot;, optimizer = Adam())
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">model.compile(loss='mean_squared_error', optimizer=Adam())
</code></pre>

<hr />
<h2 id="available-optimizers">Available optimizers</h2>
<h2 id="sgd">SGD</h2>
<p>A plain implementation of SGD which provides optimize method. After setting 
optimization method when create Optimize, Optimize will call optimization method at the end of 
each iteration.</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = SGD(learningRate = 1e-3, learningRateDecay = 0.0, 
                      weightDecay = 0.0, momentum = 0.0, dampening = Double.MaxValue, 
                      nesterov = false, learningRateSchedule = Default(), 
                      learningRates = null, weightDecays = null)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningRate</code> : learning rate</li>
<li><code>learningRateDecay</code> : learning rate decay</li>
<li><code>weightDecay</code> : weight decay</li>
<li><code>momentum</code> : momentum</li>
<li><code>dampening</code> : dampening for momentum</li>
<li><code>nesterov</code> : enables Nesterov momentum</li>
<li><code>learningRateSchedule</code> : learning rate scheduler</li>
<li><code>learningRates</code> : 1D tensor of individual learning rates</li>
<li><code>weightDecays</code> : 1D tensor of individual weight decays</li>
</ul>
<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = SGD(learningrate=1e-3, learningrate_decay=0.0, weightdecay=0.0, 
                   momentum=0.0, dampening=DOUBLEMAX, nesterov=False, 
                   leaningrate_schedule=None, learningrates=None, 
                   weightdecays=None)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningrate</code> : learning rate</li>
<li><code>learningrate_decay</code> : learning rate decay</li>
<li><code>weightdecay</code> : weight decay</li>
<li><code>momentum</code> : momentum</li>
<li><code>dampening</code> : dampening for momentum</li>
<li><code>nesterov</code> : enables Nesterov momentum</li>
<li><code>leaningrate_schedule</code> : learning rate scheduler</li>
<li><code>learningrates</code> : 1D tensor of individual learning rates</li>
<li><code>weightdecays</code> : 1D tensor of individual weight decays</li>
</ul>
<h2 id="adam">Adam</h2>
<p>An implementation of Adam optimization, first-order gradient-based optimization of stochastic  objective  functions. <a href="http://arxiv.org/pdf/1412.6980.pdf">http://arxiv.org/pdf/1412.6980.pdf</a></p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = new Adam(learningRate = 1e-3, learningRateDecay = 0.0, beta1 = 0.9, beta2 = 0.999, Epsilon = 1e-8)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningRate</code> learning rate. Default value is 1e-3. </li>
<li><code>learningRateDecay</code> learning rate decay. Default value is 0.0.</li>
<li><code>beta1</code> first moment coefficient. Default value is 0.9.</li>
<li><code>beta2</code> second moment coefficient. Default value is 0.999.</li>
<li><code>Epsilon</code> for numerical stability. Default value is 1e-8.</li>
</ul>
<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = Adam(learningrate=1e-3, learningrate_decay=0.0, beta1=0.9, beta2=0.999, epsilon=1e-8)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningrate</code> learning rate. Default value is 1e-3. </li>
<li><code>learningrate_decay</code> learning rate decay. Default value is 0.0.</li>
<li><code>beta1</code> first moment coefficient. Default value is 0.9.</li>
<li><code>beta2</code> second moment coefficient. Default value is 0.999.</li>
<li><code>epsilon</code> for numerical stability. Default value is 1e-8.</li>
</ul>
<h2 id="adamax">Adamax</h2>
<p>An implementation of Adamax: <a href="http://arxiv.org/pdf/1412.6980.pdf">http://arxiv.org/pdf/1412.6980.pdf</a></p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = new Adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, Epsilon = 1e-8)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningRate</code> : learning rate</li>
<li><code>beta1</code> : first moment coefficient</li>
<li><code>beta2</code> : second moment coefficient</li>
<li><code>Epsilon</code> : for numerical stability</li>
</ul>
<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = Adam(learningrate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningrate</code> : learning rate</li>
<li><code>beta1</code> : first moment coefficient</li>
<li><code>beta2</code> : second moment coefficient</li>
<li><code>epsilon</code> : for numerical stability</li>
</ul>
<h2 id="adadelta">Adadelta</h2>
<p><em>AdaDelta</em> implementation for <em>SGD</em> 
It has been proposed in <code>ADADELTA: An Adaptive Learning Rate Method</code>.
<a href="http://arxiv.org/abs/1212.5701.">http://arxiv.org/abs/1212.5701.</a></p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = Adadelta(decayRate = 0.9, Epsilon = 1e-10)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>decayRate</code> : decayRate, also called interpolation parameter rho</li>
<li><code>Epsilon</code> : for numerical stability</li>
</ul>
<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = AdaDelta(decayrate=0.9, epsilon=1e-10)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>decayrate</code> : decayRate, also called interpolation parameter rho</li>
<li><code>epsilon</code> : for numerical stability</li>
</ul>
<h2 id="adagrad">Adagrad</h2>
<p>An implementation of Adagrad. See the original paper:
 <a href="http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = new Adagrad(learningRate = 1e-3, learningRateDecay = 0.0, weightDecay = 0.0)
</code></pre>

<ul>
<li><code>learningRate</code> : learning rate</li>
<li><code>learningRateDecay</code> : learning rate decay</li>
<li><code>weightDecay</code> : weight decay</li>
</ul>
<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = Adagrad(learningrate=1e-3, learningrate_decay=0.0, weightdecay=0.0)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningrate</code> : learning rate</li>
<li><code>learningrate_decay</code> : learning rate decay</li>
<li><code>weightdecay</code> : weight decay</li>
</ul>
<h2 id="rmsprop">Rmsprop</h2>
<p>An implementation of RMSprop (Reference: <a href="http://arxiv.org/pdf/1308.0850v5.pdf">http://arxiv.org/pdf/1308.0850v5.pdf</a>, Sec 4.2)</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">val optimMethod = new RMSprop(learningRate = 0.002, learningRateDecay = 0.0, decayRate = 0.99, Epsilon = 1e-8)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningRate</code> : learning rate</li>
<li><code>learningRateDecay</code> : learning rate decay</li>
<li><code>decayRate</code> : decayRate, also called rho</li>
<li><code>Epsilon</code> : for numerical stability</li>
</ul>
<p><strong>Python:</strong></p>
<pre><code class="python">optim_method = RMSprop(learningrate=0.002, learningrate_decay=0.0, decayrate=0.99, epsilon=1e-8)
</code></pre>

<p>Parameters:</p>
<ul>
<li><code>learningrate</code> : learning rate</li>
<li><code>learningrate_decay</code> : learning rate decay</li>
<li><code>decayrate</code> : decayRate, also called rho</li>
<li><code>epsilon</code> : for numerical stability</li>
</ul>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>