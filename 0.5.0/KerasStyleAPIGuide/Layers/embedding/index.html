<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Embedding Layers - Analytics Zoo</title>
    <link href="/css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="/css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="/css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/highlight.css">
    <link href="../../../extra.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/bootstrap-3.3.7.min.js"></script>
    <script src="/js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "SparseEmbedding", url: "#sparseembedding", children: [
          ]},
          {title: "WordEmbedding", url: "#wordembedding", children: [
          ]},
        ];

    </script>
    <script src="/js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-125166209-1', 'analytics-zoo.github.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    
    <h1><strong>Embedding Layers</strong></h1>
    <hr>
    <h2 id="sparseembedding"><strong>SparseEmbedding</strong></h2>
<p>SparseEmbedding is the sparse version of layer Embedding.</p>
<p>The input of SparseEmbedding should be a 2D SparseTensor or two 2D sparseTensors.
If the input is a SparseTensor, the values are positive integer ids,
values in each row of this SparseTensor will be turned into a dense vector.
If the input is two SparseTensors, the first tensor should be the integer ids, just
like the SparseTensor input. And the second tensor is the corresponding
weights of the integer ids.</p>
<p>This layer can only be used as the first layer in a model, you need to provide the argument
inputShape (a Single Shape, does not include the batch dimension).</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">SparseEmbedding(inputDim, outputDim, combiner = &quot;sum&quot;, max_norm = -1.0, init = &quot;uniform&quot;, wRegularizer = null, inputShape = null)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">SparseEmbedding(input_dim, output_dim, combiner=&quot;sum&quot;, max_norm=-1.0, init=&quot;uniform&quot;, W_regularizer=None, input_shape=None, name=None)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><code>inputDim</code>: Int &gt; 0. Size of the vocabulary.</li>
<li><code>outputDim</code>: Int &gt;= 0. Dimension of the dense embedding.</li>
<li><code>init</code>: String representation of the initialization method for the weights of the layer. Default is "uniform".</li>
<li><code>combiner</code>: A string specifying the reduce type.
              Currently "mean", "sum", "sqrtn" is supported.</li>
<li><code>maxNorm</code>: If provided, each embedding is normalized to have l2 norm equal to
               maxNorm before combining.</li>
<li><code>wRegularizer</code>: An instance of [Regularizer], (eg. L1 or L2 regularization), applied to the input weights matrices. Default is null.</li>
<li><code>inputShape</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a <code>Shape</code> object. For Python API, it should be a shape tuple. Batch dimension should be excluded.</li>
<li><code>name</code>: String to set the name of the layer.
          If not specified, its name will by default to be a generated string.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.SparseEmbedding
import com.intel.analytics.bigdl.utils.Shape
import com.intel.analytics.bigdl.tensor.Tensor

val indices1 = Array(0, 0, 1, 2)
val indices2 = Array(0, 1, 0, 3)
val values = Array(2f, 4, 1, 2)
val input = Tensor.sparse(Array(indices1, indices2), values, Array(3, 4))
val layer = SparseEmbedding[Float](inputDim = 10, outputDim = 4,
  combiner = &quot;sum&quot;, inputShape = Shape(10))
layer.build(Shape(-1, 10))
val output = layer.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: 
(0, 0) : 2.0
(0, 1) : 4.0
(1, 0) : 1.0
(2, 3) : 2.0
[com.intel.analytics.bigdl.tensor.SparseTensor of size 3x4]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">-0.03674142 -0.01844017 -0.015794257    -0.045957662    
-0.02645839 -0.024193227    -0.046255145    -0.047514524    
-0.042759597    0.002117775 -0.041510757    1.9092667E-4    
[com.intel.analytics.bigdl.tensor.DenseTensor of size 3x4]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import SparseEmbedding
from zoo.pipeline.api.keras.models import Sequential
from bigdl.util.common import JTensor

model = Sequential()
model.add(SparseEmbedding(input_dim=10, output_dim=4, input_shape=(4, )))
input = JTensor.sparse(
    a_ndarray=np.array([1, 3, 2, 4]),
    i_ndarray = np.array([[0, 0, 1, 2],
             [0, 3, 2, 1]]),
    shape = np.array([3, 4])
)
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">JTensor: storage: [1. 3. 2. 4.], shape: [3 4] ,indices [[0 0 1 2]
 [0 3 2 1]], float
</code></pre>

<p>Output is</p>
<pre><code class="python">[[ 0.00771878 -0.05676365  0.03861053  0.04300173]
 [-0.04647886 -0.03346863  0.04642192 -0.0145219 ]
 [ 0.03964841  0.0243053   0.04841208  0.04862341]]
</code></pre>

<hr />
<h2 id="wordembedding"><strong>WordEmbedding</strong></h2>
<p>Embedding layer that directly loads pre-trained word vectors as weights.</p>
<p>Turn non-negative integers (indices) into dense vectors of fixed size.</p>
<p>Currently only GloVe embedding is supported.</p>
<p>The input of this layer should be 2D.</p>
<p>This layer can only be used as the first layer in a model, you need to provide the argument inputLength (a Single Shape, does not include the batch dimension).</p>
<p><strong>Scala:</strong></p>
<pre><code class="scala">WordEmbedding(embeddingFile, wordIndex = null, trainable = false, inputLength = -1)
</code></pre>

<p><strong>Python:</strong></p>
<pre><code class="python">WordEmbedding(embedding_file, word_index=None, trainable=False, input_length=None, name=None)
</code></pre>

<p><strong>Parameters:</strong></p>
<ul>
<li><code>embeddingFile</code>: The path to the embedding file.
                   Currently the following GloVe files are supported:
                   "glove.6B.50d.txt", "glove.6B.100d.txt", "glove.6B.200d.txt"
                   "glove.6B.300d.txt", "glove.42B.300d.txt", "glove.840B.300d.txt".
                   You can download them from: https://nlp.stanford.edu/projects/glove/.</li>
<li><code>wordIndex</code>: Map of word (String) and its corresponding index (integer).
               The index is supposed to start from 1 with 0 reserved for unknown words.
               During the prediction, if you have words that are not in the wordIndex
               for the training, you can map them to index 0.
               Default is null. In this case, all the words in the embeddingFile will
               be taken into account and you can call WordEmbedding.getWordIndex(embeddingFile) to retrieve the map.</li>
<li><code>trainable</code>: To configure whether the weights of this layer will be updated or not.
               Only false is supported for now.</li>
<li><code>inputLength</code>: Only need to specify this argument when you use this layer as the first layer of a model. For Scala API, it should be a positive integer. For Python API, it should be a positive int. Batch dimension should be excluded.</li>
</ul>
<p><strong>Scala example:</strong></p>
<pre><code class="scala">import com.intel.analytics.zoo.pipeline.api.keras.layers.WordEmbedding
import com.intel.analytics.zoo.pipeline.api.keras.models.Sequential
import com.intel.analytics.bigdl.tensor.Tensor
import com.intel.analytics.bigdl.utils.T

val model = Sequential[Double]()
model.add(WordEmbedding[Double](&quot;/path/to/glove.6B.50d.txt&quot;, wordIndex = WordEmbedding.getWordIndex(&quot;/path/to/glove.6B.50d.txt&quot;), inputLength = 1))
val input = Tensor(data = Array(0.418), shape = Array(1, 1))
val output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="scala">input: com.intel.analytics.bigdl.tensor.Tensor[Double] =
0.418
[com.intel.analytics.bigdl.tensor.DenseTensor$mcD$sp of size 1x1]
</code></pre>

<p>Output is:</p>
<pre><code class="scala">output: com.intel.analytics.bigdl.nn.abstractnn.Activity =
(1,.,.) =
0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.00.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.00.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0

[com.intel.analytics.bigdl.tensor.DenseTensor of size 1x1x50]
</code></pre>

<p><strong>Python example:</strong></p>
<pre><code class="python">import numpy as np
from zoo.pipeline.api.keras.layers import WordEmbedding
from zoo.pipeline.api.keras.models import Sequential

model = Sequential()
model.add(WordEmbedding(&quot;/path/to/glove.6B.50d.txt&quot;, word_index=WordEmbedding.get_word_index(&quot;/path/to/glove.6B.50d.txt&quot;), input_length=1))
input = np.random.random([1, 1])
output = model.forward(input)
</code></pre>

<p>Input is:</p>
<pre><code class="python">array([[0.18575166]])
</code></pre>

<p>Output is</p>
<pre><code class="python">array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0.]]], dtype=float32)
</code></pre>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>